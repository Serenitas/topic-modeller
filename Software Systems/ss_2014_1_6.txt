:543.51.
Системы, основанные на использовании GPU (Graphic Processing Units, графических ускорителей), привлекают особое внимание исследователей [1]. Быстрое развитие программного и аппаратного обеспечения, позволяющего использовать графические ускорители для вычислений общего назначения [2], ставит вопрос об эффективности применения таких систем для моделирования больших молекулярных структур, особенно для разработок в сфере нанотехнологий [3].
При исследовании поведения заряженных частиц в электрических и магнитных полях широко распространена модель частиц в ячейках [4].
Очень важным приложением такой модели является программное моделирование в области массспектроскопии.
Так, масс-спектрометры на основе ионного циклотронного резонанса и преобразования Фурье являются лидирующими устройствами в области определения масс для задач протеомики.
Для прямого моделирования поведения ионов в таких установках, а также для исследования физических эффектов, влияющих на точность работы этих устройств, cоздан целый ряд программ [5, 6].
В них расчет парного кулоновского взаимодействия частиц осуществляется через вычисление на трехмерной разностной сети коллективного самосогласованного поля, покрывающего объем ловушки экспериментальной установки.
Поиск такого поля проводится на каждом шаге по времени с помощью решения задачи для уравнения Пуассона.
Для удовлетворительной точности необходимо порядка миллиона временных шагов, поэтому требование быстрого решения является ключевым.
Наиболее часто применяемый здесь метод основан на использовании быстрого преобразования Фурье [5].
С помощью данного подхода были созданы параллельные программы для расчета подобных задач на суперкомпьютерах, в частности на суперкомпьютере IBM Blue Gene/P, установленном на факультете  .
Однако желательно иметь возможность моделирования таких задач еще и на многоядерных системах с использованием вычислительных ресурсов GPU.
В настоящей работе для решения краевой задачи для уравнения Пуассона используются различные графические ускорители и библиотека cuFFT для быстрого преобразования Фурье.
При этом основное внимание уделяется тому, насколько применяемые устройства позволяют ускорить вычисления на основе модели частиц в ячейке.
Также исследуется эффективность нахождения решения для различных параметров задачи и асимптотической сложности алгоритма.
Основная программа является параллельной и написана на языке Fortran.
В ней расчет внешних удерживающих полей на разностной сетке от системы электродов произвольной формы реализован на основе параллельных алгоритмов решения пограничных интегральных уравнений [6].
В конечном счете задача сводится к решению алгебраической системы уравнений с плотной матрицей.
В настоящей работе решение системы находится с использованием библиотеки CULA, реализующей ряд процедур пакета LAPACK на основе технологии NVIDIA CUDA.
Альтернативой данной библиотеке может служить библиотека MAGMA, разрабатываемая в университете штата Теннесси, Ноксвилл .
Полноценное сравнение MAGMA и CULA не входит в задачи этой работы.
Расчеты осуществлялись на системах с различными GPU, в том числе на суперкомпьютере Ломоносов НИВЦ МГУ, где имеется возможность использования гибридных вычислительных систем с CPU  и GPU.
Система позволяет реализовывать различные стратегии распределения параллельных процессов по CPU-устройствам с параллельным выполнением части задач на GPU.
В работе проведено сравнение различных схем такого распределения.
Обозначим через значение потенциала на стенках электродов.
 В методе частица-сетка есть два раздела вычислений, которые являются наиболее трудоемкими и требуют специальных модулей в программе.
В первом разделе рассчитываются силы парного кулоновского взаимодействия частиц.
Особенность задачи при моделировании масс-спектрометра заключается в том, что необходимо моделировать эволюцию большого числа ионов  в течение нескольких миллионов шагов по времени.
Во втором разделе вычисляется удерживающее ионы поле ловушки, создаваемое системой раздельных электродов сложной формы.
Количество рассматриваемых частиц должно быть велико по сравнению с размерностью пространственной сетки.
Это требуется для того, чтобы избежать флуктуаций силы при переходе через границы ячеек.
Обычно берут порядка ста частиц на одну ячейку.
Таким образом, решение первой краевой задачи для уравнения Пуассона  на пространственной сетке является ключевым для расчета сил парного взаимодействия частиц.
Переформулируем задачу  для трехмерной разностной сети размера, равномерно покрывающей прямоугольную область [0, ] [0, ] [0, ].
Здесь и сеточные функции, заданные значениями функций и в узлах сетки; разностная аппроксимация трехмерного оператора Лапласа.
Первая процедура включает в себя инициализацию работы с графическим ускорителем, выделение необходимой памяти, вычисление собственных значений для дальнейшего использования в программе.
Вторая предназначена для решения задачи Дирихле на GPU с использованием дискретного преобразования Фурье.
Использование именно Фурье-преобразования для решения этой задачи существенно, так как соответствует принципам работы реальных устройств масс-спектроскопии.
Третья процедура завершает работу с картой, освобождая всю выделенную память.
Предполагается, что инициализация и завершение работы с ускорителем вынесены за пределы временного цикла, чтобы избежать повторных вычислений.
Взаимодействие с основной программой осуществляется через отдельный интерфейсный модуль, обеспечивающий возможность вызова процедур, описанных на языке C, из основной программы на Fortran.
Для гарантии соответствия типов при его описании использовались именованные константы, предоставляемые модулем ISO_C_BINDING, который входит в стандарт Fortran 2003.
Внутри главной процедуры решения уравнения для осуществления преобразования Фурье используются вызовы библиотеки cuFFT версии 5.5 из состава окружения CUDA Toolkit.
Особенность использования этой библиотеки в том, что она не содержит процедур для синус-преобразований, которые в данном случае необходимы для того, чтобы полученное решение удовлетворяло нулевым граничным условиям.
Поэтому для получения нужного результата применимы следующие соображения.
Расширяем сеточную функцию в два раза по каждому из направлений, продолжив ее нечетно относительно границ трехмерной области.
Применив к расширенной функции дискретное преобразование Фурье, получим результат, на исходном множестве эквивалентный применению синус-преобразования.
Таким образом, процедура решения будет состоять из следующих этапов.
1.
Пересылка значений из основной оперативной памяти в память GPU.
2.
Переупорядочение массива этих значений из расположения по столбцам, принятого в Fortran, на расположение по строчкам, принятое в C.
3.
Нечетное продолжение сеточной функции на вдвое большую область по каждому из направлений, что влечет за собой увеличение количества элементов в массиве в восемь раз.
4.
Прямое преобразование Фурье продолженной сеточной функции.
5.
Деление всех мнимых частей  полученного образа на собственные значения разностной задачи.
6.
Обратное преобразование Фурье образа решения.
7.
Нормировка каждого из элементов результирующего массива на общее количество элементов в нем.
8.
Обратное переупорядочение массива по столбцам.
9.
Пересылка части массива, соответствующего полученному решению на исходной области, из памяти GPU в основное ОЗУ.
Разделим условно процедуру решения на две части.
К первой отнесем прямое и обратное преобразования Фурье, а также деление на собственные числа.
Будем считать, что непосредственно к вычислению решения относятся только эти шаги.
Остальные действия отнесем ко второй части, которую можно рассматривать как подготовку данных.
Для технических характеристик карт, полученных через CUDA runtime API, рассчитываются следующие величины: максимально возможная пропускная способность памяти; пиковая производительность на операциях с одинарной точностью; пиковая производительность на операциях с двойной точностью.
Эти параметры зависят только от характеристик ускорителей.
Они вычисляются по формулам:  При проведении тестов измерялось время решения задачи для уравнения Пуассона на каждом шаге и вычислялось среднее по 100 шагам.
Проведено сравнение вычислений с точным аналитическим решением.
Решение считалось удовлетворительным, если абсолютная ошибка составляла величину.
Время оценивалось при помощи интерфейса CUDA event API, содержащего функции управления событиями и позволяющего вычислить время между двумя событиями в программе.
Для оценки производительности программ с использованием GPU обычно используют две метрики: достигнутая пропускная способность памяти и достигнутая производительность [7].
Первая метрика рассчитывается по следующей формуле:  где количество прочитанных данных в байтах; количество записанных.
Достигнутая производительность определяется соотношением  Чтобы мультипроцессоры карты были достаточно загружены обработкой элементов, не будем рассматривать сетки с <32.
Время решения задачи в зависимости от числа точек  на устройстве GeForce GTX TITAN для одинарной и двойной точности  показано в таблице 1.
Эти данные можно аппроксимировать в смысле наименьших квадратов.
Результат представлен на рисунке 1.
С увеличением размера задачи накладные расходы начинают играть все меньшую роль и на первый план выходит вычислительная составляющая.
Время вычисления решения составляет от 50 до 70 % от общего и растет с увеличением размера задачи.
В таблице 2 показано отношение времени, затраченного непосредственно на вычисление решения, к общему времени выполнения процедуры, включающему пересылку и подготовку данных.
Соответствующие результаты приведены в таблице 3.
Из таблицы видно сильное превосходство ускорителей GeForce GTX TITAN и Tesla C2075 над GeForce GT 640 по количеству потоковых мультипроцессоров  и по пропускной способности памяти, что находит свое отражение в результатах, сокращая время вычисления в 23 раза.
Кроме того, GeForce GTX TITAN за счет большего количества ядер на мультипроцессоре показывает лучшие, чем Tesla C2075, результаты.
Анализ вычисленных метрик позволяет заключить, что при выбранном количестве точек сети порядка 1,7 пиковая производительность GPUустройств не достигается.
Для достижения пиковой производительности требуется на 2 порядка больше данных.
И такие задачи могут возникать при обработке больших БД экспериментальных измерений.
Но в данном случае выбиралось разумное количество точек сети для серийных расчетов по модели частиц в ячейках.
Такое количество данных еще допускается широтой пропускания памяти.
В данном случае широта пропускания не используется полностью.
Это объясняется тем, что после чтения исходных данных каждый раз приходится останавливаться для того, чтобы подготовить их для преобразования Фурье.
Остановка включает в себя транспонирование и расширение массива в 8 раз тяжелые операции для памяти GPU, обладающей высокой латентностью.
Это оказывает влияние на достигнутую вычислительную производительность.
В зависимости от ускорителя она составляет от 44 до 150 Гфлопс, что соответствует 410 % от пиковой.
Но, несмотря на не самый оптимальный для использования всех ресурсов GPU алгоритм, задача решается достаточно быстро для использования в приложениях.
Оно проводится с помощью потенциала двойного слоя.
На поверхности данный потенциал имеет разрыв.
В каждом уравнении один из интегралов в сумме имеет сингулярность.
В численной схеме такие члены c интегрируемой особенностью должны иметь специальную аппроксимацию.
Внешняя часть поверхности является отталкивающей, а внутренняя притягивающей.
В правой части уравнения имеются два члена.
Первый член приложенный потенциал на поверхности, второй сумма потенциалов от всех поверхностей в системе, которые индуцируют электрическое поле на данной поверхности .
Уравнения должны решаться итерационно для каждой поверхности системы.
Так система для дипольных моментов получается связанной.
Идея итерационной процедуры состоит в фиксации дипольных моментов для всех поверхностей, кроме.
Поскольку правая часть уравнения известна на текущем итерационном шаге, она может быть вычислена.
А как только решения для всех поверхностей на этом шаге будут найдены, можно вычислить и новую правую часть.
Пусть номер итерации, тогда получаем следующую итерационную процедуру:.
Здесь правая часть известна с предыдущей итерации.
Ее первый член заданный поверхностный потенциал, а второй найден на предыдущем шаге и представляет собой влияние других поверхностей на распеделение дипольного момента с номером.
Главным преимуществом этой формы является возможность независимого решения уравнений для каждой поверхности, что важно для параллельных вычислений.
Так как матрица является плотной, для решения можно использовать соответствующие стандартные процедуры библиотеки LAPACK.
На любой итерации алгоритма электрический потенциал, индуцированный на каждой поверхности электрода, может быть вычислен независимо.
Для этого в программе порождается +1 MPI-процесс, каждый из которых соответствует своей поверхности, за исключением процесса с номером 0.
На каждой итерации определяются поля, индуцированные собственной поверхностью процесса на всех других поверхностях с номерами.
Эти поля посылаются в нулевой процесс и складываются.
Там же определяется и полное поле, индуцированное всеми электродами.
Затем индуцированное поле на каждом электроде посылается обратно на соответствующий вычислительный процесс с номером и определяется новая правая часть в уравнениях.
На новой итерации используется модифицированное граничное условие для каждой поверхности.
В каждом MPI-процессе, выполняемом на CPU, требуется решать алгебраическую систему уравнений  со своей матрицей и правой частью.
В программе это решение проводится с помощью GPU-устройств параллельно для каждой поверхности.
Схема параллельного алгоритма показана на рисунке 2.
Расчеты проводились на суперкомпьютере Ломоносов.
Каждый из вычислительных узлов суперкомпьютера оборудован двумя 4-ядерными CPU и двумя GPU Tesla X2070, по характеристикам аналогичными Tesla C2070.
Это позволяет использовать различные стратегии распределения параллельных процессов по узлам.
Были использованы две схемы.
1.
Двадцать MPI-процессов распределяются по одному процессу на вычислительное ядро процессора.
Доступ к каждому из GPU осуществляется одновременно с нескольких процессов.
Для осуществления такой стратегии достаточно трех узлов суперкомпьютера.
2.
Каждый из 20 MPI-процессов запускается на своем вычислительном узле.
При использовании первой схемы несколько процессов конкурируют за ресурсы GPU, каждое из которых единовременно может выполнять лишь один CUDA-контекст, соответствующий одному MPI-процессу.
При этом процедура решения СЛАУ в CULA немонолитна и разбивается на некоторое количество подзадач, что позволяет переключать контексты нескольких процессов по мере необходимости.
Накладные расходы на такие переключения сокращаются при увеличении размеров обрабатываемых данных.
По второй схеме каждый процесс получает доступ к GPU сразу, без ожидания и выполняет подзадачи одного процесса до получения окончательного ответа.
M PI M PI M PI M PI M PI M PI M PI M PI G PU G PU M PI M PI M PI M PI M PI M PI M PI M PI NODE CPU NODE Distributor CPU NODE CPU В таблице 4 показаны среднее время решения СЛАУ при использовании первого и второго вариантов и отношение этих значений.
Чем больше параллельно обращающихся к одному GPU процессов, тем сильнее разброс времени возвращения из процедуры решения.
В каждом параллельном процессе на CPU используется GPU для решения алгебраической системы уравнений.
Для сравнения показано также время решения при использовании лишь одной видеокарты GeForce GT 740M .
Таким образом, задача моделирования работы масс-спектрометров на основе ионного циклотронного резонанса и преобразования Фурье может быть решена на гибридных системах.
В заключение отметим, что в статье представлены результаты реализации кода Pic3D частиц в ячейке для моделирования работы масс-спектрометров на основе ионного циклотронного резонанса и преобразования Фурье на гибридных системах с CPU и GPU.
Вычисления показывают, что ускорители могут быть использованы для определения кулоновского взаимодействия ионов с помощью решения первой краевой задачи для уравнения Пуассона и параллельного вычисления полей от каждой поверхности электрода через решение алгебраических систем с достаточной эффективностью.
Эффективность параллельного использования GPU существенно зависит от выбранной схемы распределения процессов параллельной программы.
В настоящее время происходят интенсивное распространение и развитие технологий для GPGPU, что выражается в частом обновлении архитектур и, как следствие, в появлении новых возможностей.
В качестве примера здесь можно привести появление функций Hyper-Q в последней версии архитектуры NVIDIA Kepler.
Возможность повышения эффективности параллельного использования графических ускорителей за счет новых свойств представляет немалый интерес и требует особого исследования.
