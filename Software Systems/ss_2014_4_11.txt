
Первой и наиболее известной векторной  является модель Поттса [15].
Эта модель по-прежнему исследуется учеными из таких различных областей, как физика, медицина, сегментация изображений и НС.
Позже была предложена модель параметрической НС [6], всесторонне исследованная небольшим коллективом Института оптико-нейронных технологий РАН .
Похожая модель  была независимо разработана и продолжает исследоваться в Йоркском университете, Канада [7].
В работе [5] представлена модель векторной НС с мерой близости между состояниями нейронов.
Эта модель обобщила все перечисленные модели.
Исследователями рассматривались как полносвязные, так и персептронные архитектуры.
Были изучены различные правила обучения векторных сетей [9].
Полученные результаты говорят об их высокой эффективности.
Для практических приложений, требующих реализации ассоциативной памяти, более всего подходят персептроны .
Однако они имеют существенный недостаток: достаточно даже одному выходному нейрону переключиться в неправильное состояние, чтобы входной вектор был идентифицирован неверно.
Для борьбы с этим приходится повышать надежность каждого нейрона путем повышения избыточности НС либо уменьшения загрузки сети.
Иначе можно сказать, что векторный персептрон состоит из надежных нейронов, которым нельзя ошибаться, а это противоречит всей идеологии НС.
Альтернативный подход заключается в применении слабых нейронов .
При равных затратах оперативной памяти совокупность слабых нейронов оказывается эффективнее небольшого числа надежных нейронов.
Смысл в том, чтобы оснастить векторный персептрон дополнительным слоем из одного нейрона, количество состояний которого равно количеству запомненных образов.
Его задача заключается в накоплении информации от предыдущего слоя и непосредственной идентификации входного образа.
К предложенной идее близка идея, изложенная в работах [10, 11].
Данная статья содержит формальное описание модели, качественное описание, в котором сделана попытка на простом примере показать суть предлагаемого нововведения, и экспериментальные результаты.
Авторы решают задачу поиска ближайшего соседа, которая заключается в следующем.
Первый  слой состоит из векторных нейронов, каждый из которых имеет 2 фиктивных состояния, описываемых орт-векторамипозиции.
Фиктивность состояний заключается в том, что на этапе обучения нейроны второго слоя имеют дискретных состояний, а в процессе работы нейроны рассматриваются как простые сумматоры.
Это сделано с целью упрощения описания модели.
Состояние персептрона описывается тремя векторами: 1) входной слой описываетсяпозиции.
Если последнее требование не исполняется, вероятность ошибки возрастает на несколько порядков.
Для этого сначала вычислим локальные поля нейронов внутреннего слоя:.
Для каждого разбиения можно вычислить вероятностей  того, что входной образ принадлежит каждому из этих подмножеств.
Каждый векторный нейрон это своего рода решатель, выбирающий подмножество, имеющее максимальную вероятность .
Пересечение подмножеств, выбранных всеми решателями, определяет выход однослойного персептрона.
При этом при оценке этих вероятностей могут совершаться ошибки, связанные со статистической природой проводимых вычислений.
Следовательно, решение, основанное только на выборе подмножеств по максимальной вероятности, может быть ошибочным.
Достаточно неправильно определить подмножество-победитель x x N q q r k  m.
.
.
.
.
.
.
.
.
хотя бы в одном разбиении, чтобы конечное решение было ошибочным.
Базовой идеей предлагаемой модели является учет этого недостатка.
Для этого необходимо трактовать полученные дляспособом.
Если раньше мы рассматривали как индикаториндикатор.
Решение о том, какой паттерн был подан на вход, принимается на основании этих интегральных индикаторов, что позволяет использовать информацию о всех подмножествах во всех разбиениях.
.
Пусть на вход персептрона был подан искаженный паттерн B.
На рисунке рядом с каждым подмножеством указана вычисленная вероятность того, что входной паттерн принадлежит данному подмножеству.
При идентификации по схеме однослойного персептрона в первом разбиении подмножествомпобедителем является первое подмножество, которое действительно содержит входной паттерн, а во втором тоже подмножество 1 , однако оно не содержит входной паттерн.
Результатом пересечения этих подмножеств будет пустое множество, то есть сеть не может идентифицировать входной паттерн.
Таким образом, ошибка на одном нейроне влечет за собой ошибку всей системы.
То есть почти с равной вероятностью входной паттерн может принадлежать как первому, так и второму подмножеству.
В предложенной модели этот факт учитывается, и решение принимается уже на основании значений вероятностей из обоих разбиений для каждого паттерна.
В качестве ответа выбирается образ, которому соответствует максимальное суммарное значение вероятности.
В результате получаем, что сеть правильно идентифицировала, какой паттерн был подан на вход системы.
Столь небольшие дополнительные затраты на второй слой с лихвой окупаются повышением емкости памяти и надежности.
На примерах был показан смысл дополнительного слоя.
Далее необходимо исследовать свойства модели и сопоставить характеристики однослойного и двухслойного персептронов.
Выполнить это можно несколькими способами.
1.
Взять ряд БД, содержащих реальные данные из различных областей, и исследовать, насколько успешно на них работают предложенная модель и исходный однослойный персептрон.
Полученные таким путем результаты будут очень важны, так как позволят понять, какое место занимают эти алгоритмы среди уже имеющихся.
Недостаток данного подхода в том, что для понимания причин, приводящих к улучшению или ухудшению работы, необходим глубокий анализ данных, а эта задача сама по себе нетривиальна.
2.
Другой подход заключается в том, чтобы создать искусственные наборы данных эталонных векторов  и тестировать на них исследуемые модели.
В этом случае появляется возможность преднамеренно создавать ситуации, в которых ярко проявляются интересующие свойства моделей.
Полученные в результате оценки позволяют глубже понимать происходящие внутри НС процессы.
Очевидно, что для всестороннего исследования необходимо пройти по обоим путям.
В настоящей работе авторы идут по второму пути: в качестве эталонных образов выступают векторы, компоненты которых сгенерированы независимо и с одинаковой вероятностью равны +1 и 1.
Выбор такого алгоритма генерации эталонных векторов связан с тем, что, во-первых, это наиболее простой случай для аналитических вычислений, во-вторых, полученные оценки емкости будут ограничением сверху, то есть мы оценим максимально достижимую емкость памяти НС, которую нельзя будет превзойти.
При этом вероятность правильного распознавания сильно зависит от величин корреляций в каждом конкретном случае.
Поэтому различные модели ассоциативной памяти можно сравнивать только по оценке сверху емкости памяти, полученной в наиболее простом случае.
Например, известный результат по емкости ассоциативной памяти для сети Хопфилда, равный 0,14, получен в тех же предположениях.
Необходимо дать определение термину емкость памяти.
Емкость ассоциативной памяти это величина, определяющая количество эталонных образов, на которых можно обучить НС так, чтобы она была способна безошибочно распознавать все запомненные образы.
Это классическое определение можно сформулировать иначе.
Емкость ассоциативной памяти это такое количество эталонных образов, при распознавании которых вероятность ошибочного распознавания предъявленного эталонного образа равна 1/.
При этом принято, что НС тестируется на запомненных эталонных образах без внесения в них каких-либо искажений.
Для принятых условий авторам удалось оценить емкость памяти обеих моделей.
Полученные оценки хорошо согласуются с экспериментом, лишь в 1,13 раза отличаясь по величине от экспериментально полученных результатов.
 Емкость памяти однослойного персептрона  2 ln.
 Проанализируем емкость памяти обеих моделей.
Из  и  можно сделать следующие выводы: емкость памяти обеих моделей растет линейно от, ; с ростом доли искаженных компонент во входном векторе емкость памяти моделей убывает квадратично; повышение требований к надежности распознавания, то есть уменьшение допустимой вероятности ошибки, приводит к логарифмическому убыванию емкости памяти сети; самое главное: емкость памяти двухслойной сети больше памяти однослойной сети в раз.
Покажем, что добавление второго слоя повышает вероятность правильного распознавания НС входных векторов.
Для этого проведем экспериментальное сравнение двух моделей однои двухслойной сети.
В этих экспериментах будем варьировать внешние параметры задачи, ,.
Детально рассмотрим поведение модели в зависимости от внутренних параметров модели и.
Увеличение обоих этих параметров повышает вероятность правильного распознавания.
Однако эти параметры оказывают разное влияние на модель.
Увеличение приводит к снижению количества информации на долю одной межсвязи, а увеличение позволяет накопить больше статистической информации.
Исследуем емкость памяти модели.
Проверим согласованность полученных оценок с экспериментальными.
Рассмотрим еще одну возможность, которую предоставляет предлагаемая модель, возможность решения задачи поиска ближайших соседей.
Если количество образов, их размерность и параметр шума  определяются решаемой задачей, то количествонадежности.
Сначала рассмотрим, как меняется ошибка при фиксированных параметрах и от и .
Как и ожидалось, увеличение размерности запомненных векторов либо понижение их количества приводит к экспоненциальному понижению ошибки.
К тому введение дополнительного слоя позволяет понизить вероятность ошибки более чем на один порядок .
Выигрыш тем существеннее, чем меньше ошибка на исходной однослойной сети.
Помехоустойчивость двухслойной сети тоже лучше: кривая с маркерами, соответствующая однослойной сети, лежит гораздо выше ..
Получается, что с точки зрения надежности во втором слое лучше использовать небольшое количество надежных  нейронов.
Однако такие сети неустойчивы к разрушениям самой нейросети.
Представленные на рисунке 6 результаты  доказывают, что надежные и устойчивые к разрушениям нейросистемы могут создаваться и из ненадежных элементов, имеющих значительный разброс параметров.
В первом случае ключевую роль играет второй слой, накапливающий информацию от большого количества ненадежных элементов .
Рисунок 7 демонстрирует зависимость ошибки от внутренних параметров и.
Представим результаты экспериментального измерения емкости памяти DLP и посмотрим, насколько хорошо полученная оценка емкости памяти  согласуется с результатами.
Из представленных графиков видно, что формула  отлично согласуется с экспериментом.
Полученные кривые подтвержают правильность выводов, сделанных ранее.
Продемонстрируем это экспериментально.
Сгенерируем случайных независимых некоррелированных паттернов и дополнительно сгенерируем еще 6 паттернов, которые будут похожи друг на друга в разной степени.
Компоненты локальных полей, соответствующие этим шести паттернам, будут больше значений полей, соответствующих остальным паттернам.
Это свойство можно использовать и для того, чтобы повысить надежность распознавания при решении поставленной задачи поиска первого ближайшего соседа.
Далее вычислить скалярные произведения входного вектора с этими эталонными векторами и уже по результату выбрать победителя по максимальному значению.
В результате ценой нескольких дополнительных скалярных произведений можно существенно понизить вероятность ошибочного распознавания.
На рисунке 14 продемонстрирована очень высокая эффективность этого улучшения.
В настоящей статье показано, что эффективность однослойной модели векторного персептрона можно увеличить, добавив дополнительный слой.
Продемонстрирована исключительно высокая эффективность предложенной модели.
Наглядно показано, что целенаправленное конструирование НС в противоположность слепому увеличению избыточности может дать великолепные результаты.
