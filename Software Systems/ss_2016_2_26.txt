Методы и средства анализа информативности признаков при обработке медицинских данных



Применение современных информационных технологий в медицине способствует накоплению огромных объемов медицинских данных, хранимых и обрабатываемых с помощью медицинских информационных систем (МИС). Эти данные содержат в себе медицинские знания, которые можно извлекать и использовать для принятия решений, например, при распознавании (диагностике) патологических процессов. При анализе медицинских данных, обнаружении закономерностей в этих данных и их извлечении приходится сталкиваться с проблемой размерности. Размерность хранимых данных, определяемая числом различных признаков, описывающих состояние здоровья пациента, весьма велика и порой достигает нескольких десятков и сотен показателей. Поэтому проблема снижения размерности признакового пространства и выделения наиболее информативных признаков весьма актуальна для МИС.

Существует несколько обстоятельств, обусловливающих возможность перехода от большего числа исходных показателей состояния пациента к существенно меньшему числу наиболее информативных признаков. Это прежде всего дублирование информации вследствие наличия связей между признаками, низкая информативность отдельных признаков, взвешенное суммирование некоторых признаков и конструирование обобщенных признаков. Информативность признака является понятием относительным. Одна и та же система признаков может быть информативной для решения одной задачи распознавания и неинформативной для другой. Так, при дифференциальной диагностике заболевания почек диагностически значимыми будут одни признаки, а при диагностике бронхиальной астмы другие. Признаком (Feature) принято называть некоторый показатель или определенную характеристику объекта произвольной природы. Набор признаков рассматриваемого множества объектов называется признаковым пространством, а совокупность значений признаков, относящихся к одному объекту, – признаковым описанием этого объекта . В задачах медицинской диагностики объектами являются пациенты, а в качестве признаков выступают показатели состояния их здоровья.

Различают следующие типы признаков. Количественные признаки – это признаки, измеренные в некоторой числовой шкале. Качественные (порядковые, балльные) признаки используются для выражения терминов и понятий, не имеющих числовых значений (например тяжесть заболевания), измеряются в порядковых шкалах. Номинальные признаки – это признаки, измеренные в шкале наименований (например группа крови, пол). При анализе таких признаков каждую отметку номинальной шкалы считают отдельным самостоятельным признаком, принимающим одно из двух значений: 1 («да») или 0 («нет») . Количественное выражение качественных и номинальных признаков в анализе данных часто называют шкалированием . После шкалирования к качественным и номинальным признакам можно применять различные методы численного анализа, включая статистические.

В работе рассматриваются методы снижения размерности признакового пространства, а также статистические методы анализа информативности признаков при обработке медицинских данных на примере дифференциальной диагностики заболевания почек. Считается, что изначально признаки могут быть разнотипными, однако на этапе анализа их информативности они уже отшкалированы и представлены в количественном виде.

Постановка задачи сокращения признакового пространства и обзор существующих методов решения 

Пусть множество объектов,  конечное множество количественных признаков этих объектов. Для всякого объекта известно его признаковое описание n-мерный вектор, причем i-я координата этого вектора равна значению i-го признака. Совокупность признаковых описаний объектов из заданной выборки объектов задана в виде матрицы размера, называемой таблицей «объект  признак». Пусть  мера информативности подмножества признаков , определенная на. Требуется среди всех различных подмножеств множества выбрать некоторое такое подмножество, что.

В теории распознавания образов поставленная задача называется FEATURES SELECTION (селекция признаков) . Задача FEATURES SELECTION является вычислительно сложной, поскольку при перебор всех различных подмножеств требует времени.

Задача FEATURES SELECTION может быть обобщена путем определения преобразования, позволяющего из X формировать новое пространство признаков. В такой постановке задача называется задачей FEATURES EXTRACTION (извлечение или конструирование признаков) . При решении FEATURES EXTRACTION формируются новые признаки на основе уже имеющихся в X. Самое простое преобразование Z=F(X) – это линейное преобразование.

Тот или иной вариант конкретизации FEATURES SELECTION или FEATURES EXTRACTION заключается в задании меры информативности, а для FEATURES EXTRACTION и класса допустимых преобразований, что приводит к конкретному методу решения.

Основными способами решения задачи FEATURES EXTRACTION являются методы факторного анализа и метод экстремальной группировки признаков. Факторный анализ позволяет выделить обобщенные признаки (факторы), каждый из которых представляет сразу несколько исходных признаков. Один из методов факторного анализа – метод главных компонент . Суть его состоит в поиске линейных комбинаций признаков из X и в конструировании на их основе меньшего по мощности пространства признаков, информативность которого равнозначна информативности X в целом. Как показывает практика, использование метода главных компонент оказывается наиболее результативным, когда все признаки однотипны и измерены в одних и тех же единицах. В противном случае полученные линейные комбинации исходных признаков трудно интерпретируемы.

В методе экстремальной группировки по матрице «объект – признак» выборки A вычисляется корреляционная матрица и множество X разбивается на группы так, чтобы внутри одной группы признаки были сильно скоррелированы, а между группами наблюдалась относительно слабая корреляция. Далее осуществляется замена каждой группы признаков одним равнодействующим признаком. Недостатком данного метода является невозможность определения оптимального числа групп .

В том случае, когда требуется лишь оценить значимость отдельных независимых признаков на основе заданной меры информативности, широко используются статистические методы: метод накопленных частот, метод Шеннона и метод Кульбака . Выбор этих методов обусловлен следующими причинами: данные методы основаны на достаточно простых алгоритмах вычисления меры информативности, результаты применения этих методов легко интерпретируются. Именно эти особенности существенны для различных категорий пользователей МИС, большинство из которых не являются специалистами в области анализа данных и IT-технологий. Кроме того, данные методы составляют математический базис многих алгоритмов решения задачи FEATURES SELECTION, направленных на сокращение полного перебора всех различных подмножеств в . Разработан целый спектр таких алгоритмов . Наиболее известный из них алгоритм AdDel, который сводится к последовательному выполнению процедур добавления (Addition) наиболее информативных и исключения (Deletion) наименее информативных признаков. Помимо этих алгоритмов, возможен также выбор наиболее значимых признаков экспертами-медиками на основе вычисленных оценок информативности признаков.

Следует заметить, что оценка информативности признаков всегда зависит от того, что от чего нужно отличать, то есть от списка распознаваемых образов . Чаще всего эти образы задаются разбиением выборки A на две обучающие выборки – A1 и A2. В данном случае FEATURES SELECTION сводится к решению следующей частной задачи: для заданных обучающих выборок A1, A2 и меры информативности требуется вычислить для каждого признака и указать те признаки, которые в наибольшей степени, согласно, объясняют различие между A1 и A2. Рассмотрим особенности решения этой задачи методом накопленных частот (МНЧ), методами Шеннона и Кульбака.

Метод накопленных частот

Суть МНЧ заключается в следующем. Пусть имеются два набора значений признака х  X, принадлежащие двум матрицам «объект – признак» обучающих выборок A1 и A2. Далее не будем делать различия между выборками и соответствующими им матрицами «объект – признак». По двум наборам значений признака x строятся эмпирические распределения и подсчитываются накопленные частоты как суммы частот от начального до текущего интервала распределения. Мерой информативности признака х служит модуль максимальной разности накопленных частот: , 

где накопленная частота для j-го интервала выборки A1; накопленная частота для j-го интервала выборки A2; число интервалов.

Продемонстрируем алгоритм вычисления меры информативности  на примере дифференциальной диагностики заболевания почек. Рассмотрим следующее множество исходных признаков: Возраст пациента, Длина почки, Ширина почки, Толщина почки,  Толщина паренхимы, Скорость кровотока, Ускорение артериального потока.

Пусть заданы обучающие выборки А1 и A2, отражающие результаты измерения этих параметров для двух состояний – «Здоровая почка» и «Имеются множественные кисты» соответственно. Результаты этих измерений взяты из работы  и представлены в таблице 1. Детальное описание расчетов покажем на примере признака x = «Возраст пациента».

Построим эмпирические распределения признака х по каждой выборке. Для этого вычислим минимальное и максимальное значения этого признака и размах для всех данных таблицы 1.

Зададим количество интервалов распределения так, чтобы размах значений признака примерно нацело делился на число q. В данном случае. Теперь найдем величину  интервала распределения по формуле Вычислим границы каждого j-го интервала.

Для построения эмпирического распределения признака х по выборке необходимо найти количество попаданий значения данного признака в каждый интервал исходя из соотношения. Далее накопленную частоту для j-го интервала вычисляем следующим образом.

Для исходных данных из таблицы 1 результаты всех вычислений по формулам – приведены в таблице 2.

Подставляя значения последнего столбца таблицы 2 в формулу , получаем окончательный результат: мера информативности исследуемого признака x = «Возраст пациента» равна 3 и отвечает диапазону возраста.

Меры информативности всех признаков исходного множества , вычисленные по МНЧ, приведены в таблице 3. Таким образом, согласно заданным обучающим выборкам А1, A2 и мере информативности , наиболее значимыми признаками для дифференциальной диагностики заболевания почек являются «Длина почки», «Возраст пациента», «Ускорение артериального потока».

Метод Шеннона 

В методе Шеннона в качестве меры информативности признака x рассматривается средневзвешенное количество информации, которое свойственно анализируемому признаку. Информативность признака x вычисляется по формуле.

Обозначения, используемые в формуле : q – количество градаций признака; k = 1, 2 – номер обучающей выборки; Рi – вероятность попадания значения признака в i-ю градацию,  где частота появления значения признака в i-й градации для выборки Ak; общее число признаковых описаний объектов, входящих в А1 и A2; вероятность появления значения признака в i-й градации.

Результаты вычислений по формулам (7)–(9) для исходных данных из таблицы 1 и признака х = «Возраст пациента» представлены в таблице 4. Подставляя значения последнего столбца таблицы 4 в формулу , получаем .

Заметим, что метод Шеннона дает оценку информативности исследуемого признака в виде нормированной величины, которая принимает значения от 0 до 1. Это следует из формулы , поскольку значения вероятностей pi k находятся в интервале от 0 до 1, а логарифм от таких значений меньше нуля. Об информативности признака x в этом случае говорят, что, чем ближе I(x) к 1, тем выше информативность x и, наоборот, чем ближе I(x) к 0, тем ниже информативность x. Меры информативности всех признаков исходного множества , вычисленные по методу Шеннона, приведены в таблице 5.

Согласно заданным обучающим выборкам А1, A2 и мере информативности , наиболее значимые признаки при дифференциальной диагностике заболевания почек  это «Скорость кровотока», «Ускорение артериального потока», «Возраст пациента».

Метод Кульбака 

В данном методе в качестве меры информативности признака х рассматривается величина, называемая дивергенцией Кульбака и отражающая расхождение между выборками A1 и A2 следующим образом,  где q – количество градаций признака; вероятность попадания значения признака в i-ю градацию,  где частота появления значения признака в i-й градации выборки Ak.

Для исходных данных из таблицы 1 и признака х = «Возраст пациента» результаты вычислений по формулам – представлены в таблице 6. Согласно формуле  имеем.

Метод Кульбака дает оценку информативности исследуемого признака в виде величины, которая принимает значения от 0 до 2. В этом случае считают, что, чем ближе к 2, тем выше информативность x и, наоборот, чем ближе к 0, тем ниже информативность x. Меры информативности всех признаков исходного множества , вычисленные по методу Кульбака, приведены в таблице 7.

Из таблицы 7 следует, что метод Кульбака определяет тот же набор информативных признаков, что и метод Шеннона.

Сравнение таблиц 3, 5, 7 позволяет сделать следующие выводы: рассмотренные методы не противоречат друг другу и дают близкие наборы наиболее информативных признаков на одних и тех же обучающих выборках, результаты методов Шеннона и Кульбака в основном совпадают. Приведенные выше расчеты были выполнены в предположении, что все признаки из множества  являются независимыми. Для проверки этого предположения была осуществлена попытка группировки признаков с помощью статистического пакета для социальных нужд IBM SPSS (Statistical Package for the Social Sciences) . Эксперименты показали, что признаки из таблицы 1 не подлежат группировке.

Программная реализация методов анализа информативности признаков 

Рассмотренные выше методы анализа информативности признаков реализованы в виде комплекса программ InformSigns на языке программирования С++ в среде Embarcadero RAD Studio XE8. Исходными данными для InformSigns является матрица «объект – признак», имеющая вид таблицы 1 и разделенная на две выборки. Для ввода исходных данных имеется интерфейс, вид которого представлен на рисунке. Возможен также ввод исходных данных из внешнего файла формата Microsoft Excel.

При наличии в матрице «объект–признак» качественных или номинальных признаков комплекс программ InformSigns позволяет выполнить шкалирование, то есть установить соответствие между текстовым или номинальным значениями признака и его числовым эквивалентом. Пример шкалирования номинального признака «Пол» представлен на рисунке 1.

В программном комплексе InformSigns предусмотрено несколько вариантов вывода результатов: в виде таблицы значений информативности, круговой диаграммы и гистограммы. Для табличных данных возможен вывод результатов во внешний файл формата Microsoft Excel. Графическое представление данных позволяет наглядно оценить полученные значения информативности всех признаков для каждого метода отдельно, а также для всех трех методов одновременно. Пример вывода результатов таблиц 3, 5, 7 в виде круговых диаграмм представлен на рисунке 2.

Рассмотренные в работе методы оценки информативности признаков являются наиболее простыми и понятными для алгоритмизации и применения. Результаты их использования хорошо интерпретируемы. Программная реализация данных методов не является трудоемкой и не влечет за собой значительных вычислительных ресурсов. Именно поэтому они могут успешно использоваться при решении задач диагностики патологических процессов в различных лечебных учреждениях. Следует отметить, что МНЧ, методы Шеннона и Кульбака реализованы в некоторых универсальных программных средствах анализа данных. Однако для освоения и применения этих программных инструментов требуется профессиональная подготовка в области IT-технологий. Если в качестве пользователя выступает врач, то целесообразно применение узкоспециализированных медицинских информационных систем, к которым можно отнести InformSigns.

Комплекс программ InformSigns в настоящее время успешно используется в учебном процессе Красноярского государственного медицинского университета имени профессора В.Ф. Войно-Ясенецкого на кафедре медицинской информатики и инновационных технологий. Применение InformSigns подтвердило эффективность и полезность реализованных в системе методов сокращения признакового пространства для решения задач медицинской диагностики по независимым признакам. Дальнейшее развитие программного комплекса InformSigns направлено на реализацию методов выявления скрытых зависимостей между данными, привязку к БД конкретных МИС и патологиям.

