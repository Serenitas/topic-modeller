
Большинство крупных производителей заинтересованы в повышении эффективности работы сотрудников отделов продаж .
Одним из возможных способов повышения эффективности является использование систем SFA .
Необходимо учитывать тот факт, что у дистрибьюторов и производителя разные .
С учетом всего перечисленного сегодня наиболее часто используются следующие варианты развертывания SFA-систем: 1) у каждого дистрибьютора устанавливается серверное оборудование SFA-системы; данный сервер подключается к КИС дистрибьютора и отвечает за обслуживание небольшой группы ТП; 2) в специализированном дата-центре устанавливается сервер SFA-системы, который отвечает за обслуживание всех дистрибьюторов и ТП.
Главным достоинством данного варианта является возможность распределения нагрузки на серверные элементы SFA-системы и ускорения попадания данных в КИС дистрибьютора.
Второй вариант развертывания SFA-системы  позволяет в большей или меньшей степени устранить недостатки первого варианта, однако в данном случае к серверу SFA-системы предъявляются повышенные требования по надежности аппаратно-программного комплекса в целом.
На самом деле проблема надежности становится первоочередной, так как выход центрального узла из строя приведет к коллапсу всей SFAсистемы.
В данной статье описывается разработка архитектуры развертывания серверной составляющей SFA-системы OMOBUS , которая позволила бы повысить отказоустойчивость системы в целом и полностью устранила бы возможные риски, связанные с простоем системы.
Проведенные предварительные исследования показали, что фактически единственным способом, которым можно повысить отказоустойчивость системы, является организация кластера, состоящего из нескольких узлов [3].
Под понимается группа компьютеров, объединенных высокоскоростными каналами связи и представляющая, с точки зрения пользователя, единый аппаратный ресурс.
Для решения поставленной задачи необходимо использовать кластеры высокой доступности с модульной избыточностью.
Только в этом случае гарантируется, что разные узлы кластера будут находиться всегда в едином состоянии либо различия гарантированно не повлияют на дальнейшую работу.
При этом количество узлов, входящих в кластер, должно быть не менее двух, желательно расположенных в разных дата-центрах.
Все остальные виды кластеров по тем или иным причинам не позволяют решить поставленную задачу в полной мере: отказоустойчивые кластеры с холодным/горячим резервом подразумевают потенциальную возможность простоя, если основной узел вышел из строя и не гарантирует эквивалентность данных, хранящихся на узлах; кластер с балансировкой нагрузки предполагает наличие центральных узлов, отвечающих за диспетчеризацию данных; вычислительные кластеры и grid-системы используются для других целей.
С учетом всего перечисленного схема элементов HA-кластера с модульной избыточностью будет иметь вид, приведенный на рисунке 2.
Как видим, кластер не содержит центральный узел, все элементы кластера равноправны и не зависят друг от друга.
Выход любого узла кластера из строя не повлечет за собой отказ всей системы.
Как видно из рисунка 2, процедура выбора доступного узла кластера реализована на стороне мобильного терминала.
Данный подход позволяет отказаться от использования центрального узла кластера, который отвечает за диспетчеризацию принимаемых потоков данных.
Фактически за диспетчеризацию данных отвечает каждый мобильный терминал самостоятельно.
Список узлов кластера загружается при запуске мобильного терминала из текстового конфигурационного файла.
Из этого же файла загружаются параметры, связанные с проверкой доступности узла кластера .
Код процедуры проверки доступности узла написан на языке C с использованием BSD sockets [4].
Данный подход позволяет добиться высокой производительности и переносимости кода.
В настоящий момент процедура проверки доступности узла используется на терминалах, работающих под управлением Windows Mobile и Android.
Схема проверки доступности узла кластера приводится на рисунке 3.
Как видно из приведенной схемы, в ходе выполнения процедуры выбирается первый доступный узел из списка.
Узел считается доступным и готовым к приему данных только в случае, если все перечисленные шаги выполнены успешно.
При штатной работе кластера большая часть данных будет передаваться на первый узел кластера.
В результате этого возникает вторая задача, которая подразумевает репликацию полученных на каждый узел данных на другие узлы кластера.
Данная задача решается с помощью модуля omobus-pk-repl.
Модуль omobus-pk-repl работает на каждом узле кластера и отвечает за репликацию входящих потоков данных на общие ресурсы, доступные всем заинтересованным сторонам.
При необходимости пакеты могут сжиматься с помощью алгоритма deflate или БарроузаУилера.
Исходя из этого omobus-pk-repl выполняет следующие операции.
Распаковывается исходный пакет .
Это позволяет провести тонкую проверку входящих данных на соответствие требуемой спецификации.
Если все проверки прошли успешно, пакет дублируется в локальную папку, к которой имеется доступ через FTP с других узлов.
Дальнейший ввод реплицированных пакетов осуществляется с помощью сервера документов omobus-node, который загружает данные, хранящиеся на удаленном узле.
На узле node2 реплицированные для узла node1 пакеты будут накапливаться до тех пор, пока узел node1 не будет возвращен к активной работе.
Как только это произойдет, все накопленные данные будут загружены на узел node1 и обработаны.
После этого узлы станут идентичными .
В настоящий момент времени разработанное ПО находится в стадии финального тестирования.
Реальная эксплуатация кластера SFA-системы OMOBUS намечена на ноябрьдекабрь 2013 года.
К кластеру планируется подключить около 500 мобильных терминалов, предназначенных для работы на территории всей страны.
В заключение следует отметить, что полученная архитектура кластера высокой доступности с избыточной модульностью позволяет реализовать SFA-систему, работающую в режиме 24X7X365, то есть фактически обеспечить необходимый сервис в непрерывном режиме в любой момент времени круглый год.
