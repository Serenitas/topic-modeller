
К одному из сложных видов  относятся рекуррентные, в которых имеются обратные связи [1, 2].
В первых рекуррентных ИНС главной идеей было обучение своему выходному сигналу на предыдущем шаге.
Рекуррентные сети реализуют нелинейные модели, которые могут быть применены для оптимального управления процессами, изменяющимися во времени, то есть обратные связи позволяют обеспечить адаптивное запоминание прошлых временных событий.
Обобщение рекуррентных ИНС позволит создать более гибкий инструмент для построения нелинейных моделей.
Рассмотрим некоторые архитектуры рекуррентных ИНС.
В основе сети Джордана лежит многослойный персептрон.
Обратная связь реализуется через подачу на входной слой не только исходных данных, но и сигналов выхода сети с задержкой на один или несколько тактов, что позволяет учесть предысторию наблюдаемых процессов и накопить информацию для выработки правильной стратегии управления [35].
Сеть Элмана, как и сеть Джордана, получается из многослойного персептрона введением обратных связей.
Только сигналы на входной слой идут не от выходов сети, а от выходов нейронов скрытого слоя [35].
Для обобщения рекуррентных ИНС в статье предлагается добавить задержку сигналов обратной связи скрытого слоя на несколько тактов.
Для этого добавим у слоя динамическую стековую память.
Пример архитектуры такой ИНС показан на рисунке 1.
Количество временных задержек будем изменять от 1 до.
При детальном рассмотрении архитектуры рекуррентной сети видно, что обратные связи от скрытого слоя или от выхода сети можно исключить путем добавления в обучающую выборку сигналов обратной связи.
Рассмотрим процесс трансформации обучающей выборки для решения задачи прогнозирования временного ряда с помощью рекуррентной ИНС с динамической стековой памятью.
В качестве примера будем использовать среднемесячные значения плотности потока солнечного излучения на длине волны 10,7 за 20102012 гг.[6].
Трансформируем временной ряд методом скользящих окон [7], как показано в таблице 2.
Пусть в рекуррентной ИНС скрытый слой содержит три нейрона, выходной один нейрон, стек динамической памяти обратные сигналы скрытого слоя с задержкой на два такта.
Так как число нейронов скрытого слоя, имеющих обратную связь с входным слоем, равно трем, размер входного вектора во время обучения ИНС при запоминании предыдущего выходного сигнала на один шаг назад увеличится на три, при запоминании двух предыдущих выходных сигналов на шесть.
В таблице 3 приведена трансформированная обучающая выборка.
Таким образом, обучение рекуррентной ИНС с динамической стековой памятью методом обратного распространения ошибки можно свести к обучению многослойного персептрона [8], трансформируя обучающую выборку.
Для реализации предлагаемой методологии обучения рекуррентной ИНС с динамической стековой памятью расширены возможности нейроэмулятора NeuroNADS [9].
Объектно-ориентированная модель рекуррентной ИНС с динамической стековой памятью представлена на диаграмме классов.
В отличие от класса Layer, который является контейнером для нейронов многослойного персептрона, класс LayerMemory cодержит память stackOut, реализованную в виде стека предыдущих сигналов слоя.
Размер стека задается с помощью свойства stackSize.
Спрогнозируем среднемесячную плотность солнечной активности на длине волны 10,7 см на первые шесть месяцев 2012 г.на основе данных за 20102011 гг.из таблицы 1.
Для этого построим и обучим рекуррентную ИНС с динамической стековой памятью с помощью нейроэмулятора NeuroNADS.
Первые 24 примера временного ряда возьмем для обучающей выборки, а оставшиеся шесть примеров для тестовой выборки.
Обучение проведем гибридным алгоритмом [10].
Параметры алгоритма: шаг обучения 0,3, максимальное количество особей в поколении 10, коэффициент мутации 0,1.
Критерии остановки обучения: среднеквадратическая ошибка 0,001, количество эпох 1 000.
По результатам обучения можно сделать вывод, что рекуррентная ИНС с динамической стековой памятью справилась с задачей, показатели ошибок прогнозирования временного ряда соответствуют допустимым значениям.
Таким образом, рекуррентные ИНС с динамической стековой памятью можно обучать с помощью предложенной методологии, а построенные модели ИНС использовать для прогнозирования временных рядов.
