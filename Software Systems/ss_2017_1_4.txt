:519.816.

Методы обучения с подкреплением (reinforcement learning, RL) [1], основанные на использова-
нии большого количества информации для обучения в произвольной окружающей среде, являются
одной из наиболее активно развиваемых областей искусственного интеллекта, связанных с разработ-
кой перспективных интеллектуальных систем реального времени (ИС РВ), типичными примерами
которых являются интеллектуальные системы поддержки принятия решений реального времени
(ИСППР РВ) [2, 3].
Одним из наиболее перспективных в плане использования в ИС РВ, относящихся к классу динамических интеллектуальных систем [4, 5], является обучение на основе темпоральных различий  [1], когда процесс обучения основывается непосредственно на получаемом опыте без предварительных знаний о модели поведения окружающей среды.
Ключевой особенностью TD-алгоритмов является обучение на основе различий во временных последовательных предсказаниях.
TD-методы, предназначенные для многомерных временных рядов, способны обновлять расчетные оценки, основанные в том числе и на других полученных оценках, не дожидаясь окончательного результата, то есть являются самонастраиваемыми.
Последнее свойство весьма важно для ИС семиотического типа, способных адаптироваться  к изменениям в управляемом объекте и/или окружающей среде [3].
При разработке современных ИС РВ большое внимание должно быть уделено также средствам прогнозирования развития ситуации на объекте и последствий принимаемых решений, экспертным методам и средствам обучения [3, 7].
Эти средства необходимы для модификации и адаптации ИС РВ к изменениям на объекте и во внешней среде, а также для расширения области применения и улучшения эффективности функционирования систем.
Далее будет дан анализ ряда методов обучения с подкреплением, в частности TD-методов, в плане их последующей интеграции в инструментальную среду для ИС РВ типа ИСППР РВ с применением мультиагентного подхода.
В функции RL-обучения входит адаптация немарковской модели принятия решений к сложившейся ситуации за счет анализа предыстории процесса принятия решений, вследствие чего повышается качество принимаемых решений [1, 8, 9].
В RL-обучении модуль принятия решений, способный посредством взаимодействия с внешней средой и анализа оценочной функции  корректировать стратегию принятия решений, называется.
Задачей агента является нахождение в процессе обучения оптимальной  или приемлемой  стратегии принятия решений, называемой также.
Интеллектуальный агент должен уметь поддерживать несколько путей обучения и адаптировать накопленный опыт к изменениям в окружающей среде.
В RL-обучении взаимодействие агентокружающая среда моделируется посредством контроллера, связывающего ИС и среду.
Процесс восприятия отображает состояния среды  во внутренние представления агента, а процесс воздействия отображает предлагаемые агентом воздействия в действия  внешней среды.
Обобщенная схема взаимодействия агентокружающая среда приведена на рисунке 1.
Данный подход применим в прикладных задачах, когда завершающий шаг можно определить естественным образом исходя из природы решаемой задачи, то есть когда взаимодействие агентокружающая среда можно разбить на последовательности, называемые.
Основной проблемой RL-обучения является нахождение агентом компромисса между изучением и применением.
Для получения большего вознаграждения агент должен предпочитать действия, ранее уже применявшиеся и показавшие свою эффективность с точки зрения получения поощрения.
С другой стороны, чтобы обнаруживать такие действия, агенту необходимо пробовать выполнять новые действия.
Таким образом, агент должен как применять уже известные действия, так и изучать новые для возможности иметь наилучший выбор в будущем.
Важной характеристикой RL-обучения является получение отложенных вознаграждений, которые имеют место в сложных динамических системах.
Это означает, что действие агента может повлиять не только на текущую награду, но и на все последующие.
В плане применения в ИС РВ TD-методы могут решать несколько задач: значений некоторых переменных в течение нескольких временных шагов и, основанную на RL-обучении агента тому, как влиять на окружающую среду.
Таким образом, агент должен предсказывать последующие состояния окружающей среды и использовать эти значения для ее изменения с целью максимизации вознаграждений.
Для возможности обучения и адаптации к изменениям внешней среды агент должен обладать памятью для хранения предыстории.
При этом возникает ряд проблем, связанных с объемом доступной агенту информации о прошлом, с запоминанием, хранением, использованием доступной информации и т.д.
Для решения этих проблем агент может использовать скользящее окно для истории  или строить зависящую от состояния прогнозную модель окружающей среды.
Можно применить комбинацию этих подходов, когда агент анализирует чувствительную к предыстории политику непосредственно при обучении.
TD-методы для решения задачи предсказания используют имеющийся опыт.
Так как в своих корректировках TD-метод частично основывается на существующих оценках, он является Одним из преимуществ TD-методов является то, что они не требуют знания модели окружающей среды с ее вознаграждениями и вероятностным распределением последующих состояний.
Данное преимущество TD-методов часто имеет решающее значение при использовании в ИС РВ, так как в некоторых ситуациях эпизоды могут быть настолько продолжительными, что задержки процесса обучения, связанные с необходимостью завершения эпизодов, будут слишком велики.
Возможны также ситуации, когда возникают непрерывные задачи, а эпизоды как таковые отсутствуют.
TD-методы обучаются на основе каждого перехода вне зависимости от осуществляемых в дальнейшем действий и, соответственно, не чувствительны к ситуациям, когда необходимо игнорировать эпизоды или снижать значимость эпизодов, в которых предпринимаются экспериментальные действия, что может сильно замедлить обучение.
TD-методы в целом можно разделить на две основные категории методы с интегрированной  и методы с разделенной  оценкой ценности стратегий.
В методах с интегрированной оценкой стратегия, используемая для управления, аналогична оценочной стратегии, которая совершенствуется во время обучения.
В методах с разделенной оценкой стратегия управления не имеет взаимосвязи с оценочной стратегией.
TD-метод с интегрированной оценкой ценности стратегий.
Данная корректировка имеет место после каждого перехода из нетерминального состояния.
В методах с интегрированной оценкой ценности стратегий все время оценивается функция для стратегии поведения и в то же время стратегия делается более жадной по отношению к.
Свойство сходимости алгоритма SARSA непосредственно связано с зависимостью стратегии от функции Например, можно использоватьстратегии.
метод с разделенной оценкой ценности стратегий, который находит оптимальные значения функции для выбора последующих действий и одновременно определяет оптимальную стратегию.
Аналогично методу TD в каждой итерации есть только знание о двух состояниях: и одного из его предшествующих.
Таким образом, значения функции позволяют получить некоторое представление о будущем качестве действий в предшествующих состояниях и сделать задачу выбора действия проще.
Одношаговоесостояния.
В этом случае искомая функция ценности действия непосредственно аппроксимирует оптимальную функцию ценности действия независимо от применяющейся стратегии.
Стратегия определяет, какие пары состояниедействие посещаются и корректируются.
Всовершенствуется.
Например, для генерирования действий может быть использована стратегия с равномерным распределением по пространству действий.
Для обеспечения сходимости необходимо, чтобы все пары продолжали корректироваться.
Это является минимальным требованием в том смысле, что каждый метод, гарантированно находящий оптимальную линию поведения, в общем случае должен удовлетворять данному условию.
Установлено, что при таком условии и в случае стохастической аппроксимации для последовательности значений длины шага функция сходится к с вероятностью 1 [1].
метод, в котором временное различие имеет протяженность в шагов.
Вводится дополнительная переменная памяти, соответствующая каждому состоянию,.
Такие следы показывают степень приемлемости каждого состояния при происходящих изменениях в обучении, если возникает подкрепляющее событие.
Используя следы приемлемости, все состояния должны быть обновлены на каждом шаге .
В то же время информация о текущих выплатах распространяется обратно к состояниям с более высокими значениями следов приемлемости.
Структура мультиагентной системы для RL-обучения аналогична рассмотренной ранее схеме взаимодействия агентокружающая среда с отличием в том, что на окружающую среду оказывают влияние несколько агентов одновременно и, соответственно, действия каждого агента могут зависеть от действий остальных агентов системы.
К преимуществам мультиагентных систем в RL-обучении можно отнести следующие: возможность параллельных вычислений, так как используется распределенный характер взаимодействия агентов, в рамках ускорения работы системы; обмен опытом между агентами, средствами обучения и имитации, позволяющий помочь RLагентам с похожими задачами обучаться быстрее и достичь более высокой производительности; отказоустойчивость при выводе из строя одного или несколько агентов система продолжает функционировать; масштабируемость включение или исключение агента из системы не влияет на работу системы в целом.
Но при этом возникают определенные сложности: сложность задания цели обучения; нестационарность проблемы обучения, возникающая из-за того, что все агенты обучаются одновременно и каждый агент сталкивается с проблемой изменяющейся цели обучения, поэтому основная стратегия может меняться при изменении стратегий других агентов; таким образом, RL-агенту необходимо найти компромисс между использованием текущих знаний и исследованием среды для сбора информации и улучшения этих знаний; необходимость координации; экспоненциальный рост дискретного пространства состояний-действий, так как основной алгоритм Q-обучения оценивает значения всех возможных пар состояниядействия, что ведет, соответственно, к экспоненциальному увеличению вычислительной сложности.
Затем полученный прогноз корректируется относительно значений ряда, полученного на основе метода экспоненциального сглаживания, а далее с учетом экспертных методов ранжирования и непосредственной оценки.
Вероятность каждого исхода, полученного статистическими методами, корректируется  в зависимости от значений экспертных оценок для указанных исходов.
Выполнена программная реализация прототипа подсистемы прогнозирования с использованием статистического и экспертного модулей для решения задач экспертного диагностирования сложного технологического объекта одной из подсистем АЭС  с целью выполнения прогнозирования для оценки развития ситуации на объекте [10].
По результатам тестирования установлено, что необходимо привлечение дополнительных средств: методов RL-обучения на основе темпоральных различий, которые позволяют выявить имеющиеся закономерности посредством анализа предыстории процесса и таким образом уменьшить влияние случайных явлений.
Особое внимание уделено методам на основе темпоральных различий .
Предложен комбинированный метод прогнозирования, основанный на статистических и экспертных методах прогнозирования, и реализованы алгоритмы для комбинированного метода.
Предложена архитектура подсистемы прогнозирования, включающая модуль прогнозирования, мультиагентный модуль RL-обучения и модуль анализа и принятия решений.
В настоящее время разрабатывается мультиагентный модуль RL-обучения для его включения в интегрированную среду, ориентированную на использование в ИСППР РВ семиотического типа, с целью расширения области применения, повышения производительности и эффективности функционирования современных ИСППР РВ.
