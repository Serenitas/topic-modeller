
Проблема построения эффективных вычислительных систем, решающих разнообразные задачи от научных до военных, упирается в доставку данных до вычислительных компонент: будь то центральный процессор, специализированная интегральная схема или графический ускоритель.
В то время как на аппаратном уровне идет бурное развитие различных сетевых идеологий, с программной точки зрения самым распространенным средством передачи данных для многопроцессорных комплексов является интерфейс передачи сообщений  [1].
Разработанная в НИИСИ РАН микросхема 1890ВМ6Я, являющаяся микросхемой общего назначения, имеет в своем составе, помимо всего прочего, центральный процессор, мощный математический сопроцессор и контроллер RapidIO [2].
Коммутатор RapidIO 1890КП3, также разработанный в НИИСИ РАН, позволяет объединить эти микросхемы в сеть RapidIO и сформировать высокопроизводительный многопроцессорный вычислительный комплекс.
Далее в данной статье представлена реализация MPI для вычислительного комплекса на базе микросхем семейства 1890ВМ6Я с возможностью исполнения операций приема/передачи данных по RapidIO без копирования .
При решении задачи запуска MPI-приложений на вычислительном комплексе возникает один из главных вопросов рабочее окружение, состоящее из таких компонентов, как ядро ОС и набор библиотек.
В качестве ядра ОС было выбрано ядро Linux, так как оно в связке с библиотеками MPI показало свою эффективность в распределенных вычислениях [35], а в качестве библиотеки MPI MPICH как одна из широко используемых реализаций MPI [6].
В верхней части библиотеки находится совместимый со стандартом MPI версии 2.2 интерфейс, предоставляемый прикладным программам.
Ниже по уровню располагается логическое устройство, реализующее абстрактный интерфейс устройства .
Портирование MPICH для определенной коммуникационной среды может заключаться в реализации такого ADI3-устройства (например, на рисунке 1 это CH3 Device).
Данный подход позволяет наилучшим образом наложить интерфейс библиотеки на конкретное оборудование, однако процесс разработки и сопровождения полного ADI3устройства может быть весьма трудоемким из-за своей сложности и постоянного развития верхнего уровня .
Другой подход заключается в разработке устройства более низкого уровня, взаимодействующего с имеющимся в библиотеке MPICH ADI3 устройством  по интерфейсу CH3I.
Устройства такого уровня называются каналами.
Естественно, добавление нового уровня абстракции немного снижает производительность, но разработка и сопровождение такого канала требуют значительно меньше трудовых затрат.
Однако практика показала, что разработчики вместо реализации каналов для конкретного оборудования перерабатывали само CH3-устройство.
Стало ясно, что интерфейс CH3I не удовлетворял потребностям современных на то время технологий.
Отсюда возникла необходимость в создании еще одного слоя абстракции, позволяющего стабилизировать CH3Iинтерфейс.
Таким образом, был представлен канал Nemesis для CH3-устройства [9].
Ключевыми особенностями Nemesis являются масштабируемость, высокая производительность для внутриузловых и межузловых обменов, а также поддержка разнородных сетевых каналов передачи.
Базовой функцией сетевого модуля является передача служебных сообщений и массивов данных через коммуникационную среду.
Рассмотрим процесс разработки стека RapidIO от драйвера контроллера RapidIO до реализации сетевого модуля для Nemesis и обоснуем принятые решения.
Дверные звонки могут передать максимум 2 байта данных за одну транзакцию.
Они предназначены, например, для оповещения о случившемся прерывании, а не для передачи массивов данных.
Использование IO-транзакций  возможно путем выполнения процессорных инструкций чтения/записи специальной транслируемой области памяти, которая указывает на память удаленного узла.
Каждая такая инструкция может либо загрузить, либо записать данные размером до 8 байт.
Сообщения позволяют передавать до 4 Кб данных.
Каждое сообщение состоит из сегментов, количество которых может достигать 16, а размер каждого из них не превышает 256 байт.
По спецификации RapidIO гарантируются доставка сообщений до места назначения и сохранение порядка передачи с помощью полей номер письма  и номер сегмента в заголовке.
Оно также имеет дополнительное поле  номер почтового ящика, который служит для разделения входящих потоков сообщений.
Таким образом, сообщения являются наиболее приемлемым способом передачи данных ввиду своей высокой эффективности и необходимости наименьшего числа транзакций для данных размером более 8 байт.
Далее по тексту под буфером будет пониматься область памяти, разбитая на сообщения либо на отправку, либо на прием.
Контроллер сообщений RapidIO микросхемы 1890ВМ6Я имеет ряд особенностей.
8 выделенных очередей, программируемых на прием сообщений от заданного узла с заданным номером почтового ящика.
С помощью механизма выделенных очередей становится возможным прием данных напрямую в буфер назначения без промежуточных копирований данных .
Такой прием назовем приемом ожидаемого пакета.
Наличие общей очереди, куда попадают сообщения, не попавшие ни в одну из выделенных очередей.
Такие входящие сообщения будем называть неожиданными.
Проблема сегментов разной длины.
Отправитель посылает сегменты один за другим, одновременно ожидая статуса доставки каждого сегмента от узла получателя.
При возникновении заторов в коммуникационной среде любой сегмент может быть отклонен получателем, что приводит к повторной отправке недоставленного сегмента.
Отсюда возможна следующая ситуация, когда все сегменты, кроме последнего, были отклонены получателем.
Тогда этот успешно полученный последний сегмент расположится в памяти по адресу.
Если же размер всех предыдущих сегментов меньше 256 байт, можно с уверенностью сказать, что произошла порча памяти у получателя в момент записи последнего сегмента, если, конечно, буфер был рассчитан только на прием действительного размера сообщения.
В качестве простого решения этой проблемы введем ограничение выравненности размера буферов отправки и приема на 256 байт.
Физический адрес буфера должен быть выравнен на 8 байт.
Из предыдущего требования вытекает требование выравненности адреса буфера, лежащего в транслируемой виртуальной памяти, на 256 байт.
Но логика работы контроллера RapidIO микросхемы 1890ВМ6Я при приеме в выделенную очередь заключается в том, что количество отправленных сообщений и размер каждого из них должны точно совпадать с числом и размерами запрограммированных сообщений на стороне получателя.
Поэтому для упрощения увеличим требование выравненности для адреса такого буфера до 4Кб.
Для передачи сообщения на уровне прикладной программы, использующей MPI, на первом узле необходимо вызвать функцию передачи MPI_Send, а на другом функцию приема MPI_Recv.
Для MPI_Recv должны быть заданы соответствующие параметры, которые либо точно совпадают с параметрами отправки ID и TAG, либо представляют собой особые значения MPI_ANY_SOURCE для ID или MPI_ANY_TAG для TAG, которые пропускают любые соответствующие значения.
Такой парный подход вызовов MPI_Send/MPI_Recv для приема/передачи данных позволяет воспользоваться выделенными очередями контроллера RapidIO микросхемы 1890ВМ6Я для приема и последующей отправки в них данных без копирования.
После успешного открытия выделенной очереди информация о номере почтового ящика должна быть передана узлу-отправителю.
Для передачи такого рода контрольной информации зарезервируем нулевой почтовый ящик, а остальные номера предоставим выделенным очередям.
Все эти функции должны работать с допустимыми  для работы контроллера RapidIO буферами.
Для обработки неожиданных сообщений драйвер при инициализации открывает общую очередь.
Эта работа отнесена на отдельный управляющий уровень.
При недостатке свободных очередей или почтовых ящиков запросы на выделенную очередь откладываются до тех пор, пока не станет доступным недостающая часть.
Для приема или передачи буфера без копирования необходимо, чтобы его физический адрес и размер удовлетворяли введенным требованиям выравненности.
При этом, если буфер находится в транслируемой виртуальной области памяти, осуществляется преобразование виртуального адреса в физические адреса, при необходимости с подкачкой страниц памяти с внешнего носителя.
В случае неудовлетворения введенным требованиям выделяется правильный связующий буфер  для обмена данными  с применением копирования.
Прием в выделенную очередь  происходит только после открытия выделенной очереди и получения контрольной информации о выделенном номере почтового ящика отправителем.
Чтобы отличить маленькие данные от контрольной информации, необходимо передавать заголовок с типом данных и их размером вместе с предполагаемыми данными.
Граница между маленькими и большими данными сильно зависит от самой аппаратуры, поэтому устанавливается эмпирическим путем.
Если по результатам измерения эта граница расположилась за пределами 4 Кб, то данные маленького размера, превышающие 4 Кб, будут разбиты на несколько сообщений RapidIO драйвером контроллера.
Для последующей обработки таких данных на узле-получателе необходимо их все собрать в единый пакет, что возможно благодаря введенному заголовку.
В общем случае пользователей управляющего интерфейса может быть несколько.
Основная проблема заключается в распознавании пользователя-получателя неожиданных пакетов, так как для ожидаемых пакетов получатель известен по определению.
Для ее решения в заголовке неожиданного пакета необходимо передавать идентификатор пользователя-получателя.
Для этого введем еще одну интерфейсную функцию-регистрацию пользователя-получателя, которая формирует рабочий контекст, и постулируем работу функций управляющего уровня в рамках этого контекста.
Стек Ethernet является базовой необходимостью, так как с его помощью работают многие полезные протоколы, например telnet и ssh.
Другим пользователем является уровень MX , который реализует интерфейс передачи сообщений по значениям ключ-маска, в достаточно хорошей степени точности соответствующий интерфейсу передачи сообщений MPI, описанному выше.
Интерфейс Myrinet eXpress [12], использующийся в сетевом модуле mx библиотеки MPICH, также соответствует интерфейсу передачи сообщений MPI.
Этот сетевой модуль был взят как справочник при реализации библиотеки пользовательского уровня MXlib, которая реализует интерфейс Myrinet eXpress для работы с ядерным уровнем MX.
С этой библиотекой стала возможной работа библиотеки MPICH поверх канала RapidIO через использование сетевого модуля mx.
В качестве аппаратуры для измерения производительности MPI были взяты 4 ПЛИС с ядром микросхемы семейства 1890ВМ6Я, соединенные каналами RapidIO, пиковая пропускная способность которого составляет 192 МБ/с.
На одном, двух и четырех узлах были запущены тесты NAS Parallel Benchmarks  [13] класса А, которые достигли 80 %-ной эффективности масштабирования при увеличении количества узлов.
Также была измерена производительность между двумя узлами тестами OSU Micro-Benchmarks [14], результаты которых представлены в таблице.
На основании изложенного можно сделать следующие выводы.
Реализованный в данной статье стек RapidIO для MPI показал свою работоспособность на различных программах тесты NAS Parallel Benchmarks, OSU Micro-Benchmarks, тесты MPICH.
На тесте передачи сообщений OSU удалось достичь скорости передачи в 84 % от пиковой.
Благодаря расширению области тестирования путем запуска разнообразных MPI-программ были выявлены и устранены ранее неизвестные ошибки в логике работы контроллера RapidIO.
При использовании готовых библиотек добиться максимальной производительности можно только на аппаратуре, функциональные возможности которой соответствуют идеологии обмена данными, заложенной в самой библиотеке.
В целом контроллер RapidIO микросхемы 1890ВМ6Я лишь частично соответствует стандарту MPI, а соответственно, и его библиотечным реализациям.
Так, например, для передачи большого массива данных необходимо произвести ручное программирование каждого сообщения в соответствии с таблицей физических адресов, в то время как такая тривиальная работа может быть реализована в железе.
Необходимость разрешения конфликтов выравненности буферов на обоих узлах также влечет дополнительные накладные расходы на процессор, в худшем случае прибегая к копированию данных.
Микросхема 1890ВМ6Я предоставляет весьма скудный механизм такого удаленного доступа к памяти IO-транзакции типа NREAD и NWRITE с полезной нагрузкой до 8 байт, хотя по стандарту RapidIO полезная нагрузка может составлять 256 байт.
Недостатком является и получившаяся сильная перегруженность стека драйверов, что делает отправку и прием данных слишком дорогой операцией.
По грубым оценкам, получилось больше 20 тысяч процессорных инструкций для передачи всего лишь одного байта данных.
Это означает, что при частоте процессора в 1 ГГц передать один байт получится не меньше, чем через несколько десятков микросекунд.
Конечно, с ростом передаваемого объема данных сама подготовка к передаче будет становиться исчезающе малой, но факт задержки, тем не менее, останется.
Поэтому необходима дальнейшая работа по ее уменьшению.
This paper presents MPI implementation for SoC 1890VM6IA developed in SRISA RAS.
It is a base for multipurpose multiprocessor computing systems, nodes of which can be connected through high speed RapidIO channels.
To implement it we took MPICH, which is the most widespread library of MPI, and Linux kernel that proved itself as the best solution for supercomputing.
Using network module mx, which is well-fitting for RapidIO architecture, gave the opportunity to focus only on the development of the Linux driver for RapidIO controller.
The features of RapidIO controller of 1890VM6IA, MPI interface and the idea of using RapidIO channel for purposes other than MPI in Linux, allowed implementing a message passing stack with the some degree of universality using RapidIO channel without unnecessary data copying.
In the end of the paper there are the results of different MPI test programs such as NAS Parallel Benchmarks and OSU Micro-Benchmarks for 4 nodes using the RapidIO channels.
The author made a conclusion and summarized the results using RapidIO controller of 1890VM6IA SoC as a transport facility for MPI.
2012.
Design of high reliability multiprocessor modules based on high-performance RapidIO interconnect architecture.
2013, no.
4, pp.
MilkyWay-2 supercomputer: system and application..
2014, no.
345356.
Performance Evaluation of Open MPI on Cray XE/XK Systems..
2012, pp.
4047.
Performance Evaluation of OpenMP and MPI Hybrid Programs on a Large Scale Multi-core Multi-socket Cluster, T2K Open Supercomputer..
2009, pp.
206213..
[MPICH library architecture, the 1st vrsion].
A uGNI-Based MPICH2 Nemesis Network Module for Cray XE Computer Systems.
2011, pp.
110119.
Design and Evaluation of Nemesis, a Scalable, Low-Latency, MessagePassing Communication Subsystem..
2006, pp.
521530.
RapidIO Trade Association.
2011.
RapidIO Trade Association.
2011.
1.
Кулешов А.С.
2015.
4 .
С.
9398.
2.
Кулешов А.С.
2015; -235X.112.093-098.
3.
Kuleshov A.S.
MPI support in Linux OS for RapidIO-based multiprocessor computer systems.
2015, no.
