Препроцессорная обработка множеств прецедентов для построения решающих функций в задачах классификации



В последнее время интенсивно развивается группа методов под общим названием Data Mining, ориентированных прежде всего на обработку массивов данных значительного размера, содержащих информацию об интересующих объектах, с целью извлечения из них сведений требуемого вида . Как правило, на первом этапе исследования выделяемых объектов решается задача классификации, в которой каждый такой экземпляр должен быть отнесен к какому-либо из уже известных непересекающихся классов объектов  из общей совокупности. Обычно индивидуальные свойства исследуемых объектов выражают через значения их существенных характерных числовых признаков. При этом каждому исследуемому объекту с номером s взаимно-однозначно соответствует своя точка в n-мерном пространстве значений признаков U, на котором введена некоторая мера . Такой подход позволяет геометрически иллюстрировать методы классификации в n-мерных метрических пространствах. В частности, в работах  для построения классификатора предложено использовать специальное бинарное дерево решений, у которого в качестве узловых правил используются гиперплоскости в U, нормальные к межцентровым векторам разделяемых множеств.

Наряду с задачей формирования адекватного набора признаков , передающего все существенные характеристики исследуемых объектов, значительную проблему при получении исходных данных о реальных системах создают погрешности в получаемых элементах информационного массива, вызванные ошибками измерения, помехами и шумами. Поэтому для результативного обследования реальной системы с целью извлечения информации о некоторых объектах, содержащихся в ней, необходимо выполнение следующих двух основных условий: правильное выделение набора числовых признаков , адекватно характеризующих все существенные свойства исследуемых объектов, и учет влияния погрешностей на получаемые исходные данные задачи классификации.

Обработка зашумленных данных является многогранной проблемой, решаемой как в статистических методах, так и в задачах искусственного интеллекта. Предварительный анализ экспериментальных данных, имеющих вид случайной величины, сделан в  рассматриваются вопросы моделирования рассуждений на основе прецедентов и классификации в условиях таких данных в интеллектуальных системах поддержки принятия решений. В работах  представлено несколько расчетных схем, позволяющих выделять детерминированные компоненты из временных рядов с аддитивной хаотической погрешностью.

В данной статье предлагаются методы, позволяющие на основе геометрической интерпретации задачи классификации не только проанализировать качество обучающей выборки, но и выявить возможные причины ошибок, содержащихся в ней, выполнить их коррекцию, необходимую для построения эффективного классификатора.

В задаче классификации объект должен быть отнесен к какому-либо из уже известных непересекающихся классов объектов, входящих в их совокупность . Смысловой вариант определения задачи классификации нового, ранее неизвестного объекта относительно выделенной на U совокупности классов  заключается в выяснении включения в один из классов данной совокупности.

Ответом в задаче является номер i класса , в который включается объект at . Математическую модель (формульную, алгоритмическую и др.), решающую задачу  для любой заданной точки, называют классификатором либо решающей функцией. Он задает отображение вида.

В реальных задачах для заданной совокупности классов  решающая функция  формируется на основе обучения – обработки набора прецедентов (обучающей выборки)  – совокупности пар вида , где . В них не только заданы координаты некоторой задающей объект точки, но и явно при помощи методов из предметной области указано, в какой из классов  входит данная точка (и соответствующий ей объект). Обозначим общий объем обучающей выборки через NN, а число объектов в классах из .

Одним из необходимых условий успешного применения геометрического подхода к построению классификатора, основанного на соответствующей интерпретации пространства признаков U, является выполнение гипотезы компактности, по которой отдельным классам, содержащим близкие по свойствам объекты, в геометрическом пространстве значений признаков U соответствуют обособленные непересекающиеся сгустки (классы), которые могут быть разделены в пространстве U достаточно простыми гиперповерхностями.

Допустим, требуется разработать автоматический классификатор , решающий задачу  на основании обучающей выборки Р, представляющей собой набор проб, образцов с определенной какими-либо методами из предметной области их принадлежностью к классам. Ошибки в выделении набора числовых признаков  , шумы в исходных данных приводят к тому, что в прецедентах нарушается правильность отображения  в задаче , то есть точке as может быть сопоставлен неверный класс cls. Назовем такую ситуацию выбросом. Наличие выбросов в обучающей выборке Р дает существенное нарушение гипотезы компактности и значительно затрудняет как построение решающей функции , так и последующее решение задачи классификации.

Вспомогательные обозначения. Постановка задачи 

Для численного учета общей доли выбросов в выборке Р предложено использовать две пороговые величины: предельно допустимая доля удаляемых выбросов, при которой можно без ущерба для общей информативности выборки Р удалить из нее все выбросы; предельно допустимая доля корректируемых выбросов, при превышении которой выборку Р уже нельзя считать достаточно информативной для решения задачи.

Поскольку объемы NN обучающих выборок и цели классификации существенно различаются для задач из разных предметных областей, к заданию пороговых величин  и  следует применять дифференцированный подход.

Например, при  в задачах дефектоскопии, обработки кадастровых и геологических данных предельно допустимое число отбрасываемых прецедентов можно принять равным , а предельно допустимую долю ошибок  .

При в задачах классификации компонентов тканей организмов (биотехнология, медицина).

При наличии выбросов в Р, помимо неадекватного отображения реальной картины, построение классификатора приводит к следующим негативным последствиям:  практическая невозможность полного разделения набора прецедентов по заданной совокупности классов , что характерно для нейросетевых методов классификации;  слишком сложная структура классификатора, получаемого в более универсальных методах, полностью решающих задачу разделения классов.

Для предотвращения данных ситуаций наиболее приемлемы либо повторное аппаратно-программное исследование реальной системы (что зачастую сложно либо вообще невозможно выполнить), либо препроцессорный анализ обучающей выборки Р с целью обнаружения выбросов с последующей коррекцией Р с учетом обеих основных причин возникновения выбросов – выделения набора адекватных числовых признаков  и учета влияния погрешностей при формировании Р.

Рассмотрим один из возможных путей практической реализации автоматической препроцессорной обработки обучающей выборки.

Анализ прецедентов 

Допустим, некий объект из обучающей выборки Р задан точкой  , которая изначально включена в класс. Требуется проверить правомерность включения точки  в класс  (с использованием пространства U и его меры ), а не в какой-либо другой класс из совокупности , то есть проверить, не будет ли точка  выбросом.

Для этого предложено использовать специальную меру близости объекта  к произвольному классу Aq. Ее наиболее удобной функциональной реализацией представляется мера близости точки  и множества точек Aq, аналогичная методу ближайшего соседа с той разницей, что соседство определяется не по одной ближайшей точке, а по нескольким (s) ближайшим.

С целью сокращения общего числа операций при расчете минимума в расстоянии для сортировки расстояний предложено использовать дискретизацию значений расстояний с точностью до 0,05 % с кодированием их целыми числами от 0 до 1 000, которые используются в качестве индексов вспомогательного линейного массива V. Для быстрого выполнения такого упорядочения надо использовать блочную (карманную, корзинную) сортировку.

В том случае, когда точка  пространства U геометрически наиболее близка к точкам из множества, теоретически должно выполняться следующее идеальное соотношение.

В практических расчетах минимум (или очень близкая к нему величина) в формуле  может достигаться не на одном классе: наряду с классом  такая же степень близости R может достигаться и на другом классе из совокупности . При этом в зависимости от последовательности анализа, а также погрешности вычислений возможно принятие другого класса, отличного от , в качестве ближайшего к  . Для устранения влияния неединственности решения  и погрешности расчетов в качестве проверочного условия включения  в  предложена следующая формула, использующая  явно выделенный минимум степеней близости Мk,  где  – достаточно малое вещественное число. При таком определении все классы равноправны при определении их близости к точке  .

Поскольку включение объекта  в , как правило, неслучайно и в большинстве случаев является обоснованным, для подтверждения предпочтительности включения  в  по сравнению с другими классами необходимо брать повышенные значения . Практически выбор  зависит от решаемой задачи классификации, размерности n и выбранной меры  пространства значений признаков. По результатам расчета примеров в качестве средних значений  для основных вариантов меры  предложено принять следующие величины, обеспечивающие примерно равные допустимые отклонения для фиксированной обучающей выборки:  квадрат евклидова расстояния;  евклидово расстояние;  манхэттенское расстояние.  

При выполнении условий – начальный вариант включения принимается, иначе – нет. При невыполнении условий в качестве корректирующего класса принимается тот класс Ar, на котором достигается явно выделенный минимум Мk.

Соотношения (6)–(7) позволяют разработать на их основе алгоритм анализа обучающей выборки для последующего построения классификатора для совокупности классов. Он заключается в последовательном переборе всех прецедентов, для каждого из которых вначале рассчитываются все расстояния, затем определяется их минимум Мk и проверяется условие . Если оно выполнено, то начальное приписывание точке as класса cls подтверждается. В противном случае прецедент рs считается выбросом из обучающей выборки, в качестве корректирующего класса принимается тот класс, на котором достигается явно выделенный минимум Мk.

В двух выходных целочисленных массивах алгоритма анализа NV и NCOR соответственно запоминаются номера s выбросов в обучающей выборке Р и номера корректирующих классов для них. Параллельно с формированием массивов NV и NCOR рассчитывается доля ошибочных начальных присвоений объектов классам (выбросов), которую обозначим через.

Выяснение принципиальной возможности коррекции выборки P производится путем проверки условия,  где  – введенная выше предельно допустимая доля корректируемых выбросов.

Приведем алгоритм анализа обучающей выборки, в котором применяется матрица расстояний между точками.

Исходные данные алгоритма:  число k выделенных классов в пространстве значений признаков;  массив  чисел объектов в классах ;  количество s ближайших точек в классе Ak к точке as , по которым вычисляется ;  NN – общий объем обучающей выборки P;  n – число характерных признаков объектов;  массив координат точек, объектов из P, упорядоченных по принадлежности к классам;  массив классов объектов из P; коэффициент запаса при проверке расстояний; предельно допустимая доля корректируемых выбросов.

Решаемая задача: анализ выбросов в обучающей выборке.

Выходные данные:  COR – логическая переменная, равная true, если коррекция P возможна, и false – иначе;  общее число Nв выбросов в обучающей выборке;  доля (delta) выбросов в обучающей выборке;  массив номеров выбросов в обучающей выборке;  массив  номеров корректирующих классов для выбросов в обучающей выборке.

В описании алгоритма номера прецедентов обозначены через No, классов – через Nc.

Завершение работы алгоритма.

Если в результате анализа выборки получено значение, необходима коррекция набора признаков  , а возможно, и аппаратных средств исследования.

Если же выполнено условие, можно считать, что в этом случае числовые признаки  адекватно отражают свойства разделяемых классов объектов и Р задает удовлетворительный по качеству массив данных. Затем выполняется переход ко второму этапу, на котором учитывается влияние погрешностей при формировании Р.

Оценим сложность алгоритма анализа обучающей выборки. Сложность расчета расстояний определяется метрикой на пространстве значений признаков U. Общее количество данных расчетов в алгоритме равно. В алгоритме используются следующие элементарные операции: присваивание, проверка простых условий, сложение и вычитание (в процессорах с плавающей запятой выполняются сходно), умножение, деление. Поскольку алгоритм адаптивный, действительное число операций определяется составом конкретной анализируемой выборки. Результаты моделирования показывают, что при  сложность алгоритма оценивается величиной.

Коррекция обучающей выборки 

Предложен алгоритм коррекции обучающей выборки, учитывающий результаты анализа – долю выбросов, массивы. Предварительно производится проверка условия.

Если оно выполнено, можно считать, что выбросы оставляют малую долю от общего числа прецедентов. В простейшем случае выбросы просто удаляются из обучающей выборки Р. Учитывая их малое число, возможен также их индивидуальный человеко-машинный анализ. Если условие  не выполнено, доля выбросов достаточно велика. При их отбрасывании будет потеряна значительная часть исходной информации. Вручную проанализировать их также затруднительно из-за большого объема. Поэтому в качестве реального выхода предлагается автоматизированная коррекция выбросов, отмеченных в массиве NV. Для каждого выброса ошибочный прецедент заменяется прецедентом, в котором номер r принимается из массива NCOR. При этом условие  корректности прецедента будет выполнено и выброс устранен.

Приведем алгоритм коррекции обучающей выборки.

Исходные данные алгоритма:  n – число характерных признаков объектов;  число классов k;  NN – общий объем обучающей выборки P;  массив  координат точек  , объектов из P, упорядоченных по принадлежности к классам;  массив классов объектов из P; (delta0) – предельно допустимая доля удаляемых выбросов;  общее число Nв выбросов в обучающей выборке;  доля  (delta) выбросов в обучающей выборке;  массив NV размерности Nв номеров выбросов в обучающей выборке;  массив NCOR размерности Nв номеров корректирующих классов для выбросов в обучающей выборке.

Решаемая задача: коррекция выбросов в обучающей выборке.

Выходные данные: общий объем новой скорректированной обучающей выборки; новый массив координат точек;  новый массив классов объектов из скорректированной обучающей выборки.

В описании алгоритма номера прецедентов обозначены через No, классов – через Nc.

Сложность алгоритма коррекции обучающих выборок оценивается величиной.

Получаемая в результате коррекции обучающая выборка Р задает более плавные границы классов в пространстве значений признаков, в результате чего данные множества точек в большей степени удовлетворяют гипотезе компактности. Помимо устранения противоречивости входной информации, коррекция существенно упрощает отделимость классов, то есть построение решающей функции с более простой структурой обусловливает сокращение количества вычислительных операций на решение задачи классификации –.

В заключение отметим, что предлагаемые алгоритмы анализа и коррекции обучающих выборок позволяют выявить причины противоречивости получаемой исходной информации и эффективно с вычислительной точки зрения устранить ее. Сложность алгоритма анализа квадратична по объему выборки, сложность алгоритма коррекции линейна. При необходимости алгоритмы могут быть использованы для ручного контроля уже найденных выбросов, освобождая пользователя от большого объема рутинной работы.

Необходимая дополнительная настройка алгоритмов путем задания в них оптимального числа s ближайших точек при расчете расстояния от объекта до класса, а также величин коэффициентов , ,  должна учитывать метрику пространства значений признаков U и специфику класса решаемых задач. Изменение этих величин позволяет с использованием предлагаемых алгоритмов автоматически получать различные варианты обучающих выборок и соответствующих классификаторов. Из них можно выбирать оптимальные по тем или иным критериям. Выполненные на конкретных выборках расчеты позволили эффективно устранить в них выбросы и значительно упростить построение классификаторов в виде бинарных деревьев с разделяющими гиперплоскостями.

Массовое практическое применение предлагаемого подхода в конкретных предметных областях (психология и социология, медицина и биотехнология, экология, материаловедение и др.) поможет не только выработать соответствующие рекомендации по выбору величин, обеспечивающих построение оптимальных классификаторов, но и, возможно, создать его специализированные модификации.

