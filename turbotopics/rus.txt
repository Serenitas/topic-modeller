Вероятностное тематическое моделирование
К. В. Воронцов
16 октября 2013 г.

Содержание
1 Задачи тематического моделирования
4
1.1 Вероятностная модель коллекции документов . . . . . . . . . . . . . . 5
1.2 Униграммная порождающая модель . . . . . . . . . . . . . . . . . . . . 10
1.3 Предварительная обработка текстовых данных . . . . . . . . . . . . . . 11
2 Вероятностный латентный семантический анализ
2.1 EM-алгоритм . . . . . . . . . . . . . . . . . . . . .
2.2 Обобщённый EM-алгоритм . . . . . . . . . . . . .
2.3 Онлайновый EM-алгоритм . . . . . . . . . . . . .
2.4 Стохастический EM-алгоритм . . . . . . . . . . .
2.5 Начальные приближения . . . . . . . . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

12
12
14
16
18
19

3 Латентное размещение Дирихле
3.1 Байесовский вывод . . . . . . .
3.2 Сэмплирование Гиббса . . . . .
3.3 Оптимизация гиперпараметров
3.4 Действительно ли сглаживание

. . . . . . . .
. . . . . . . .
. . . . . . . .
необходимо?

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

21
22
23
24
25

4 Робастная тематическая модель
4.1 Тематическая модель с шумом и фоном
4.2 Робастный онлайновый EM-алгоритм .
4.3 Упрощённый робастный алгоритм . . .
4.4 Выделение стоп-слов . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

27
27
30
31
32

5 Регуляризация тематических моделей
5.1 Сглаживание и разреживание . . . . .
5.2 Частичное обучение . . . . . . . . . .
5.3 Разреживание как L0 -регуляризация .
5.4 Повышение различности тем . . . . .
5.5 Повышение когерентности тем . . . .
5.6 Учёт связей между документами . . .
5.7 Траектория регуляризации . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

32
33
35
38
39
40
42
43

.
.
.
.
.
.
.

2

6 Тематические модели классификации
6.1 Моделирование классов темами . . . . . .
6.2 Моделирование классов распределениями
6.3 Частотный регуляризатор . . . . . . . . .
6.4 Тематическая модель классификации . .
6.5 Тематическая модель категоризации . . .

. . .
тем
. . .
. . .
. . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

44
45
45
47
48
50

7 Динамические тематические модели
52
7.1 Модель с фиксированной тематикой . . . . . . . . . . . . . . . . . . . . 53
7.2 Модель с медленно меняющейся тематикой . . . . . . . . . . . . . . . . 54
7.3 Модели с непрерывным временем . . . . . . . . . . . . . . . . . . . . . 54
8 Иерархические тематические модели
54
8.1 Определение тематического дерева . . . . . . . . . . . . . . . . . . . . . 54
8.2 Фиксированная иерархия . . . . . . . . . . . . . . . . . . . . . . . . . . 55
8.3 Реконструкция иерархии . . . . . . . . . . . . . . . . . . . . . . . . . . 58
9 Многоязычные тематические модели
9.1 Параллельные тексты . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Сопоставимые тексты . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Регуляризация матрицы переводов слов . . . . . . . . . . . . . . . . . .

58
58
58
58

10 Модели текста как последовательности слов
10.1 Коллокации . . . . . . . . . . . . . . . . . . .
10.2 Марковские модели синтаксиса языка . . . .
10.3 Выделение ключевых фраз . . . . . . . . . .
10.4 Тематическая структура документа . . . . .

.
.
.
.

58
58
58
58
59

11 Многомодальные тематические модели
11.1 Коллаборативная фильтрация . . . . . . . . . . . . . . . . . . . . . . .
11.2 Модель научной социальной сети . . . . . . . . . . . . . . . . . . . . . .
11.3 Персонализация рекламы в Интернете . . . . . . . . . . . . . . . . . . .

59
59
59
59

12 Критерии качества тематических моделей
12.1 Внутренние оценки качества тематических моделей
12.2 Критерии условной независимости . . . . . . . . . .
12.3 Критерии качества классификации документов . .
12.4 Критерии качества тематического поиска . . . . . .
12.5 Интерпретируемость тем . . . . . . . . . . . . . . .
12.6 Когерентность . . . . . . . . . . . . . . . . . . . . . .
12.7 Точность восстановления модельных данных . . . .

.
.
.
.
.
.
.

59
59
60
63
64
64
64
65

.
.
.
.
.

65
65
66
72
73
78

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.

13 Эксперименты с тематическими моделями
13.1 Экспериментальные текстовые коллекции . . . . . . . . . . . .
13.2 Неустойчивость LDA (Глушаченков В. В.) . . . . . . . . . . . .
13.3 Сравнение PLSA, LDA и SWB (Потапенко А. А.) . . . . . . .
13.4 Разреживание матриц Φ и Θ (Потапенко А. А.) . . . . . . . .
13.5 Разреживание распределений тем p(t | d, w) (Потапенко А. А.)

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

3

13.6
13.7
13.8
13.9
13.10
13.11

Экономное сэмплирование (Потапенко А. А.) . . . . . . . . . . . . . .
Частота обновления параметров ϕwt и θtd (Потапенко А. А.) . . . . .
Оптимизация параметров робастного алгоритма (Потапенко А. А.) .
Онлайновые алгоритмы (Китов В. В., Потапенко А. А.) . . . . . . .
Категоризация: тематическая модель против SVM (Гаврилюк К. А.)
Качество категоризации для иерархических моделей . . . . . . . . .

.
.
.
.
.
.

79
83
83
83
83
83

4

1

Задачи тематического моделирования

Тематическое моделирование (topic modeling) — одно из современных приложений машинного обучения к анализу текстов, активно развивающееся с конца 90-х
годов. Тематическая модель (topic model) коллекции текстовых документов определяет, к каким темам относится каждый документ и какие слова (термины) образуют
каждую тему.
Вероятностная тематическая модель (ВТМ) описывает каждую тему дискретным распределением на множестве терминов, каждый документ — дискретным
распределением на множестве тем. Предполагается, что коллекция документов —
это последовательность терминов, выбранных случайно и независимо из смеси таких распределений, и ставится задача восстановления компонент смеси по выборке.
Поскольку документ или термин может относиться одновременно ко многим
темам с различными вероятностями, говорят, что ВТМ осуществляет «мягкую» кластеризацию документов и терминов по кластерам-темам. Тем самым решаются проблемы синонимии и омонимии терминов, возникающие при обычной «жёсткой» кластеризации. Синонимы, часто употребляющиеся в схожих контекстах, с большой вероятностью попадают в одну тему. Омонимы, употребляющиеся в разных контекстах,
распределяются между несколькими темами соответственно частоте употребления.
Тематические модели применяются для выявления трендов в научных публикациях или новостных потоках [68, 55], для классификации и категоризации документов [48] изображений и видеопотоков [30, 18, 56], для информационного поиска [65],
в том числе многоязычного [57], для тегирования веб-страниц [26], для обнаружения
текстового спама [6], для рекомендательных систем [64] и других приложений.
Одним из основных приложений является информационный поиск. Поисковые
системы представляют документы векторами частот слов. Поиск документов по коротким запросам реализуется путём поиска векторов, в которых часто встречаются
слова запроса [5]. Тематическая модель позволяет использовать тот же механизм для
поиска документов схожей тематики по целому документу или по длинному фрагменту текста. При этом документы представляются векторами частот тем, а не отдельных слов. Векторами частот тем представляются также объекты, упоминаемые
в документах или связанные с документами: термины, авторы, годы публикации,
институты, конференции, журналы, сайты и т. д., что позволяет задавать в качестве
запроса любой объект или совокупность объектов и искать по ним объекты того же
или другого типа, имеющие схожую тематику.
Тематические модели могут учитывать различные особенности языка и текстовых коллекций. Существуют модели, выявляющие ключевые фразы (термины
предметной области), учитывающие морфологию слов и синтаксическую структуру
предложений, отслеживающие изменения тематики во времени или внутри отдельных документов, строящие иерархические отношения между темами, учитывающие
связи между документами через авторство или ссылки, и т. д. Многочисленные разновидности вероятностных тематических моделей описаны в обзоре [14].
Большинство моделей разрабатываются на основе латентного размещения Дирихле LDA [9], математического аппарата графических моделей и байесовского вывода. Это современный активно развивающийся вероятностный инструментарий, находящий применения повсеместно в задачах анализа данных. Однако в тематическом

5

моделировании он порождает две открытые проблемы, которые на удивление мало
освещаются в литературе по ВТМ.
Первая проблема заключается в том, что априорные распределения Дирихле
и их обобщения — процессы Дирихле и Питмана-Йора — имеют крайне слабые лингвистические обоснования. Они не моделируют какие-либо явления языка. Их применение продиктовано исключительно математическим удобством, так как они позволяют аналитически выполнять интегрирование в байесовском выводе.
Второй проблемой является сложность совмещения большого числа функциональных требований в одной модели [14]. В частности, для обработки больших коллекций научных публикаций нужна модель, удовлетворяющая десятку требований
одновременно (иерархическая, динамическая, n-грамная, мультиязычная, разреженная, робастная, и т. д.). Многие исследователи признают, что общепринятый математический аппарат слишком сложен для совмещения более 2–3 требований.
В данной работе развивается теория аддитивной регуляризации тематических
моделей (АРТМ), которая решает обе эти проблемы. За основу берётся классическая
модель вероятностного латентного семантического анализа PLSA [22]. Для оценивания параметров модели PLSA применяется EM-алгоритм, который ищет максимум
правдоподобия. Максимум достигается на бесконечном множестве моделей, то есть
задача построения ВТМ является некорректно поставленной. Это служит обоснованием для введения регуляризации [7]. К функционалу логарифма правдоподобия добавляются штрафные слагаемые (регуляризаторы), выражающие различные дополнительные требования к модели, не обязательно вероятностного характера. Каждая
аддитивная поправка к функционалу приводит к аддитивной поправке в формуле
M-шага EM-алгоритма. Это позволяет комбинировать произвольное число требований и строить многоцелевые тематические модели. Модели LDA соответствует
свой аддитивный регуляризатор. Многочисленным модификациям LDA также соответствуют свои регуляризаторы. Теория АРТМ описывает огромное разнообразие
тематических моделей, созданных за последнее десятилетие, не прибегая к избыточным вероятностным допущениям и сложным техникам байесовского вывода.

§1.1

Вероятностная модель коллекции документов

Пусть D — множество (коллекция) текстовых документов, W — множество
(словарь) всех употребляемых в них терминов (слов или словосочетаний). Каждый
документ d ∈ D представляет собой последовательность nd терминов (w1 , . . . , wnd ) из
словаря W . Термин может повторяться в документе много раз.
Вероятностное пространство и гипотеза независимости. Предполагается, что
существует конечное множество тем T , и каждое употребление термина w в каждом
документе d связано с некоторой темой t ∈ T , которая не известна. Коллекция документов рассматривается как множество троек (d, w, t), выбранных случайно и независимо из дискретного распределения p(d, w, t), заданного на конечном множестве
D × W × T . Документы d ∈ D и термины w ∈ W являются наблюдаемыми переменными, тема t ∈ T является латентной (скрытой) переменной.
Гипотеза о независимости элементов выборки эквивалентна предположению,
что порядок терминов в документах не важен для выявления тематики, то есть тематику документа можно узнать даже после произвольной перестановки терминов,

6

хотя для человека такой текст теряет смысл. Это предположение называют гипотезой «мешка слов» (bag of words). Порядок документов в коллекции также не имеет
значения; это предположение называют гипотезой «мешка документов».
Приняв гипотезу «мешка слов», можно перейти к более компактному представлению документа как подмножества d ⊂ W , в котором каждому элементу w ∈ d
поставлено в соответствие число ndw вхождений термина w в документ d.
Постановка задачи тематического моделирования. Построить тематическую
модель коллекции документов D — значит найти множество тем T , распределения
p(w | t) для всех тем t ∈ T и распределения p(t | d) для всех документов d ∈ D. Можно
также говорить о задаче совместной «мягкой» кластеризации множества документов
и множества слов по множеству кластеров-тем. Мягкая кластеризация означает,
что каждый документ или термин не жёстко приписывается какой-то одной теме,
а распределяется по нескольким темам.
Найденные распределения используются затем для решения прикладных задач.
Распределение p(t | d) является удобным признаковым описанием документа в задачах информационного поиска, классификации и категоризации документов.
Гипотеза условной независимости. Будем полагать, что появление слов в документе d, относящихся к теме t, описывается общим для всей коллекции распределением p(w | t) и не зависит от документа d. Это предположение, называемое гипотезой
условной независимости, допускает три эквивалентных представления:
p(w | d, t) = p(w | t);
p(d | w, t) = p(d | t);
p(d, w | t) = p(d | t)p(w | t).

(1.1)

Вероятностная модель порождения данных. Согласно определению условной
вероятности, формуле полной вероятности и гипотезе условной независимости
X
p(w | d) =
p(t | d) p(w | t).
(1.2)
t∈T

Если распределения p(t | d) и p(w | t) известны, то вероятностная модель (1.2)
описывает процесс порождения коллекции D, см. также Алгоритм 1.1 и рис. 1.
Построение тематической модели — это обратная задача: по известной коллекции D требуется восстановить породившие её распределения p(t | d) и p(w | t).
Гипотеза разреженности. Естественно предполагать, что каждый документ d
и каждый термин w связан с небольшим числом тем t. В таком случае значительная
часть вероятностей p(t | d) и p(w | t) должна обращаться в нуль.
Если документ относится к большому числу тем (например, энциклопедия,
журнал, сборник статей), то в задачах тематического поиска или классификации
документов его имеет смысл разбивать на части, более однородные по тематике.
Если термин относится к большому числу тем, то, скорее всего, это общеупотребительное слово (стоп-слово), бесполезное для определения тематики.

7

Алгоритм 1.1. Вероятностная модель порождения коллекции документов.

1
2
3
4
5
6

Вход: распределения p(w | t), p(t | d);
Выход: выборка пар (di , wi ), i = 1, . . . , n;
для всех d ∈ D
задать длину nd документа d;
для всех i = 1, . . . , nd
выбрать случайную тему t из распределения p(t | d);
выбрать случайный термин w из распределения p(w | t);
добавить в выборку пару (d, w), при этом тема t «забывается»;

,…,
( |!)

#$
!

("| ):

"

#" $

0.023 днк
0.016 геном
0.009 нуклеотид
…… ……

0.014 базис
0.009 спектр
0.006 ортогональный
…… ……

0.018 распознавание
0.013 сходство
0.011 паттерн
…… ……

" , … , "#$ :
Разработан спектрально-аналитический подход к выявлению размытых протяженных повторов
в геномных последовательностях. Метод основан на разномасштабном оценивании сходства
нуклеотидных последовательностей в пространстве коэффициентов разложения фрагментов
кривых GC- и GA-содержания по классическим ортогональным базисам. Найдены условия
оптимальной аппроксимации, обеспечивающие автоматическое распознавание повторов
различных видов (прямых и инвертированных, а также тандемных) на спектральной матрице
сходства. Метод одинаково хорошо работает на разных масштабах данных. Он позволяет
выявлять следы сегментных дупликаций и мегасателлитные участки в геноме, районы синтении
при сравнении пары геномов. Его можно использовать для детального изучения фрагментов
хромосом (поиска размытых участков с умеренной длиной повторяющегося паттерна).

Рис. 1. Процесс порождения текстового документа вероятностной тематической моделью (1.2).

Алгоритмы, в которых нулевые значения не хранятся, намного эффективнее
по памяти и по скорости. Поэтому для больших коллекций разреженность должна
учитываться обязательно.
Частотные оценки условных вероятностей. Вероятности, связанные с наблюдаемыми переменными d и w, можно оценивать по выборке как частоты (здесь и далее
выборочные оценки вероятностей p будем обозначать через p̂):
ndw
nd
nw
ndw
,
p̂(d) = ,
p̂(w) =
,
p̂(w | d) =
,
(1.3)
n
n
n
nd
ndw — число вхождений термина w в документ d;
P
nd =
ndw — длина документа d в терминах;
w∈W
P
nw =
ndw — число вхождений термина w во все документы коллекции;
d∈D
P P
n=
ndw — длина коллекции в терминах.

p̂(d, w) =

d∈D w∈d

8

Вероятности, связанные со скрытой переменной t, также можно оценивать как
частоты, если рассматривать коллекцию документов как выборку троек (d, w, t):
nwt
ndt
ndwt
nt
p̂(t) = ,
p̂(w | t) =
,
p̂(t | d) =
,
p̂(t | d, w) =
,
(1.4)
n
nt
nd
ndw
ndwt — число троек, в которых термин w документа d связан с темой t;
P
ndt =
ndwt — число троек, в которых термин документа d связан с темой t;
w∈W
P
nwt =
ndwt — число троек, в которых термин w связан с темой t;
d∈D
P P
nt =
ndwt — число троек, связанных с темой t.
d∈D w∈d

В пределе n → ∞ частотные оценки p̂(·), определяемые формулами (1.3)–(1.4),
стремятся к соответствующим вероятностям p(·), согласно закону больших чисел.
Частотная интерпретация даёт ясное понимание всех условных вероятностей, которые будут использоваться в дальнейшем.

Стохастическое матричное разложение. Если число тем |T | много меньше числа
документов |D| и числа терминов |W |, то равенство (1.2) можно понимать как задачу
приближённого представления заданной матрицы частот

F = p̂wd W ×D , p̂wd = p̂(w | d) = ndw /nd ,

в виде произведения F ≈ ΦΘ двух неизвестных матриц меньшего размера — матрицы терминов тем Φ и матрицы тем документов Θ:
Φ = (ϕwt )W ×T , ϕwt = p(w | t);
Θ = (θtd )T ×D , θtd = p(t | d).
Матрицы, столбцы которых неотрицательны и нормированы, следовательно,
могут пониматься как дискретные распределения, называются стохастическими.
Одно из наиболее известных представлений вида F ≈ ΦΘ строится из |T | главных компонент сингулярного разложения матрицы F и является решением задачи
наименьших квадратов
2
X
XX
2 X X 
p̂wd −
ϕwt θtd = kF − ΦΘk2 → min . (1.5)
p̂wd − p(w | d) =
d∈D w∈W

d∈D w∈W

Φ,Θ

t∈T

Метод главных компонент не подходит для тематического моделирования, так
как получаемые с его помощью матрицы Φ, Θ в общем случае не являются стохастическими. Кроме того, квадратичная функция потерь плохо подходит для сравнения
вероятностных распределений с «тяжёлыми хвостами».
В вероятностном тематическом моделировании вместо принципа наименьших
квадратов используется принцип максимума правдоподобия.
Принцип максимума правдоподобия. Для оценивания параметров Φ, Θ тематической модели по коллекции документов D будем максимизировать правдоподобие
(плотность распределения) выборки:
YY
YY
p(D; Φ, Θ) = C
p(d, w)ndw =
p(w | d)ndw Cp(d)ndw → max,
| {z }
Φ,Θ
d∈D w∈d

d∈D w∈d

const

9
0.04

0.020

0.020

P

P

0.03
0.02

Q

P

0.015

0.015

0.010

0.010

0.005

0.005

Q

Q
0.01
0

0
0

50

100

150

200

0
0

KL(P kQ) = 0.442
KL(QkP ) = 2.966

50

100

150

200

KL(P kQ) = 0.444
KL(QkP ) = 0.444

0

50

100

150

200

KL(P kQ) = 2.969
KL(QkP ) = 2.969

Рис. 2. Дивергенция Кульбака–Лейблера KL(P kQ) является несимметричной мерой вложенности
распределения P = (pi )ni=1 в распределение Q = (qi )ni=1 . Вложенность P в Q приблизительно одинакова на левом и среднем графиках, вложенность Q в P — на левом и правом графиках.

где C — нормировочный множитель, зависящий только от чисел ndw . Отбросим множители C и p(d), не влияющие на положение точки максимума, подставим выражение для p(w | d) из (1.2) и воспользуемся обозначениями θtd = p(t | d), ϕwt = p(w | t).
Прологарифмируем p(D; Φ, Θ), чтобы превратить произведения в суммы. Получим
задачу максимизации логарифма правдоподобия (log-likelihood) при ограничениях
неотрицательности и нормированности столбцов матриц Φ и Θ:
XX
X
L(Φ, Θ) =
ndw ln
ϕwtθtd → max;
(1.6)
ϕwt = 1;

Φ,Θ

t∈T

d∈D w∈d

X

ϕwt > 0;

w∈W

X

θtd = 1;

θtd > 0.

t∈T

Дивергенция Кульбака–Лейблера или KL-дивергенция между дискретными распределениями P = (pi )ni=1 и Q = (qi )ni=1 — это несимметричная функция расстояния
KL(P kQ) ≡ KLi (pi kqi ) =

n
X
i=1

pi ln

pi
.
qi

Предполагается, что pi > 0 и qi > 0. KL-дивергенция является не вполне адекватной функцией расстояния в случае, когда у распределений P и Q не совпадают
носители ΩP = {i : pi > 0} и ΩQ = {i : qi > 0}.
Перечислим наиболее важные свойства KL-дивергенции.
1. KL-дивергенция неотрицательна. Если ΩP = ΩQ , то KL-дивергенция равна
нулю тогда и только тогда, когда распределения совпадают, pi ≡ qi .
2. KL-дивергенция является мерой вложенности двух распределений. Если
KL(P kQ) < KL(QkP ), то распределение P сильнее вложено в Q, чем Q в P , см. рис. 2.
3. Если P — эмпирическое распределение, а Q(α) — параметрическое семейство
(модель) распределений, то минимизация KL-дивергенции эквивалентна максимизации правдоподобия:
KL(P kQ(α)) =

n
X
i=1

pi
pi ln
→ min
α
qi (α)

⇐⇒

n
X
i=1

pi ln qi (α) → max .
α

10

Максимизация правдоподобия (1.6) эквивалентна минимизации взвешенной
суммы дивергенций Кульбака–Лейблера между эмпирическими распределениями
p̂(w | d) = ndw /nd и модельными p(w | d), по всем документам d из D:

n 
 X
X
dw 
ϕwt θtd → min,
nd KLw

Φ,Θ
nd
t∈T
d∈D

где весом документа d является его длина nd . Если веса nd убрать, то все документы будут искусственно приведены к одинаковой длине. Такая модификация функционала качества может быть полезна при моделировании коллекций, содержащих
документы одинаковой важности, но существенно разной длины.

Лирическое отступление: как решать задачи оптимизации с ограничениями равен- ToDo1
ствами и неравенствами. Лагранжиан. Теорема Куна–Таккера. Иногда можно игнорировать ограничения-неравенства и, получив решение, доказывать, что оно удовлетворяет этим
ограничениям. Просто везение.

§1.2

Униграммная порождающая модель

Простейшим примером вероятностной порождающей модели является униграммная модель, основанная на предположении, что каждое слово появляется в тексте независимо от остальных слов. Модели, в которых учитываются пары, тройки,
n-ки слов (обычно соседних), называются, соответственно, биграммными, триграммными, n-граммными. Рассмотрим два варианта униграммной модели.
Униграммная модель документов. Предполагается, что слова каждого документа генерируются случайно и независимо из распределения p(w|d) = ξdw , своего для
каждого документа d. Запишем задачу максимизации правдоподобия при ограничениях нормировки и неотрицательности на параметры модели ξdw :
XX
X
ndw ln ξdw → max,
ξdw = 1,
ξdw > 0.
ξ

d∈D w∈d

w∈W

Запишем функцию Лагранжа, проигнорировав ограничения-неравенства (потом, получив решение, убедимся, что они выполнены автоматически):
 P

P P
L =
ndw ln ξdw − λd
ξdw − 1 ;
d∈D

w∈d

w∈W

приравняем нулю производные по переменным ξdw :
∂L
ndw
=
− λd = 0.
∂ξdw
ξdw

Суммируя по w ∈ d, получим значение двойственных переменных λd = nd , и, подставляя их обратно в уравнение для ξdw , найдём, что искомый параметр ξdw равен
частотной оценке условной вероятности слова w в документе d:
ξdw = p̂(w|d) = ndw /nd .

(1.7)

11

Униграммная модель коллекции. Предполагается, что слова каждого документа
генерируются случайно и независимо из распределения p(w|d) = ξw , общего для всех
документов коллекции. По аналогии с предыдущим случаем,
XX
X
ndw ln ξw → max,
ξw = 1,
ξw > 0.
ξ

d∈D w∈d

L =

P P

ndw ln ξw − λ

d∈D w∈d

nw
∂L
=
− λ = 0;
∂ξw
ξw

w∈W

P

w∈W


ξw − 1 ;

откуда следует, что λ = n, и искомый параметр ξw равен частотной оценке вероятности слова w во всей коллекции:
ξw = p̂(w) = nw /n.

(1.8)

Обе униграммные модели имеют простые, интуитивно очевидные решения (1.7)
и (1.8), но не являются тематическими. Тематическая модель (1.2) занимает между
ними промежуточное положение. Набор её параметров богаче униграммной модели
коллекции, но беднее униграммной модели документов.
Лирическое отступление: распределения с тяжёлыми хвостами. Законы Ципфа, ToDo2
Ципфа–Мандельброта, Хипса.

§1.3

Предварительная обработка текстовых данных

Понятие «термина» может изменяться в зависимости от целей построения тематической модели и таких особенностей задачи, как язык документов, средняя длина
документов, тематика коллекции.
Лемматизация и стемминг. При построении тематической модели нет смысла
различать формы (склонения, спряжения) одного и того же слова. Это приведёт
к неоправданному разрастанию словаря, дроблению статистики, увеличению ресурсоёмкости и снижению качества модели.
Лемматизация — это приведение каждого слова в документе к его нормальной
форме. В русском языке нормальными формами считаются: для существительных —
именительный падеж, единственное число; для прилагательных — именительный падеж, единственное число, мужской род; для глаголов, причастий, деепричастий —
глагол в инфинитиве. Разработка хорошего лемматизатора (lemmatizer) требует составления грамматического словаря со всеми формами слов, либо аккуратной формализации правил языка со всеми исключениями, что является трудоёмким проектом.
Известные лемматизаторы совершенствуются постепенно. Их недостатком является
неполнота словарей, особенно по части специальной терминологии и неологизмов,
которые во многих приложениях как раз и представляют наибольший интерес.
ссылка на рекомендуемые русский и английский лемматизаторы

Стемминг — это более простая технология, которая состоит в отбрасывании
изменяемых частей слов, главным образом, окончаний. Она не требует хранения словаря всех слов и основана на правилах морфологии языка. Недостатком стемминга

ToDo3

12

является большее число ошибок. Стемминг хорошо подходит для английского языка,
но хуже подходит для русского.
ToDo4

ссылка на рекомендуемые русский и английский стеммеры

Отбрасывание стоп-слов. Слова, встречающиеся во многих текстах различной тематики, бесполезны для тематического моделирования, и могут быть отброшены.
К ним относятся предлоги, союзы, числительные, местоимения, некоторые глаголы, прилагательные и наречия. Число таких слов обычно варьируется в пределах
нескольких сотен. Их отбрасывание почти не влияет на длину словаря, но может
приводить к заметному сокращению длины некоторых текстов.
Отбрасывание редких слов. Слова, встречающиеся в длинном документе слишком
редко, например, только один раз, также можно отбрасывать, полагая, что данное
слово не характеризует тематику данного документа. При обработке коллекций коротких новостных сообщений этот приём лучше не использовать.
Выделение ключевых фраз. При обработке коллекций научных, юридических или
других специальных текстов вместо отдельных слов выделяют ключевые фразы —
словосочетания, являющиеся терминами предметной области. Это отдельная довольно сложная задача, для решения которой используются тезаурусы, составленные экспертами [4], либо методы машинного обучения [44, 69], при этом для формирования
обучающих выборок всё равно приходится привлекать экспертов.
Далее будем полагать, что словарь W получен в результате предварительной
обработки всех документов коллекции D и может содержать как отдельные слова,
так и ключевые фразы. Элементы словаря w ∈ W будем называть «терминами».

2

Вероятностный латентный семантический анализ

Вероятностный латентный семантический анализ (probabilistic latent semantic
analysis, PLSA) был предложен Томасом Хофманном в [22].
Вероятностная модель появления пары «документ–термин» (d, w) записывается
тремя эквивалентными способами:
X
X
X
p(d, w) =
p(t)p(w | t)p(d | t) =
p(d)p(w | t)p(t | d) =
p(w)p(t | w)p(d | t),
t∈T

t∈T

t∈T

где p(t) — распределение тем во всей коллекции. Первое представление называется
симметричным, второе и третье — несимметричными. Они приводят к немного разным итерационным процессам обучения тематической модели. Сейчас возьмём за
основу второе представление, совпадающее с (1.2).

§2.1

EM-алгоритм

Для решения задачи (1.6) в PLSA применяется итерационный процесс, в котором каждая итерация состоит из двух шагов — Е (expectation) и М (maximization) [15].
Перед первой итерацией выбирается начальное приближение параметров ϕwt , θtd .

13

На E-шаге по текущим значениям параметров ϕwt, θtd с помощью формулы
Байеса вычисляются условные вероятности p(t | d, w) всех тем t ∈ T для каждого
термина w ∈ d в каждом документе d:
Hdwt = p(t | d, w) =

p(w | t)p(t | d)
ϕwt θtd
= P
.
p(w | d)
ϕws θsd

(2.1)

s∈T

На M-шаге, наоборот, по условным вероятностям тем Hdwt вычисляется новое
приближение параметров ϕwt , θtd . Это легко сделать, если заметить, что величина
(2.2)

n̂dwt = ndw p(t | d, w) = ndw Hdwt

оценивает (не обязательно целое) число ndwt вхождений термина w в документ d,
связанных с темой t. Просуммировав n̂dwt по документам d и по терминам w, получим оценки n̂wt , n̂dt , n̂t , и через них, согласно (1.4), — частотные оценки условных
вероятностей ϕwt , θtd :
n̂wt
,
n̂t
n̂dt
θtd =
,
n̂d

n̂t =

ϕwt =

P

n̂wt ,

n̂wt =

w∈W

n̂d =

P

P

(2.3)

ndw Hdwt .

d∈D

n̂dt ,

n̂dt =

t∈T

P

(2.4)

ndw Hdwt .

w∈d

Эти простые, но не вполне строгие рассуждения поясняют суть EM-алгоритма.
Покажем теперь, что оценки (2.3)–(2.4) действительно являются решением задачи
максимизации правдоподобия (1.6) при фиксированных Hdwt .
Запишем лагранжиан задачи (1.6) при ограничениях нормировки, проигнорировав ограничения неотрицательности (позже убедимся, что решение действительно
неотрицательно):
 X X

XX
X
X X
L (Φ, Θ) =
ndw ln
ϕwt θtd −
λt
ϕwt − 1 −
µd
θtd − 1 .
d∈D w∈d

t∈T

|

t∈T

{z

p(w | d)

w∈W

d∈D

t∈T

}

Продифференцировав лагранжиан по ϕwt и приравняв нулю производную, получим
X
θtd
λt =
ndw
.
(2.5)
p(w | d)
d∈D
Домножим обе части этого равенства на ϕwt , просуммируем по всем терминам w ∈ W ,
применим условие нормировки вероятностей ϕwt в левой части и выделим переменную Hdwt в правой части. Получим
XX
λt =
ndw Hdwt .
d∈D w∈W

Снова домножим обе части (2.5) на ϕwt, выделим переменную Hdwt в правой части
и выразим ϕwt из левой части, подставив уже известное выражение для λt . Получим
P
ndw Hdwt
d∈D
ϕwt = P P
.
ndw′ Hdw′ t
w ′ ∈W d∈D

14

Алгоритм 2.1. PLSA-EM: рациональный EM-алгоритм для модели PLSA.

1
2
3
4

Вход: коллекция документов D, число тем |T |, начальные приближения Θ, Φ;
Выход: распределения Θ и Φ;
повторять
обнулить n̂wt , n̂dt , n̂t для всех d ∈ D, w ∈ W , t ∈ T ;
для всехPd ∈ D, w ∈ d
Z :=
ϕwt θtd ;
t∈T

5

для всех t ∈ T таких, что ϕwt θtd > 0
увеличить n̂wt , n̂dt , n̂t на δ = ndw ϕwt θtd /Z;

6
7
8
9

ϕwt := n̂wt /n̂t для всех w ∈ W , t ∈ T ;
θtd := n̂dt /nd для всех d ∈ D, t ∈ T ;
пока Θ и Φ не сойдутся;

Обозначив числитель через n̂wt , получим (2.3). Проделав аналогичные действия
с производной лагранжиана по θtd , получим (2.4).
Заметим, что если начальные приближения θtd и ϕwt положительны, то и после каждой итерации они будут оставаться положительными, несмотря на то, что
ограничение неотрицательности было проигнорировано в ходе решения.
Эффективность EM-алгоритма по времени и по памяти. Число операций растёт линейно по длине коллекции n, числу тем T и числу итераций.
Перебор всех терминов w во всех документах d можно организовать очень эффективно, если хранить каждый документ d в виде последовательности пар (w, ndw ).
Рациональный EM-алгоритм. Вычисление переменных n̂wt , n̂dt , n̂t на М-шаге требует однократного прохода всей коллекции в цикле по всем документам d ∈ D и всем
терминам w ∈ d. Внутри этого цикла переменные Hdwt можно вычислять непосредственно в тот момент, когда они понадобятся. От этого результат алгоритма не изменяется, Е-шаг встраивается внутрь M-шага без дополнительных вычислительных
затрат, отпадает необходимость хранения трёхмерной матрицы Hdwt . Заметим также,
что переменную n̂d можно не вычислять, поскольку n̂d = nd . Этот вариант реализации EM-алгоритма будем называть рациональным; он показан в Алгоритме 2.1.

§2.2

Обобщённый EM-алгоритм

В ЕМ-алгоритме нет необходимости сверхточно решать задачу максимизации
правдоподобия на M-шаге. Достаточно ещё немного приблизиться к точке максимума правдоподобия и снова выполнить E-шаг. Это связано с тем, что сам функционал
правдоподобия известен не точно — он зависит от приближённых значений Hdwt ,
полученных на Е-шаге. ЕМ-алгоритм с сокращённым M-шагом называется обобщённым ЕМ-алгоритмом (generalized EM-algorithm, GEM). Для него справедливы те же
доказательства сходимости, что и для основного варианта ЕМ-алгоритма [15].
Другое обобщение состоит в том, что E-шаг выполняется только для части
скрытых переменных Hdwt . После этого M-шаг выполняется только для тех основных

15

Алгоритм 2.2. PLSA-GEM: обобщённый EM-алгоритм для модели PLSA.

1
2
3
4

Вход: коллекция документов D, число тем |T |, начальные приближения Θ, Φ;
Выход: распределения Θ и Φ;
обнулить n̂wt , n̂dt , n̂t , n̂d , ndwt для всех d ∈ D, w ∈ W , t ∈ T ;
повторять
для всехPd ∈ D, w ∈ d
Z :=
ϕwt θtd ;
t∈T

5
6
7
8
9
10
11
12

для всех t ∈ T таких, что ndwt > 0 или ϕwt θtd > 0
δ := ndw ϕwt θtd /Z;
увеличить n̂wt , n̂dt , n̂t , n̂d на (δ − ndwt );
ndwt := δ;

если пора обновить параметры Φ, Θ то
ϕwt := n̂wt /n̂t для всех w ∈ W , t ∈ T таких, что n̂wt изменился;
θtd := n̂dt /n̂d для всех d ∈ D, t ∈ T таких, что n̂dt изменился;
пока Θ и Φ не сойдутся;

переменных ϕwt , θtd , которые зависят от изменившихся скрытых переменных. Для
этого случая также имеются обоснования сходимости [38].
В случае PLSA сокращение M-шага сводится к более частому обновлению параметров θtd и ϕwt по значениям счётчиков n̂wt и n̂dt . В Алгоритме 2.1 обновления
происходят после каждого прохода коллекции. Возможные и другие варианты обновлений: после каждого документа, после каждого термина (d, w), после заданного
числа терминов, после каждого вхождения термина. В Алгоритме 2.2 моменты обновления выбираются на шаге 9.
На больших коллекциях частые обновления повышают скорость сходимости.
Проведённые в [2] эксперименты показывают, что частота обновления влияет именно
на скорость сходимости и почти не влияет на значение правдоподобия в конце итераций. Отсюда следует практическая рекомендация делать обновления после каждого
термина, при этом каждый термин документа обрабатывается только один раз. Этот
способ имеет дополнительное преимущество — внутри алгоритма можно отказаться
от хранения матриц Θ и Φ, поскольку значения θtd и ϕwt требуются только на шаге 6,
и их можно вычислять «на лету» по тем же формулам, что на шаге 10 и 11. Обновления после каждого вхождения термина являются избыточно частыми, в этом случае
каждый термин документа приходится обрабатывать ndw раз.
На первой итерации (т. е. при первом проходе коллекции) частые обновления не
делаются, чтобы в счётчиках накопилась информация по всей коллекции. В противном случае оценки параметров θtd и ϕwt по начальному фрагменту выборки могут
оказаться хуже начального приближения. Начиная со второй итерации, для каждой
пары (d, w) из счётчиков n̂wt и n̂dt вычитается ndwt — то самое значение δ, которое
было к ним прибавлено при обработке пары (d, w) на предыдущей итерации. Таким
образом, счётчики n̂wt и n̂dt всегда содержат результат последнего однократного прохода всей коллекции.

16

Необходимость хранения трёхмерной матрицы ndwt делает Алгоритм 2.2 неприменимым к большим коллекциям. Этот недостаток устраняется путём реорганизации
итераций, либо применением сэмплирования. Далее рассматриваются оба способа.

§2.3

Онлайновый EM-алгоритм

На больших коллекциях Алгоритмы 2.1 и 2.2 могут сходиться очень медленно.
Причина в том, что за однократный проход по всем документам коллекции оценки распределений терминов в темах ϕwt = n̂wt /n̂t уточняются огромное число раз
и успевают сойтись, в то же время распределения тем в документах θd проходят
лишь одну итерацию. На начальных итерациях, пока распределения θd не сошлись,
вычислительный ресурс тратится впустую на достижение сходимости ϕt к приближениям, далёким от оптимальных. Суть этой проблемы в том, что параметры θtd
привязаны к отдельным документам d, а параметры ϕwt — ко всей коллекции. Проблема решается реорганизацией шагов итерационного процесса.
Пакетный алгоритм. Проход каждого документа d ∈ D производится несколько
раз подряд. На каждом проходе документа выполняется E-шаг и обновляется распределение θd . Обновление распределений ϕt производится после каждого прохода
коллекции, как в Алгоритме 2.1. В результате распределения ϕt и θd сходятся более
согласованно. Кроме того, такая реорганизация итерационного процесса позволяет
отказаться от хранения трёхмерных массивов Hdwt или ndwt .
Можно также отказаться от двумерных массивов, размер которых пропорционален |D|, что позволит обрабатывать очень большие коллекции документов. Вероятности θtd всех тем t ∈ T документа d не нужны по окончании обработки документа d,
поэтому двумерный массив (θtd )T ×D можно заменить одномерным (θt )T .
Хранение двумерного массива (θtd )T ×D всё же имеет смысл, если размер коллекции |D| относительно невелик, и на шаге 5 инициализация θtd производится только во время первого прохода коллекции, а при последующих проходах используется
текущая оценка θtd , оставшаяся с предыдущего прохода. Хорошее начальное приближение обеспечивает сходимость θd за меньшее число итераций.
Скорость сходимости зависит от выбора числа итераций на внутреннем цикле
по документу и внешнем цикле по коллекции. На начальных итерациях внешнего
цикла можно делать меньше итераций внутреннего цикла, поскольку нет смысла
добиваться сходимости θd , пока распределения ϕt далеки от оптимальных.
Ещё одна идея ускорения сходимости состоит в том, чтобы начальные итерации
провести не по всей коллекции, а по случайному подмножеству (пакету) документов
D ′ ⊆ D. Если коллекция имеет избыточный размер, то для получения хорошего приближения Φ достаточно будет просмотреть относительно небольшую её часть.
Алгоритм 2.3 назван пакетным (batch algorithm), так как он может обрабатывать коллекцию по частям. Развитие этой идеи приводит к онлайновому алгоритму [21], одному из самых быстрых в тематическом моделировании. Он реализован
в библиотеке онлайновых алгоритмов Vowpal Wabbit Джона Лэнгфорда.
Онлайновый алгоритм. В машинном обучении онлайновыми принято называть
алгоритмы, способные адекватно настраивать параметры модели за один проход по
выборке. Онлайновые алгоритмы используются для обработки потоковых данных.

17

Алгоритм 2.3. PLSA-BatchEM: пакетный EM-алгоритм для модели PLSA.
Вход: коллекция документов D, число тем |T |;
Выход: распределения Θ и Φ;
1
2
3
4
5
6
7
8

инициализировать ϕwt для всех w ∈ W , t ∈ T ;
повторять
n̂wt := 0; n̂t := 0 для всех w ∈ W , t ∈ T ;
для всех d ∈ D
инициализировать θtd для всех t ∈ T ;
повторять
P
Zw :=
ϕwt θtd для всех w ∈ d;
t∈T P
θtd := n1d
ndw ϕwt θtd /Zw для всех t ∈ T ;
w∈d

9
10
11
12

пока θd не сойдётся;
увеличить n̂wt , n̂t на ndw ϕwt θtd /Zw для всех w ∈ d, t ∈ T ;

ϕwt := n̂wt /n̂t для всех w ∈ W , t ∈ T ;
пока Φ не сойдутся;

Во многих приложениях тематического моделирования коллекция документов пополняется динамически, и требуется обновлять модель, не обрабатывая заново всю
коллекцию. Обновление модели предполагает вычисление распределения θtd = p(t | d)
для нового документа d и уточнение распределений ϕwt = p(w | t) для всех тем t ∈ T ,
имеющих ненулевые вероятности для слов документа d. Если коллекция уже имеет
большой объём, то добавление документа не сильно влияет на Φ. Чем больше коллекция, тем лучше текущее приближение Φ, и тем меньше итераций потребуется для
добавления нового документа.
Стратегии ускорения сходимости. Первый пакет — особенный! Для первого пакета:
ToDo5
1) число итераций θd ограничить сверху числом прошедших итераций Φ +несколько.
2) инициализировать двумерный массив θtd только при 1-м проходе.

Онлайновый Алгоритм 2.4 является модицификацией пакетного Алгоритма 2.3.
Теперь вся коллекция разбивается на пакеты документов D1 , D2 , . . . , Dj , . . . Способ
разбиения остаётся на усмотрение разработчика, в частности, пакеты могут пересекаться либо не пересекаться, просматриваться по одному разу либо многократно,
выбираться случайно, по времени поступления или публикации документов, и т. д.
Желательно, чтобы размер первого пакета |D1 | был достаточным для оценивания
матрицы Φ с приемлемой точностью. Обработка каждого пакета производится пакетным Алгоритмом 2.3 при фиксированных ϕwt . Затем счётчики ñwt , вычисленные
по обработанному пакету документов, складываются со счётчиками n̂wt , вычисленными по всем предыдущим пакетам.
Режимы работы онлайнового алгоритма отличаются для больших и малых, динамических и статических коллекций. Для большой коллекции достаточно одного
прохода, так как матрица Φ стабилизируется после нескольких тысяч первых документов и далее почти не меняется. Малые коллекции, наоборот, требуют многократных проходов. Динамические коллекции (например, новостные сообщения) имеют

18

Алгоритм 2.4. PLSA-OEM: онлайновый EM-алгоритм для модели PLSA.
Вход: коллекция документов D, число тем |T |, параметр ρj ;
Выход: распределения Θ и Φ;
1
2
3
4
5
6
7
8
9
10

инициализировать ϕwt для всех w ∈ W , t ∈ T ;
n̂wt := 0, n̂t := 0 для всех w ∈ W , t ∈ T ;
для всех пакетов Dj , j = 1, . . . , J
повторять
ñwt := 0, ñt := 0 для всех w ∈ W , t ∈ T ;
для всех d ∈ Dj
инициализировать θtd для всех t ∈ T ;
повторять
P
Zw :=
ϕwt θtd для всех w ∈ d;
t∈T P
θtd := n1d
ndw ϕwt θtd /Zw для всех t ∈ T ;
w∈d

11
12
13
14
15
16

пока θd не сойдётся;
увеличить ñwt , ñt на ndw ϕwt θtd /Zw для всех w ∈ d, t ∈ T ;
ρ n̂ + ñwt
ϕwt := j wt
для всех w ∈ W , t ∈ T таких, что ñwt > 0;
ρj n̂t + ñt
пока Φ не сойдутся;
n̂wt := ρj n̂wt + ñwt для всех w ∈ W , t ∈ T ;
n̂t := ρj n̂t + ñt для всех t ∈ T ;

изменчивую во времени тематику. В случаях малых и динамических коллекций значимость пакетов убывает по мере поступления новых. Поэтому вводится параметр
ρj ∈ (0, 1], управляющий скоростью забывания старых оценок. При поступлении
каждого нового пакета Dj частоты слов в старых пакетах уменьшаются:
n̂wt := ρj n̂wt + ñwt .
В случае большой статической коллекции, для которой достаточно одного прохода, можно взять ρj = 1. Тогда по окончании первого прохода ϕwt будут обычными
частотными оценками условных вероятностей. Несколько десятков первых пакетов
всё же лучше учесть с меньшими значениями ρj , чтобы устранить влияние самых
первых пакетов, когда матрица Φ оценивалась ещё слишком грубо.
Уменьшать параметр ρj имеет смысл в случаях малых коллекций, чтобы быстрее забывать оценки предыдущих проходов, а также в случаях больших пакетов |Dj |,
когда матрица Φ неплохо оценивается по каждому отдельному пакету.

§2.4

Стохастический EM-алгоритм

В Алгоритме 2.2 для каждой пары (d, w) происходит распределение ndw вхождений термина w в документ d между всеми |T | темами пропорционально вероятностям
p(t | d, w). При этом приходится хранить массив значений ndwt для всех тем t ∈ T .
Расход памяти объёма O(n|T |) может оказаться неприемлемым даже при небольшом

19

числе тем. В то же время, согласно гипотезе разреженности, употребление термина w
в документе d связано, скорее всего, с небольшим числом тем.
Можно было бы оставлять только несколько наибольших значений ndwt на каждом шаге. Однако эксперименты показывают, что эта эвристика приводит к накоплению систематической ошибки и смещению модели [2]..
Проблема разреживания условного распределения p(t | d, w) адекватно решается с помощью стохастического EM-алгоритма (stochastic EM-algorithm, SEM) [11].
Распределение скрытой переменной t, вычисленное на E-шаге, не используется непосредственно на M-шаге. Вместо этого из него сэмплируется искусственная выборка,
по этой выборке вычисляется эмпирическое распределение, и оно уже используется в формулах M-шага. Это позволяет упростить задачу M-шага, сохранив свойства
несмещённости оценок и сходимости EM-алгоритма. Размер сэмплируемой выборки
является параметром метода.
В случае PLSA реализация SEM сводится к следующему: для каждой пары
(d, w) сэмплируются s случайных тем tdwi , i = 1, . . . , s из распределения p(t | d, w),
возможно, повторяющихся. В формулах M-шага вместо распределения p(t | d, w) используется его несмещённая эмпирическая оценка:
s

1X
p̂(t | d, w) =
tdwi = t .
s i=1

(2.6)

Модификация Алгоритма 2.2, трансформирующая его в стохастический обобщённый EM-алгоритм (PLSA-SEM), состоит из трёх изменений:
1) перед шагом 5 сэмплируется s тем tdwi , i = 1, . . . , s из p(t | d, w);
2) на шаге 5 цикл по всем t ∈ T заменяется циклом по t = tdwi , i = 1, . . . , s;
3) на шаге 6 вычисляется δ := ndw /s.
При s = ndw стохастический EM-алгоритм соответствует сэмплированию Гиббса [62] — одному из основных методов обучения вероятностных тематических моделей. В [1, 2] предложено экономное сэмплирование, когда s уменьшается до 3–5 тем,
что приводит к большему разреживанию и экономии вычислительных ресурсов без
существенной потери качества тематической модели.
Сэмплирование выборки из дискретного распределения. Пусть имеется дискретное распределение p(t), t ∈ T , и требуется получить реализацию случайной величины из этого распределения. Для этого разобьём отрезок [0, 1] на |T | частей длины
p(t) каждый. С помощью датчика случайных чисел сгенерируем случайное число r
из равномерного распределения на отрезке [0, 1]. Номер отрезка t, в который попало
число r, и будет искомой реализацией. Чтобы сгенерировать выборку из s независимых наблюдений, повторим эту процедуру s раз.

§2.5

Начальные приближения

Начальные приближения ϕt и θd можно задавать нормированными случайными
векторами из равномерного распределения.
Другая распространённая рекомендация — пройти по всей коллекции, выбрать
для каждой пары (d, w) случайную тему t и вычислить частотные оценки (1.4) вероятностей ϕwt и θtd для всех d ∈ D, w ∈ W , t ∈ T .

20

Инициализация с частичным обучением применяется в случаях, когда темы известны заранее и имеются дополнительные данные о привязке некоторых документов
или терминов к темам. Учёт этих данных улучшает интерпретируемость тем.
Если известно, что документ d относится к подмножеству тем Td ⊂ T , то в качестве начального θtd можно взять равномерное распределение на этом подмножестве:
0
θtd
=


1 
t ∈ Td .
|Td |

(2.7)

Если известно, что подмножество терминов Wt ⊂ W относится к теме t, то в качестве начального ϕwt можно взять равномерное распределение на Wt :
ϕ0wt =


1 
w ∈ Wt .
|Wt |

(2.8)

Если известно, что подмножество документов Dt ⊂ D относится к теме t,
то можно взять эмпирическое распределение слов в объединённом документе:
P
ndw
0
.
ϕwt = Pd∈Dt
d∈Dt nd

Если нет никакой априорной информации о связи документов с темами, то
последнюю формулу можно применить к случайным подмножествам документов Dt .
В [20] предлагается брать один случайный документ.
Инициализация Θ по Φ. Если для всех тем известны начальные приближения ϕ0wt ,
0
то первая итерация ЕМ-алгоритма при равномерном распределении θtd
= 1/|T | даёт
ещё одну интуитивно очевидную формулу инициализации:
θtd =

X ndw ϕwt
X
1 X
P
ndw Hdwt =
=
p̂(w | d) p̂(t | w).
nd w∈d
nd
s ϕws
w∈d
w∈d

(2.9)

Здесь распределение тем в документе d оценивается путём усреднения распределений
тем p(t | w) по словам документа d, вычисленных по формуле Байеса.
0
Сглаживание. Если полученное начальное приближение ϕ0wt или θtd
содержит нулевые вероятности, то его необходимо сгладить, смешав с каким-нибудь неразреженным распределением. Например, ϕ0wt смешивается с эмпирическим распределением
слов во всей коллекции и со случайным распределением ρ(w), при некоторых значениях параметров смеси τ1 и τ2 :

ϕwt = (1 − τ1 − τ2 )ϕ0wt + τ1 nw /n + τ2 ρ(w).
Эксперименты и рекомендации по способам задания начального приближения.

ToDo6

21

3

Латентное размещение Дирихле

Основным недостатком PLSA считается высокая размерность пространства параметров, вызывающая переобучение [9]. В задачах машинного обучения для сокращения размерности обычно используется либо отбор признаков, приводящий
к уменьшению числа параметров, либо регуляризация — наложение дополнительных ограничений на параметры. В частности, байесовская регуляризация основана
на введении априорного распределения вероятности в пространстве параметров.
Тематическая модель латентного размещения Дирихле (latent Dirichlet
allocation, LDA) [9] основана на разложении (1.2) при дополнительном предположении, что векторы документов θd = (θtd ) ∈ R|T | и векторы тем ϕt = (ϕwt ) ∈ R|W | порождаются распределениями Дирихле с параметрами α ∈ R|T | и β ∈ R|W | соответственно:
Γ(α0 ) Q αt −1
Dir(θd ; α) = Q
θ
,
Γ(αt ) t td

αt > 0,

α0 =

w

αt ,

P

βw ,

θtd > 0,

t

t

Γ(β0 ) Q βw −1
ϕ
,
Dir(ϕt ; β) = Q
Γ(βw ) w wt

P

βw > 0,

β0 =

w

P

θtd = 1;

t

ϕwt > 0,

P

ϕwt = 1.

w

где Γ(z) — гамма-функция.

Некоторые свойства распределения Дирихле. Математическое ожидание и дисперсия t-й координаты вектора θd равны, соответственно,
Z
αt
αt (α0 − αt )
Dθtd = 2
.
(3.1)
Eθtd = θtd Dir(θd ; α) dθd = ,
α0
α0 (α0 + 1)
Векторный параметр α определяет степень разреженности векторов θd , порождаемых распределением Dir(θ; α). Если αt = 1 для всех t, то распределение Дирихле
переходит в равномерное. Чем больше α0 , тем меньше дисперсия, и тем сильнее векторы θd концентрируется вокруг вектора математического ожидания Eθd . Чем меньше αt , тем сильнее значения θtd концентрируются вокруг нуля. Чем меньше α0 , тем
более разрежен вектор θd . Поэтому αt называют параметрами контраста.

Обоснования. Есть несколько доводов в пользу распределения Дирихле как байесовского регуляризатора вероятностных тематических моделей.
Во-первых, это достаточно широкое параметрическое семейство распределений
на единичном симплексе, которое описывает как разреженные, так и сконцентрированные дискретные распределения.
Во-вторых, модель LDA хорошо подходит для описания кластерных структур.
Чем меньше значения гиперпараметров α и β, тем сильнее разрежено распределение Дирихле, и тем дальше отстоят друг от друга порождаемые им векторы. Чем
меньше α0 , тем сильнее различаются документы θd . Чем меньше β0 , тем сильнее различаются темы ϕt . Векторы ϕt = p(w | t) в пространстве терминов R|W | представляют
центры тематических кластеров. Элементами кластеров являются векторы документов с эмпирическими распределениями p̂(w | d, t). Чем меньше гиперпараметры β, тем
больше межкластерные расстояния по сравнению с внутрикластерными. Таким образом, гиперпараметры позволяют моделировать тематические кластерные структуры
различной степени выраженности.

22

В-третьих, распределение Дирихле является сопряжённым к мультиномиальному, что упрощает вывод апостериорных оценок вероятностей θtd и ϕwt . Именно
математическое удобство распределения Дирихле в значительной степени определяет его популярность в тематическом моделировании.
Недостатки. Основной недостаток распределения Дирихле — отсутствие убедительных лингвистических обоснований. Предположение, что все распределения θd , d ∈ D
порождаются распределением Дирихле, да ещё и одним и тем же, кажется весьма
произвольным. То же можно сказать и о порождении множества распределений ϕt
для всех тем t ∈ T . Второй недостаток заключается в том, что параметры θtd и ϕwt
не могут обращаться в нуль, что противоречит гипотезе разреженности.

§3.1

Байесовский вывод

Рассмотрим процесс порождения документа d как выборки nd пар тема–термин
Xd = {(t1 , w1 ), . . . , (tnd , wnd )}. В каждой паре (ti , wi ) тема ti выбирается из дискретного распределения p(t | d) = θtd . Следовательно, вероятность встретить каждую
из тем t ровно ntd раз подчиняется мультиномиальному распределению:
nd ! Q ntd
p(Xd |θd ) = Q
θtd .
t ntd ! t

Распределение Дирихле является сопряжённым к мультиномиальному. Это
означает, что при априорном распределении Дирихле θd ∼ Dir(θ; α) апостериорное
распределение вектора θd принадлежит тому же семейству распределений, но с другим значением параметра: θd |Xd ∼ Dir(θ; α′ ). Действительно, по формуле Байеса
p(θd |Xd , α) =

Q ntd αt −1
p(Xd |θd ) Dir(θd ; α)
= C θtd
θtd = Dir(θd ; α′ ),
p(Xd )
t

αt′ = αt + ntd ,

где C — нормировочная константа, не зависящая от θd .
Оценим случайную величину θtd её математическим ожиданием (3.1) по апостериорному распределению:
Z
Z
ntd + αt
p(t|d, Xd, α) = p(t|d)p(θd |Xd , α) dθd = θtd Dir(θd , α′) dθd =
.
(3.2)
nd + α0

Заменив величину ntd её оценкой n̂td , получим байесовскую оценку параметра θtd для EM-алгоритма, отличающуюся от оценки максимума правдоподобия (2.4)
сглаживающими слагаемыми в числителе и знаменателе:
θtd =

n̂dt + αt
.
n̂d + α0

(3.3)

Аналогично выводится сглаженная байесовская оценка и для ϕwt :
ϕwt =

n̂wt + βw
.
n̂t + β0

(3.4)

Замена в обобщённом EM-алгоритме частотных оценок условных вероятностей
(2.3) и (2.4) сглаженными оценками (3.3) и (3.4) трансформирует PLSA в LDA. Более строгое обоснование EM-подобных алгоритмов приводится в [50, 62] для метода
сэмплирования Гиббса и в [54] для метода вариационной байесовской аппроксимации.

23

В [8] показано, что эти и другие известные алгоритмы обучения LDA являются
вариантами EM-алгоритма и отличаются, главным образом, формулой сглаживания
частотных оценок вероятностей. Оптимизация гиперпараметров α и β, предложенная
в [58, 59], ещё сильнее нивелирует различия между моделями. Согласно экспериментам на 7 текстовых коллекциях [8], более эффективным по качеству и по времени является алгоритм свёрнутой вариационной байесовской аппроксимации CVB0
(collapsed variational Bayes). В нашей нотации ему наиболее близок LDA-GEM.
Байесовский подход на данный момент доминирует в тематическом моделировании. Большинство специализированных моделей строятся на основе LDA. Такая
популярность вызвана не лингвистической обоснованностью LDA, а математическим
удобством — сопряжённые распределения допускают аналитическое интегрирование по пространству параметров модели. Однако при попытке усложнения модели,
построения многофункциональных или композитных моделей, байесовский вывод
усложняется настолько, что начинает сдерживать развитие тематического моделирования. Ниже, в разделе 5, мы рассмотрим новый подход — аддитивную регуляризацию тематических моделей, в котором те же оценки получаются гораздо проще,
без байесовского вывода, априорных распределений Дирихле и интегрирования.

§3.2

Сэмплирование Гиббса

Сэмплирование Гиббса (Gibbs sampling, GS) применяется для решения задач
статистического оценивания, когда вычисление или хранение функции распределении слишком ресурсоёмко, в то же время, генерация случайной выборки из этого
распределения не вызывает затруднений. Тогда вместо исходного распределения используется его несмещённая эмпирическая оценка по сэмплированной выборке.
Применение GS к тематической модели LDA предложено в [50], см. Алгоритм 3.1.
Строгие выкладки приводятся в отчёте [62]. Однако LDA-GS можно понимать и намного проще — как специальный случай стохастического EM-алгоритма PLSA-SEM,
в котором для каждой пары (d, w) сэмплируется ровно s = ndw тем, а параметры
ϕwt , θtd обновляются после каждого вхождения термина w в документ d.
Существенное отличие LDA-GS от PLSA-SEM — в использовании сглаженных
оценок условных вероятностей (3.3) и (3.4). В экспериментах сэмплирование Гиббса
действительно плохо работает без сглаживания [2]. При сэмплировании на шаге 6 все
темы должны иметь ненулевые шансы быть выбранными из распределения p(t | d, w),
особенно на начальных итерациях. На выходе Алгоритма 3.1, наоборот, можно выдать несмещённые оценки искомых условных вероятностей, см. шаги 9, 10.
Ещё одно, чисто техническое, отличие LDA-GS от PLSA-SEM — на шаге 5 счётчики уменьшаются на единицу, тем самым i-е вхождение термина w в документ d
не учитывается в оценке распределения p(t | d, w), из которого сэмплируется тема tdwi .
Из теории следует, что эта деталь исключительно важна [62]. Однако в экспериментах она практически не влияет на качество модели [1, 2]. Счётчики можно одновременно уменьшать для старой темы и увеличивать для новой, как в Алгоритме 2.2.
Таким образом, LDA-GS отличается от PLSA-GEM тремя эвристиками: частотой обновления параметров, сэмплированием и сглаживанием. Эти эвристики не связаны друг с другом и могут применяться в любых сочетаниях, порождая целое семейство алгоритмов тематического моделирования.

24

Алгоритм 3.1. LDA-GS: сэмплирование Гиббса для тематической модели LDA.

1
2
3
4
5
6
7
8
9
10

Вход: коллекция D, число тем |T |, гиперпараметры α, β;
Выход: распределения Θ и Φ;
обнулить n̂wt , n̂dt , n̂t для всех d ∈ D, w ∈ W , t ∈ T ;
повторять
для всех d ∈ D, w ∈ d, i = 1, . . . , ndw
если не первая итерация то
t := tdwi ; уменьшить n̂wt , n̂dt , n̂t на 1;
n̂wt + βw n̂dt + αt
сэмплировать тему tdwi из p(t | d, w) ∝
;
n̂t + β0 nd + α0
t := tdwi ; увеличить n̂wt , n̂dt , n̂t на 1;
пока n̂wt , n̂dt не стабилизируются;
ϕwt := n̂wt /n̂t для всех t ∈ T , w ∈ W ;
θtd := n̂dt /nd для всех d ∈ D, t ∈ T ;

§3.3

Оптимизация гиперпараметров

В первых работах по LDA [9] и сэмплированию Гиббса [50], а также в последовавших за ними исследованиях использовались симметричные распределения Дирихле с гиперпараметрами α = (a, . . . , a) и β = (b, . . . , b). Скалярные гиперпараметры a и b либо фиксировались (одна из стандартных рекомендаций: a = 50/|T |,
b = 0.01), либо настраивались путём перебора по сетке значений.
Позже были предложены численные методы оптимизации гиперпараметров, их
обзор и сравнение приводится в диссертации [58]. Большинство методов оптимизации гиперпараметров основаны на максимизации обоснованности (evidence) модели,
определяемой по всей коллекции X = (Xd )d∈D :
Z
Z Y
P (X|α) = P (X|θ)p(θ|α) dθ =
P (Xd |θd ) Dir(θd ; α) dθd =
d∈D

=

Y

d∈D

Y Γ(ntd + αt )

Γ(α0 )
Γ(nd + α0 ) t∈T

Γ(αt )

→ max
α

Метод неподвижной точки [36] — один из самых простых, но не самый лучший —
представляет собой итерационный процесс:
P
ψ(ntd + αt ) − ψ(αt )
αt := αt Pd
,
t ∈ T,
d ψ(nd + α0 ) − ψ(α0 )
′
где ψ(z) = ln Γ(z) = Γ′ (z)/Γ(z) — дигамма-функция.
Этот или другой аналогичный итерационный процесс встраивается между проходами по коллекции, когда значения счётчиков ntd и nd вычислены и фиксированы.
Он выполняется намного быстрее одного прохода коллекции. Таким образом, оптимизация гиперпараметров является вычислительно эффективной.
Эксперименты показали, что оптимизация гиперпараметров существенно улучшает качество тематической модели [59]. Оказалось, что априорное распределение

25

Dir(θ; α) лучше брать несимметричным и оптимизировать вектор гиперпараметров
α = (α1 , . . . , α|T | ), а распределение Dir(ϕ; β) лучше брать симметричным и оптимизировать скалярный гиперпараметр b, причём 0 < b ≪ 1.
Здесь есть неточность, т.к. в статье вводилось двухэтажное распределение Дирихле.

ToDo7

Лирическое отступление: метод простых итераций. Понятие неподвижной точки. ToDo8
Условия сходимости.

§3.4

Действительно ли сглаживание необходимо?

Согласно экспериментам [9], качество модели LDA существенно превосходит
PLSA. По аналогии с задачами классификации и регрессии отсюда был сделан
стандартный вывод, что модель PLSA имеет слишком много параметров θtd , ϕwt ,
что и приводит к переобучению. Байесовская регуляризация накладывает ограничения на параметры, следовательно, должна сокращать эффективную размерность
и уменьшать переобучение. Однако корректное сравнение PLSA и LDA показывает,
что регуляризация Дирихле в тематических моделях играет совсем другую роль.
Регуляризация Дирихле приводит к сглаживанию частотных оценок условных вероятностей (3.3)–(3.4), что является единственным принципиальным отличием
LDA от PLSA. В экспериментах на реальных данных оптимальные значения гиперпараметров αt и βw оказываются близки к нулю [59]. Оценки параметров ϕwt и θtd
в PLSA и LDA заметно отличаются только для терминов, редких в теме, и тем, редких в документе, которые не несут статистически значимой информации о тематике
коллекции. Вообще, вероятностное тематическое моделирование основано на предположении, что «тема» — это статистическое явление, связанное с частым употреблением определённых терминов. Редкие темы и термины должны были бы игнорироваться как шум, но вместо этого LDA, наоборот, повышает оценку их вероятности.
Утверждение о том, что LDA сокращает эффективную размерность пространства параметров [9], звучит неубедительно. PLSA и LDA оценивают параметры
ϕwt и θtd по одним и тем же формулам (3.3)–(3.4). Более того, в LDA вводятся дополнительные гиперпараметры αt , βw , которые тоже приходится оценивать.
Утверждение о том, что LDA гораздо меньше переобучается [9], также нуждается в перепроверке. Качество тематических моделей принято сравнивать по контрольной перплексии, которая может резко повышаться при появлении в контрольных документах редких терминов, имеющих вероятность p(w | d), близкую к нулю.
В PLSA эта вероятность может вообще оказаться равной нулю, и тогда перплексия
формально будет равна +∞. Это выглядит как переобучение, но свидетельствует
скорее о неадекватности меры качества и самой модели PLSA, которая исключает возможность появления в текстах нетематических терминов. Модель LDA менее
чувствительна к шуму за счёт завышенных частотных оценок условных вероятностей ϕwt и θtd . Однако это не решает проблему выделения шума, а лишь скрывает её.
Если из контрольных документов убрать небольшое число наиболее редких терминов или если использовать робастные модели PLSA и LDA, то контрольные перплексии PLSA и LDA практически совпадают [1, 2]. Исследования [33, 63, 31] также
подтверждают, что для больших коллекций нет существенных различий в качестве
моделей PLSA и LDA. Значимые отличия контрольной перплексии PLSA и LDA
в ранних экспериментах [9] объясняются различиями в реализациях алгоритмов обу-

26

чения. В экспериментах [1, 2] для обучения моделей PLSA и LDA использовался один
и тот же алгоритм, отличавшийся только сглаженными оценками в LDA.
Таким образом, роль регуляризации Дирихле в LDA оказывается весьма скромной. Это не сокращение размерности и не уменьшение переобучения, а всего лишь
более осторожное оценивание вероятностей шумовых терминов и тем. Кроме того,
сглаживание необходимо в алгоритме сэмплирования Гиббса, чтобы все темы из распределения p(t | d, w) имели шансы реализоваться; однако это требование скорее техническое и связано с конкретным методом.
Переобучение — не единственный мнимый недостаток PLSA, на который указывает Д. Блэй в [9], мотивируя переход к «более прогрессивной» модели LDA. Вторая
претензия заключается в том, что модель PLSA якобы неадекватно описывает новые
документы. Действительно, для добавления (folding-in) новых документов в коллекцию Т. Хофманн предлагал в [22] фиксировать матрицу Φ, найденную по всем
предыдущим документам, и определять только вектор θd для нового документа. Эта
эвристика основана на предположении, что коллекция достаточно велика, и один
документ d не может существенно повлиять на оценки распределений ϕt . Оно может не выполняться, если документ d содержит значительное число новых терминов
или относится к темам, слабо представленным в коллекции. Фиксация распределений ϕt в момент оценивания распределения θd является, по сути дела, подменой
вероятностной порождающей модели. Однако проблема легко решается путём реорганизации итерационного процесса обучения модели, что и сделано в онлайновом
EM-алгоритме. Единственным существенным отличием LDA от PLSA является регуляризация Дирихле, а варианты EM-алгоритма (рациональный, обобщённый, стохастический, онлайновый и другие) могут быть применены к обеим моделям. В работе
Хофманна 1999 года эти варианты просто не рассматривались, но это вовсе не означает, что модель не позволяет описывать новые документы. Выше мы рассмотрели
онлайновый вариант EM-алгоритма как раз для модели PLSA.
Рассуждения Д. Блэя о недостатках PLSA были впоследствии растиражированы в многочисленных работах его последователей и учеников, без попыток критического переосмысления или какой-либо перепроверки, см. обзоры [51, 14]. Генеральной
линией тематического моделирования стало развитие модели LDA на основе математического аппарата графических моделей и байесовского вывода. Однако априорные распределения Дирихле и их обобщения — процессы Дирихле и Питмана-Йора
не имеют убедительных лингвистических обоснований. Более того, переход от порождающей модели к алгоритму настройки её параметров требует весьма громоздких выкладок, которые резко усложняются при введении более сложных априорных
распределений или совместном моделировании нескольких языковых явлений.
По этим причинам мы будем придерживаться более простого подхода, основанного исключительно на принципе максимума правдоподобия. Мы также будем
избегать везде, где это возможно, избыточных вероятностных допущений и строить
«полувероятностные» модели, в которых дополнительные данные или лингвистические знания могут учитываться не только в порождающей модели, но и путём
модификации функционала или непосредственно метода его оптимизации.

27

4

Робастная тематическая модель

Согласно вероятностной модели (1.2), каждый термин w в каждом документе d порождается некоторой темой t. Однако появление отдельных терминов может
объясняться не только тематикой документа. Возможны, как минимум, ещё два альтернативных объяснения, условно называемых фоном и шумом.
Фон — это общеупотребительные слова, в частности, стоп-слова, не отброшенные на стадии предварительной обработки. Фоновые слова имеют значимые вероятности во многих темах, снижая релевантность тематического поиска.
Шум — это термины, специфичные для конкретного документа, либо редкие
термины, относящиеся к темам, слабо представленным в данной коллекции. Тематическая модель даёт слишком низкие значения вероятности p(w | d) для таких терминов, то есть не способна объяснить их появление в документах коллекции.
Описание фона и шума тематической моделью лишено смысла. Необходимо
каким-то образом исключать их из функционала правдоподобия. При этом неизвестно, какие именно термины являются фоновыми и шумовыми, но известно, что
в целом по коллекции их относительно немного.

§4.1

Тематическая модель с шумом и фоном

Робастная вероятностная тематическая модель SWB (special words with
background) представляет собой вероятностную смесь трёх компонент — тематической, шумовой и фоновой [12]:
p(w | d) =

Zdw + γπdw + επw
;
1+γ+ε

Zdw =

X

ϕwt θtd .

(4.1)

t∈T

где шумовая компонента πdw ≡ pш (w | d) — неизвестное распределение терминов
в документе d, фоновая компонента πw ≡ pф (w) — неизвестное распределение терминов во всей коллекции. Априорные вероятности тематической, шумовой и фоновой
γ
1
ε
компонент модели обозначим, соответственно, qт = 1+γ+ε
, qш = 1+γ+ε
, qф = 1+γ+ε
,
где γ и ε — неотрицательные параметры.
Суть робастной модели в том, что если тематическая компонента Zdw плохо
объясняет избыточную частоту ndw некоторого термина w в некотором документе d,
то она может быть объяснена альтернативным образом либо шумовой компонентной πdw , либо фоновой πw . Таким образом, редкие и общеупотребительные слова
в явном виде описываются как нетематические.
Требуется найти значения вероятностей ϕwt , θtd , πdw , πw , при которых логарифм
правдоподобия достигает максимума:
L(Φ, Θ, Π) =

XX
d∈D w∈d

ndw ln

Zdw + γπdw + επw
→ max ,
Φ,Θ,Π
1+γ+ε

при ограничениях неотрицательности πdw > 0, πw > 0 и нормировки
X
X
X
X
ϕwt = 1,
θtd = 1,
πdw = 1,
πw = 1.
w∈W

t∈T

w∈d

w∈W

(4.2)

28

Чтобы получить формулы М-шага, запишем лагранжиан данной задачи при
ограничениях нормировки и неотрицательности πdw , πw , проигнорировав ограничения неотрицательности θtd и ϕwt , которые будут выполнены автоматически.
Zdw + γπdw + επw
−
1
+
γ
+
ε
d∈D w∈d
 X X

X X
−
λt
ϕwt − 1 −
µd
θtd − 1 −

L (Φ, Θ, Π) =

XX
t∈T

−

X

w∈W

νd

d∈D

−ν

′

ndw ln

X

d∈D



πdw − 1 +

w∈d

X



πw − 1 +

w∈W

X

XX

t∈T

κdw πdw −

d∈D w∈d

κ′w πw .

w∈W

Двойственные переменные κdw , соответствующие ограничениям πdw > 0, должны быть неотрицательны и удовлетворять условиям дополняющей нежёсткости
κdw πdw = 0,

d ∈ D, w ∈ d.

Аналогично, для двойственных переменных κ′w , соответствующих πw > 0:
κ′w πw = 0,

w ∈ W.

По аналогии со стандартным ЕМ-алгоритмом, на E-шаге для каждой пары
(d, w) вычисляются по формуле Байеса условные вероятности тем Hdwt = p(t | d, w):
Hdwt =

ϕwt θtd
,
Zdw + γπdw + επw

(4.3)

t ∈ T,

′
а также условные вероятности того, что термин w является шумом Hdw и фоном Hdw
:

Hdw =

γπdw
;
Zdw + γπdw + επw

′
Hdw
=

επw
.
Zdw + γπdw + επw

(4.4)

Продифференцировав лагранжиан по переменным θtd и ϕwt и приравняв нулю
производные, получим прежние формулы для ϕwt (2.3) и θtd (2.4), с единственным
отличием, что теперь Hdwt вычисляются по новой формуле (4.3).
Продифференцируем лагранжиан по πdw и приравняем нулю производную:
νd =

ndw γ
+ κdw .
Zdw + γπdw + επw

(4.5)

Домножим обе части этого равенства на πdw , просуммируем по всем терминам
w ∈ W , применим условие нормировки вероятностей πdw в левой части и условие
дополняющей нежёсткости в правой части. Получим выражение двойственной переменной νd через все основные переменные:
X
X
γπdw
=
ndw Hdw .
(4.6)
νd =
ndw
Z
dw + γπdw + επw
w∈d
w∈d

29

Поскольку Hdw есть апостериорная вероятность того, что термин w в документе d является шумом, величина νd интерпретируется как оценка числа шумовых
терминов в документе d.
Проделав аналогичные действия для фоновой компоненты, получим
X
ε
ν′ =
ndw
+ κ′w ,
Z
+
γπ
+
επ
dw
dw
w
d∈D
XX
XX
επw
′
ν′ =
ndw
=
ndw Hdw
,
Zdw + γπdw + επw d∈D w∈d
d∈D w∈d
где ν ′ интерпретируется как оценка числа фоновых терминов во всей коллекции.
Мультипликативный М-шаг. Домножим обе части (4.5) на πdw , но не будем суммировать по w. Получим формулу М-шага для шумовой компоненты:
πdw =

1
γπdw
ndw Hdw
.
ndw
= P
ndw′ Hdw′
νd
Zdw + γπdw + επw
w ′ ∈d

Аналогично получается формула М-шага для фоновой компоненты:
P
′
ndw Hdw
1
επw
d∈D
= P P
πw = ′ ndw
.
′
ν
Zdw + γπdw + επw
ndw′ Hdw
′
d∈D w ′ ∈d

Неотрицательность решения πdw , πw гарантируется, коль скоро начальные приближения πdw , πw неотрицательны. Мультипликативный М-шаг приводит к аналогичной проблеме разреженности для переменных πdw и πw , что и для переменных
ϕwt и θtd . Если в начальном приближении значение πdw или πw равно нулю, то оно
сохранится и далее на протяжении итераций. Если в начальном приближении πdw
или πw не равно нулю, то оно так и останется ненулевым.
Аддитивный М-шаг решает проблему разреживания шумовой компоненты [1].
Перепишем (4.5) в другом виде:
ndw γ = (νd − κdw )(Zdw + γπdw + επw ).
Согласно условиям дополняющей нежёсткости, хотя бы одна из двух неотрицательных переменных κdw , πdw должна быть равна нулю. Отсюда следует, что если
ndw γ < νd (Zdw + επw ), то πdw = 0 и κdw > 0. Если же имеет место противоположное
неравенство, то κdw = 0 и πdw находится из уравнения ndw γ = νd (Zdw + γπdw + επw ).
Объединяя оба эти случая, получаем итоговое выражение для πdw :


ndw Zdw + επw
πdw =
−
.
(4.7)
νd
γ
+
Таким образом, если термин w в документе d встречается существенно чаще,
чем предсказывают тематическая и фоновая компоненты модели, то его появление
объясняется особенностями данного документа, и тогда πdw > 0.
Аддитивный M-шаг, в отличие от мультипликативного, приводит к автоматическому выбору структуры разреженности матрицы (πdw )D×W .

30

Алгоритм 4.1. PLSA-ROEM: робастный онлайновый EM-алгоритм.
Вход: коллекция документов D, число тем |T |;
Выход: распределения Θ, Φ, Π;
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

18
19
20
21
22
23

инициализировать ϕwt , πw для всех w ∈ W , t ∈ T ;
n̂wt := 0, n̂t := 0, n̂′w := 0, n̂′ := 0 для всех w ∈ W , t ∈ T ;
для всех пакетов Dj , j = 1, . . . , J
повторять
ñwt := 0, ñt := 0, ñ′w := 0, ñ′ := 0 для всех w ∈ W , t ∈ T ;
для всех d ∈ Dj
инициализировать θtd для всех t ∈ T ;
повторять
P
Zw :=
ϕwt θtd + γπdw + επw для всех w ∈ d;
t∈T
P
νd :=
ndw γπdw /Zw ;
w∈d
P
ñt :=
ndw ϕwt θtd /Zw для всех t ∈ T ;
w∈d  P
θtd := ñt
t ∈ T;
s∈T ñs для всех 
πdw := πdw + ndw /νd − Zw /γ + для всех w ∈ d;
пока θtd и πdw для данного d не сойдутся;
увеличить ñwt , ñt на ndw ϕwt θtd /Zw для всех w ∈ d, t ∈ T ;
увеличить ñ′w , ñ′ на ndw επw /Zw для всех w ∈ d;
ρ n̂ + ñwt
ϕwt := j wt
для всех w ∈ W , t ∈ T ;
ρj n̂t + ñt
ρ n̂′ + ñ′w
πw := j w′
для всех w ∈ W ;
ρj n̂ + ñ′
пока Φ не сойдутся;
n̂wt := ρj n̂wt + ñwt для всех w ∈ W , t ∈ T ;
n̂t := ρj n̂t + ñt для всех t ∈ T ;
n̂′w := ρj n̂′w + ñ′w для всех w ∈ W ;
n̂′ := ρj n̂′ + ñ′ ;

§4.2

Робастный онлайновый EM-алгоритм

Робастная модификация PLSA-ROEM онлайнового PLSA-OEM показана в Алгоритме 4.1. Главное отличие от обычного PLSA в том, что теперь ndw вхождений
термина w в документ d распределяются не только между темами t ∈ T , но также
между шумовой и фоновой компонентами, пропорционально вероятностям

H̃dw = Z1 ϕwt θtd , t ∈ T ; Z1 γπdw ; Z1 επw ,

где Z — нормирующий множитель.
Возможны различные варианты алгоритма PLSA-ROEM: только с шумовой
компонентой (ε = 0), только с фоновой компонентой (γ = 0), с аддитивным и мультипликативным М-шагом. В Алгоритме 4.1 показан вариант с шумом и фоном, аддитивным M-шагом, без сэмплирования и без сглаживания.

31

Сглаживание вводится в Алгоритм 4.1 заменой частотных оценок (2.3)–(2.4)
параметров ϕwt , θtd на шагах 17, 12 байесовскими оценками (3.3)–(3.4).
Сэмплирование вводится заменой распределения H̃dw его эмпирической оценкой, аналогичной (2.6).
О невозможности оптимизации априорных вероятностей шума и фона. Приравняв нулю производные лагранжиана по γ и ε, нетрудно получить формулы для
обновления γ и ε. Однако эксперименты показывают, что с итерациями γ → ∞,
ε → 0, что приводит к полному вырождению тематической модели в униграммную
модель документов. Поэтому параметры γ и ε необходимо фиксировать.

§4.3

Упрощённый робастный алгоритм

Недостатком предыдущей модели является необходимость подбирать параметры γ, ε и хранить параметры πdw , число которых сопоставимо с размером коллекции.
Рассмотрим упрощённую робастную модель, которая вообще не требует дополнительных затрат памяти или времени. Фоновая компонента в ней отсутствует, а шумовая компонента πdw включается только когда Zdw = 0, то есть когда термин w
в документе d оказывается нетематическим:
X


p(w | d) = νd Zdw + Zdw = 0 πdw ,
Zdw =
ϕwt θtd ,
(4.8)
t∈T

где πdw — новые параметры модели, πdw > 0 тогдаP
и только тогда, когда Zdw = 0;
параметр νd определяется из условия нормировки
p(w | d) = 1.
w∈W

Хотя ситуация Zdw = 0 интерпретируется как вполне нормальная, последствия оказываются катастрофическими для стандартной вероятностной модели (1.2):
в функционале правдоподобия (1.6) под логарифмом появляется нуль, распределение
тем (2.1) для данного слова не существует.
Максимизация правдоподобия (1.6) для модели (4.8) снова приводит к частотным оценкам условных вероятностей (2.3)–(2.4), но теперь Hdwt и n̂dwt оцениваются
только по тематическим терминам:


n̂dwt = Zdw > 0 ndw Hdwt .

Таким образом, при вычислении параметров ϕwt и θtd все нетематические термины просто игнорируются.
Оптимальное значение πdw достаточно определять только для тех (d, w), при
которых Zdw = 0. Оно также выражается аналитически и совпадает с униграммной
оценкой условной вероятности p(w | d):
πdw = ndw /nd .
Нормировочный множитель νd равен доле тематических терминов в документе:
X


1 X
νd =
Zdw > 0 πdw =
Zdw > 0 ndw .
nd w∈d
w∈W

Значения параметров πdw и νd не нужны для вычисления тематической компоненты модели — матриц Φ и Θ, но могут понадобиться при вычислении функционалов качества модели, непосредственно зависящих от p(w | d).

32

§4.4

5

Выделение стоп-слов

Регуляризация тематических моделей

Искомое стохастическое матричное разложение ΦΘ определено не единственным образом, а с точностью до невырожденного преобразования: ΦΘ = (ΦS)(S −1 Θ),
при условии, что матрицы Φ′ = ΦS и Θ′ = S −1 Θ также стохастические. Задача тематического моделирования имеет в общем случае бесконечно много решений. Неединственность решения влечёт неустойчивость EM-алгоритма. Стартуя из различных
начальных приближений, он будет сходиться к различным точкам бесконечного множества решений. Соответствующие эксперименты описаны в §13.2.
Задачи, решение которых неединственно или неустойчиво, называются некорректно поставленными. Общий подход к их решению называется регуляризацией [7].
Он заключается в том, чтобы некоторым разумным образом ввести дополнительные
ограничения на Φ, Θ, сузив тем самым множество решений.
Допустим, что наряду с правдоподобием (1.6) требуется максимизировать ещё
n критериев Ri (Φ, Θ), i = 1, . . . , n, называемых регуляризаторами. Для решения
задачи многокритериальной оптимизации будем максимизировать линейную комбинацию критериев L и Ri с неотрицательными коэффициентами регуляризации τi ,
при условии неотрицательности и нормировки столбцов матриц Φ и Θ:
R(Φ, Θ) =

n
X

τi Ri (Φ, Θ),

L(Φ, Θ) + R(Φ, Θ) → max,
Φ,Θ

i=1

(5.1)

Решение этой задачи приводит к обобщению формул М-шага в ЕМ-алгоритме:


∂R
∂R
n̂wt + ϕwt ∂ϕ
(Φ,
Θ)
(Φ,
Θ)
n̂
+
θ
dt
td
∂θtd
+
+
wt
 ,
 ,
ϕwt = P
θtd = P
(5.2)
∂R
∂R
n̂ut + ϕut ∂ϕ
n̂
+
θ
(Φ,
Θ)
(Φ,
Θ)
ds
sd
∂θ
+
+
ut
sd
u∈W

s∈T

где n̂wt , n̂dt определяются по прежним формулам (2.3)–(2.4).

Можно ли обосновать сходимость по аналогии с обычным ЕМ?
Есть ли что-то похожее в литературе?

Знаменатель в этих формулах нужен только для нормировки. Поэтому используется также сокращённая запись через знак пропорциональности ∝:




∂R(Φ, Θ)
∂R(Φ, Θ)
ϕwt ∝ n̂wt + ϕwt
,
θtd ∝ n̂dt + θtd
.
∂ϕwt
∂θtd
+
+
Добавление ещё одного регуляризатора приводит к добавлению соответствующей поправки в формуле M-шага. Это позволяет единообразно строить многоцелевые
тематические модели, сочетающие в себе большое число дополнительных требований, а также композитные тематические модели, объединяющие в себе несколько
более простых моделей.
Полувероятностный подход. Многие регуляризаторы допускают вероятностную
интерпретацию. Если регуляризатор с точностью до константного сомножителя является логарифмом некоторого априорного распределения, то задача (5.1) эквивалентна максимизации апостериорной вероятности.

ToDo9

33

Мы будем придерживаться более гибкой концепции полувероятностного подхода. Наличие вероятностной интерпретации — желательное, но не обязательное свойство регуляризатора. Более ценной является возможность комбинирования различных регуляризаторов, независимо от того, какие они имеют обоснования.
Принцип регуляризации некорректно поставленных задач не нуждается в дополнительных вероятностных обоснованиях. Достаточно того, что левая часть функционала L(Φ, Θ) является логарифмом правдоподобия, а правая часть R(Φ, Θ) позволяет выбрать из множества решений наиболее подходящее.
Далее рассматриваются многочисленные примеры полезных регуляризаторов.

§5.1

Сглаживание и разреживание

Сглаживающий регуляризатор Дирихле. Сглаженные частотные оценки условных вероятностей (3.3)–(3.4), обычно получаемые через априорные распределения
Дирихле и байесовский вывод, могут быть получены также через регуляризатор.
Зададим дискретные распределения на множестве терминов β̃ = (β̃w )w∈W и на
множестве тем α̃ = (α̃t )t∈T . Потребуем, чтобы распределения ϕt были похожи на β̃,
распределения ϕd — на α̃. Для этого будем минимизировать суммы KL-дивергенций:
X
X
KLw (β̃w kϕwt ) → min;
KLt (α̃t kθtd ) → min .
Φ

t∈T

Θ

d∈D

Для одновременной минимизации обеих сумм KL-дивергенций введём сумму
двух регуляризаторов c коэффициентами β0 и α0 :
XX
XX
R(Φ, Θ) = β0
β̃w ln ϕwt + α0
α̃t ln θtd → max .
t∈T w∈W

d∈D t∈T

Обозначим βw = β0 β̃w и αt = α0 α̃t . Применив общую формулу M-шага (5.2),
получим сглаженные байесовские оценки (3.3)–(3.4) в модели LDA:
ϕwt ∝ n̂wt + βw ,

θtd ∝ n̂dt + αt .

Будем называть данный регуляризатор сглаживающим регуляризатором Дирихле, хотя он не использует распределение Дирихле. Название подчёркивает его
связь с моделью латентного размещения Дирихле LDA, на основе которой в настоящее время строится подавляющее большинство более сложных тематических моделей. Популярность модели LDA объясняется тем, что главным способом оценивания
параметров Φ и Θ до сих пор был байесовский вывод с его чисто техническим требованием, чтобы априорное распределение было сопряжено с мультиномиальным,
то есть было распределением Дирихле.
В теории регуляризации тематических моделей распределение Дирихле утрачивает не только лингвистическую, но и математическую целесообразность. Это лишь
один из возможных регуляризаторов, не самый лучший и не настолько универсальный, как принято считать. В качестве базовой модели логичнее брать более простую
модель PLSA, которая не имеет собственных регуляризаторов, и добавлять к ней
регуляризаторы, адекватные конкретной задаче.

34

Разреживающий энтропийный регуляризатор. Недостатком сглаживающего
регуляризатора является его явное противоречие с гипотезой разреженности.
Для практических целей (классификации, категоризации, информационного поиска и т. д.) было бы полезно иметь тематическую модель с сильно разреженными
матрицами Φ и Θ, в которых доля нулевых значений превышает 90%.
Чем сильнее разрежено распределение, тем ниже его энтропия. Максимальной
энтропией обладает равномерное распределение. Поэтому будем максимизировать
KL-дивергенцию между равномерным распределением и искомыми распределениями ϕt и θd . Назовём энтропийным регуляризатором сумму дивергенций по всем
темам t и всем документам d с коэффициентами регуляризации β и α:
XX
XX
R(Φ, Θ) = −β
ln ϕwt − α
ln θtd → max .
t∈T w∈W

d∈D t∈T

Применив общую формулу M-шага (5.2), получим:


ϕwt ∝ n̂wt − β + ,
θtd ∝ n̂dt − α + .

Идея энтропийной регуляризации была предложена в динамической тематической модели PLSA для обработки видеопотоков [56]. В данной задаче документами
являются видеозаписи, терминами — признаки на изображениях, темами — появление определённого объекта в течение определённого времени, например, проезд
автомобиля. Сильно разреженное распределение было необходимо для описания моментов возникновения тем. Удивительно, что авторы не заметили возможность применения этой же техники для разреживания распределений ϕt и θd .
Заметим, что сглаживание и разреживание могут быть описаны одной и той же
формулой, без ограничений на знаки параметров β и α.
Максимизация апостериорной вероятности — это байесовский подход, позволяющий совместить разреживание и сглаживание, хотя и с некоторыми неестественными ограничениями. Рассмотрим задачу максимизации совместного правдоподобия
выборки (коллекции) D и модели Φ, Θ при фиксированных гиперпараметрах β, α:
p(D | Φ, Θ)p(Φ; β)p(Θ; α) → max .
Φ,Θ

Полагая, что столбцы матриц Φ и Θ являются независимыми случайными векторами из распределений Дирихле, запишем задачу максимизации правдоподобия:
YY
Y
Y
p(d, w)ndw
Dir(ϕt ; β)
Dir(θd ; α) → max .
t∈T

d∈D w∈d

d∈D

Φ,Θ

Прологарифмировав это произведение и отбросив слагаемые, не влияющие
на положение точки максимума, получим задачу максимизации
XX
X
L(Φ, Θ) =
ndw ln
ϕwtθtd +
d∈D w∈d

+

XX

t∈T

(βw − 1) [ϕwt > 0] ln ϕwt +

t∈T w∈W

+

XX
d∈D t∈T

(αt − 1) [θtd > 0] ln θtd → max,

(5.3)

35

при стандартных ограничениях неотрицательности ϕwt > 0, θtd > 0 и нормировки
X
X
ϕwt = 1,
θtd = 1.
w∈W

t∈T

Эта задача формально совпадает с задачей максимизации обычного логарифма
правдоподобия (первое слагаемое) с аддитивным регуляризатором (второе и третье
слагаемые). Поэтому к ней применима общая формула (5.2).
Условные сомножители [ϕwt > 0] и [θtd > 0] появились в (5.3) из следующих соображений. Распределения Дирихле Dir(ϕt ; β) и Dir(θd ; α) не определены при ϕwt = 0
и θtd = 0 соответственно. Однако исключать возможность их обнуления нельзя согласно гипотезе разреженности. Поэтому мы допускаем обнуление отдельных координат векторов ϕt и θd , исключая соответствующие размерности из распределений
Дирихле. Уменьшение размерности влияет на нормировочные множители, которые
образуют аддитивную поправку к логарифму правдоподобия. Решение задачи максимизации (5.3) не зависит от этой поправки, поэтому она сразу отбрасывается.
Для решения задачи (5.3) применим EM-алгоритм и воспользуемся тем, что
уравнение x = a · [x > 0] имеет решение x = a+ . Получим формулы М-шага:


ϕwt ∝ n̂wt + βw − 1 + ,
θtd ∝ n̂dt + αt − 1 + .
(5.4)
Формулы (5.4) охватывают два противоположных типа регуляризации — сглаживание и разреживание.
При αt > 1 (βw > 1) полученные формулы эквивалентны байесовским сглаженными оценками (3.3)–(3.4) с точностью до чисто технического преобразования гиперпараметров (уменьшения на единицу).
Если αt = 1 (βw = 1), то априорные распределения Дирихле симметричны
и равномерны, регуляризация отключается и формулы (5.4) переходят в оценки максимума правдоподобия (2.3)–(2.4), применяемые в PLSA.
При 0 < αt < 1 (0 < βw < 1) малые значения вероятностей θtd (ϕwt ) могут обнуляться. Однако требование неотрицательности гиперпараметров β (α) сильно ограничивает возможности разреживания. В частности, при использовании сэмплирования Гиббса счётчики n̂wt (n̂dt ) могут принимать только целые значения, поэтому ϕwt
(θtd ) смогут обратиться в нуль только при n̂wt = 0 (n̂dt = 0). Этого совершенно не достаточно для сильного разреживания матриц Φ и Θ, особенно в случаях коллекций
большого размера.
Таким образом, апостериорная вероятность — недостаточно гибкий регуляризатор, поскольку она поощряет сглаживание в большей степени, чем разреживание.

§5.2

Частичное обучение

Детальный анализ и интерпретация построенной тематической модели может
порождать дополнительные обучающие данные о том, что некоторый документ
или термин релевантен или не релевантен определённой теме. Эксперты могут также
просматривать ранжированные списки документов или терминов по темам и формировать обучающие данные о том, что некоторый документ или термин должен
быть ранжирован выше или ниже какого-то другого документа или термина. Привязки документов и терминов к темам помогают фиксировать интерпретации тем,

36

избежать их перемешивания в ходе EM-итераций, повышают устойчивость тематической модели. Поскольку такие привязки возникают лишь для небольшого числа
документов, терминов и тем, задача использования этих данных относится к области
частичного обучения (semi-supervised learning).
Данные о релевантности документов темам. Пусть для некоторых документов d
0
задано распределение θtd
на множестве тем. В частности, это может быть равномерное распределение на подмножестве тем Td ⊂ T , к которым относится документ d,
см. (2.7). Когда задаются такие требования, обычно неизвестно, относится ли документ ещё к каким-то темам, и как распределены вероятности θtd между темами из Td .
0
Поэтому явное ограничение θtd = θtd
является избыточно жёстким. Введём регуляри0
затор, максимизирующий ковариацию между распределениями θtd
и θtd . Для большей общности введём непрерывно дифференцируемую возрастающую функцию µ,
и вместо θtd запишем µ(θtd ):
X
X
0
R(Φ, Θ) = τ
md
θtd
µ(θtd ) → max,
d∈D

t∈T

где md — вес или степень важности документа d. Вес можно полагать равным длине
документа, md = nd , либо единице, если все документы одинаково важны.
Формула для θtd , согласно (5.2), принимает вид
0
θtd ∝ n̂dt + τ md θtd
θtd µ′ (θtd ).

Смысл этой формулы в том, чтобы на каждой итерации EM-алгоритма ещё
немного увеличивать оценки условной вероятности θtd = p(t | d), если известно, что
документ d относится к теме t. Похожая модификация EM-алгоритма для задач
классификации текстов с частичным обучением предлагалась в [43].
Это тоже сглаживание, но, в отличие от LDA, оно производится только для тех
θtd и ϕwt , по которым имеются обучающие данные.
Выбор возрастающей функции µ в значительной степени произволен.
При µ(z) = z максимизируется взвешенная сумма ковариаций cov(θd0 , θd ). Если
0
распределение θtd
равномерно на Td , то ковариация не накладывает никаких ограничений на распределение вероятностей θtd между темами из Td .
При µ(z) = ln z максимизация R эквивалентна минимизации взвешенной суммы
0
дивергенций KL(θd0 kθd ). В этом случае распределение θtd стремится к θtd
. Логарифм —
′
это единственная функция µ, для которой zµ (z) ≡ 1 и формула M-шага имеет наиболее простой вид с правой частью, не зависящей от переменных θtd :
0
θtd ∝ n̂dt + τ md θtd
.

Полученная формула интерпретируется как добавление в документ d «вирту0
ального термина» с частотой τ md θtd
, который всегда относится к теме t.
Найти похожие подходы в литературе.

Данные о релевантности терминов темам. Пусть для некоторых тем t задана
функция распределения ϕ0wt на множестве терминов. В частности, это может быть

ToDo10

37

равномерное распределение на подмножестве терминов Wt ⊂ W относящихся к теме t, см. (2.8). Введём регуляризатор
X
X
R(Φ, Θ) = τ
mt
ϕ0wt µ(ϕwt) → max,
t∈T

w∈W

где mt — вес темы, который можно полагать равным единице.
Формула для ϕwt , согласно (5.2), принимает вид
ϕwt ∝ n̂wt + τ mt ϕ0wt ϕwt µ′ (ϕwt ).
При µ(z) = ln z максимизация R эквивалентна минимизации взвешенной суммы
KL-дивергенций KL(ϕ0t kϕt ), и формула M-шага приобретает наиболее простой вид:
ϕwt ∝ n̂wt + τ mt ϕ0wt .
что интерпретируется как добавление в коллекцию D «виртуального документа»,
в котором термин w с частотой τ mt ϕ0wt всегда относится к теме t.
Найти похожие подходы в литературе.

ToDo11

Данные о переранжировании. Допустим, эксперты имеют возможность просмотреть список тем по любому документу d, ранжированный по убыванию частот n̂dt .
Эксперт может перенести любую тему на то место в списке, которое он считает наиболее релевантным.
Эти данные, полученные от экспертов, легко учесть в EM-алгоритме. Чтобы
тема t оказалась на k-м месте в списке тем документа d, достаточно сделать значе(k)
ние n̂dt немного бо́льшим k-го значения n̂dt и скорректировать счётчик n̂d :
если тема t для документа d должна быть на k-м месте то
(k)
(k−1) 
(k)
(k+1) 
n̂′dt := 12 n̂dt + n̂dt
[k > 1] + 21 3n̂dt − n̂dt
[k = 1];
′
n̂d := n̂d − n̂dt + n̂dt ;
n̂dt := n̂′dt ;
Аналогичным образом возможно собрать и учесть данные о переранжировании
терминов. Допустим, по теме t имеется список терминов, ранжированный по убыванию частот nwt , и эксперт может проделать с ним аналогичную работу. Чтобы
термин w оказался на k-м месте в списке терминов темы t, достаточно сделать зна(k)
чение n̂wt немного бо́льшим k-го значения n̂wt и скорректировать счётчик n̂t :
если термин w для темы t должен быть на k-м месте то
(k)
(k−1) 
(k)
(k+1) 
n̂′wt := 12 n̂wt + n̂wt
[k > 1] + 12 3n̂wt − n̂wt
[k = 1];
′
n̂t := n̂t − n̂wt + n̂wt ;
n̂wt := n̂′wt ;
Таким образом, различные виды априорной информации о связях документов
и терминов с темами формализуются либо с помощью регуляризаторов, либо непосредственно через модификацию значений счётчиков. В обоих случаях они приводят
к модификации формул M-шага.
Найти похожие подходы в литературе.

ToDo12

38

§5.3

Разреживание как L0-регуляризация

Рассмотрим ещё один разреживающий регуляризатор, равный числу нулевых
параметров в матрицах Φ и Θ:
X X
X X


R(Φ, Θ) = τϕ
ϕwt = 0 + τθ
θtd = 0 → max .
(5.5)
w∈W t∈T

d∈D t∈T

Данный критерий не является гладким, поэтому воспользоваться общими формулами (5.2) не удастся. Выбор максимального подмножества коэффициентов для
обнуления — это задача комбинаторной оптимизации. Для её решения предлагается
эвристический алгоритм постепенного принудительного разреживания.
Принудительное разреживание. Допустим, что EM-алгоритм сошёлся в точку
локального максимума правдоподобия L(Φ, Θ), и первые производные правдоподобия (1.6) по всем параметрам ϕwt , θtd равны нулю. Зададимся вопросом: обнуление
каких параметров меньше всего повлияет на значение правдоподобия? Применим
ту же технику, которая используется в методе оптимального разреживания многослойных нейронных сетей OBD (Optimal Brain Damage) [29]. Разложив правдоподобие в ряд Тейлора в окрестности точки максимума, получим квадратичную форму по
приращениям параметров ∆ϕwt , ∆θtd (здесь выписаны не все частные производные,
однако нетрудно показать, что остальные частные производные либо равны нулю,
либо в сумме дают нулевой вклад в квадратичную форму):
1 X XX
∂ 2 L(Φ, Θ)
∆ϕwt ∆ϕws
+
2 w∈W t∈T s∈T
∂ϕwt ∂ϕws
1 XXX
∂ 2 L(Φ, Θ)
∆θtd ∆θsd
+ o(∆Φ, ∆Θ).
+
2
∂θ
td ∂θsd
t∈T s∈T

L(Φ+∆Φ, Θ+∆Θ) = L(Φ, Θ) +

d∈D

Обнулить параметр ϕwt означает положить ϕwt + ∆ϕwt = 0, откуда следует
∆ϕwt = −ϕwt . Аналогично, ∆θtd = −θtd . Возьмём вторые производные правдоподобия
и перегруппируем слагаемые:
L(Φ+∆Φ, Θ+∆Θ) = L(Φ, Θ) −

1X X
1X X
n̂t
ϕwt −
n̂d
θtd + o(∆Φ, ∆Θ).
2 t∈T w∈W
2
t∈T
d∈D

Переделать. Расписать случаи, когда обнуляется:
1) одна ячейка теты или фи,
2) одна строка фи, за исключением небольшого числа ячеек,
3) одна строка теты, за исключением небольшого числа ячеек.

Заметим, что в стандартном методе OBD обычно пренебрегают смешанными
частными производными, чтобы упростить вид квадратичной формы. В данном случае квадратичная форма упрощается благодаря специальному виду функционала,
и делать какие-либо приближения нет необходимости.
Из полученной формулы следует интуитивно очевидная стратегия разреживания: после каждого прохода коллекции в каждом распределении ϕwt = n̂wt /n̂t и
θtd = n̂dt /n̂d обнуляются наименьшие значения вероятностей, для которых сумма

ToDo13

39

счётчиков n̂wt и n̂dt , соответственно, не превышает некоторый порог. Варьирование
этого порога эквивалентно варьированию коэффициента регуляризации τ .
Эвристические стратегии разреживания и результаты экспериментов подробно
обсуждаются в разделе §13.4.
Сокращение тематики и словаря. Принудительное разреживание может приводить к обнулению строк целиком в матрицах Θ и Φ.
Обнуление t-й строки в матрице Θ означает, что тема t исключается из тематической модели. Это позволяет построить процедуру оптимизации числа тем, если
задавать изначально избыточное число тем и постепенно в процессе итераций избавляться от лишних тем.
Проверить работоспособность этой идеи в эксперименте.

ToDo14

Обнуление w-й строки в матрице Φ означает, что слово w не существенно для
тематической модели. Сокращение словаря полезно с точки зрения вычислительной эффективности и повышения интерпретируемости модели за счёт отбрасывания
незначимых слов. Однако эксперименты показали, что отбрасываются в основном
наименее частотные слова (с наименьшими nw ), что практически эквивалентно грубой фильтрации словаря, вообще не требующей построения тематической модели.
Пока есть только предварительный эксперимент [L.Evans, 2013]. Посчитать процент ToDo15
совпадения множеств слов, отброшенных разреживанием и грубым частотным фильтром.
Скорее всего, они почти совпадут, и это будет поводом для перехода к ковариационному
регуляризатору.

Далее рассматривается метод максимизации ковариации тем, который также
приводит к разреживанию, но не имеет этого недостатка.

§5.4

Повышение различности тем

Тематическая модель тем полезнее, чем более различные темы она находит.
Это предположение приводит к дополнительному требованию увеличивать различность тем. Можно по-разному формализовать понятие различности тем как дискретных распределений ϕwt = p(t | w) или нормированных векторов ϕw = (ϕwt )t∈T .
Остановимся на естественной мере различности — ковариации:
X
τX X
R(Φ, Θ) = −
cov(ϕt , ϕs ) → max,
cov(ϕt , ϕs ) =
ϕwt ϕws .
2 t∈T
w∈W
s∈T \t

Этот критерий не зависит от Θ, поэтому для θtd формулы M-шага не меняются.
Формула для ϕwt , согласно (5.2), принимает вид


X
ϕwt ∝ n̂wt − τ ϕwt
ϕws .
s∈T \t

+

Смысл этой формулы в том, что условные вероятности ϕwt = p(w | t) постепенно
уменьшаются для тех слов w, которые имеют бо́льшие значения вероятности ϕws
в других темах. В процессе итераций EM-алгоритма для каждого слова вероятности
наиболее значимых тем приобретают всё большие значения, а вероятности менее

40

значимых тем уменьшаются и могут обращаться в нуль. При достаточно больших τ
требование некоррелированности приводит к разреживанию матрицы Φ.
Регуляризатор, описанный в предыдущем параграфе, также приводит к разреживанию, но минимизация ковариаций не так агрессивно обнуляет строки матрицы Φ, соответствующие редким словам. Кроме того, данный регуляризатор обладает
дополнительным полезным свойством выделять стоп-слова в отдельные темы [52].
Реализовать и сравнить в экспериментах с принудительным разреживанием. Прове- ToDo16
рить, что действительно «не так агрессивно обнуляет строки» — пока это только гипотеза.
Кажется, что от принудительного разреживания откажемся в пользу антикоррелятора.
Вариант реализации: как только изменяется ϕwt , надо пробежать по всем ϕws , кото- ToDo17
рые от него зависят, и прибавить разность. Эксперимент: то ли это надо сделать обязательно, то ли это просто ускорит сходимость, то ли это без разницы.

§5.5

Повышение когерентности тем

Тема называется когерентной, если термины, наиболее частые в данной теме,
неслучайно часто совместно встречаются рядом в документах коллекции [40, 41, 42].
Для повышения когерентности темы необходимо заранее вычислить оценки Cuv совместной встречаемости пар слов (u, v). Они могут оцениваться как по сторонней
коллекции, например, по Википедии [39], так по коллекции, для которой строится
модель [35]. Будем полагать, что Cuv > 0, причём если слова u и v совместно не встречаются, то Cuv = 0. Для экономии времени и памяти оценки Cuv можно вычислять
только для самых частых слов или сохранять только те значения Cuv , которые превышают некоторый порог. Обозначим через Q множество пар слов (u, v), для которых
имеется оценка Cuv . Будем называть такие пары слов когерентными.
Оценки совместной встречаемости принято вычислять на основе документной
частоты терминов. Введём следующие обозначения:
Nuv — число документов, в которых термины u, v хотя бы один раз встречаются
рядом, как правило, в окне ширины h = 10 слов [39];
Nu — число документов, в которых термин u встречается хотя бы один раз;
N = |D| — число документов в коллекции;
LCPuv = ln(Nuv /Nu ) — логарифм условной вероятности (log conditional probability), мера связанности слова v со словом u;
PMIuv = ln(Nuv N/Nu Nv ) — поточечная взаимная информация (pointwise mutual information), мера неслучайности совместного употребления слов u и v; если
появление слов u и v — статистически независимые события, то PMIuv ≈ 0;
IDFu = ln(N/Nu ) — инвертированная документная частота (inverted document
frequency), в информационном поиске применяется для понижения веса слишком
частных слов, в том числе стоп-слов.
Наиболее адекватной мерой когерентности отдельных тем и тематической модели в целом принято считать среднее значение LCPuv , по всем парам из 10 наиболее
частых слов в каждой теме [35, 28]. Среднее значение PMIuv , использовавшаяся ранее, признано менее удачной мерой [35]. Определение когерентности рассматривается
более подробно в §12.6, стр. 64. Вопросы о том, какую величину Cuv взять для опти-

41

мизации когерентности, и как построить критерий регуляризации, пока не выяснены
столь однозначно и требуют дополнительных исследований.
Регуляризатор ковариационного типа. В работе [39] предлагается использовать
оценку совместной встречаемости Cuv = Nuv PMIuv > 0 и регуляризатор
X
X
R(Φ, Θ) = τ
ln
Cuv ϕut ϕvt → max .
t∈T

(u,v)∈Q

Формула для ϕwt , согласно (5.2), имеет вид
P
P
Cuw ϕut +
Cwv ϕvt
ϕwt ∝ n̂wt + τ ϕwt

(u,w)∈Q

(w,v)∈Q

P

Cuv ϕut ϕvt

.

(u,v)∈Q

Таким образом, условные вероятности ϕwt увеличиваются для тех слов w, которые имеют когерентные им слова u с высокими вероятностями ϕut .
Упрощённые варианты регуляризаторов ковариационного типа. Можно использовать и другие меры близости векторов (Cuv )(u,v) и (ϕut ϕvt )(u,v) . При этом вероятности перераспределяются между когерентными словами несколько иначе.
Убрав логарифм из R(Φ, Θ), получим более простую формулу для ϕwt :
 P

P
ϕwt ∝ n̂wt + τ ϕwt
Cuw ϕut +
Cwv ϕvt .
(u,w)∈Q

(w,v)∈Q

Если в качестве регуляризатора взять сумму KL-дивергенций для пар слов,
X X
R(Φ, Θ) = τ
Cuv ln(ϕut ϕvt ) → max,
t∈T (u,v)∈Q

то формула для ϕwt упростится ещё больше:
 P

P
ϕwt ∝ n̂wt + τ
Cuw +
Cwv .
(u,w)∈Q

(w,v)∈Q

Регуляризатор, соответствующий обобщённой урновой схеме Пойя. В работе [35] предлагается нормированная оценка
(
Nu IDFu ,
u = v;
C̃uv


Cuv = P
;
C̃uv =
Nuv IDFu IDFu > 3 , u 6= v.
C̃wv
w∈W

Про регуляризацию в явном виде не говорится. Обоснование проводится для
алгоритма сэмплирования Гиббса с помощью обобщённой урновой схемы Пойя, что,
на наш взгляд, только затрудняет понимание. В результате получается несколько
необычная формула для ϕwt :
P
Cwu n̂ut + β
u∈W
ϕwt =
.
n̂t + |W |β

42

Необычность в том, что коэффициент при n̂wt равен не 1, а величине Cww ,
которая после нормировки имеет неясную интерпретацию.
Вычисления по этой формуле в алгоритме сэмплирования Гиббса организуются
следующим образом. Всякий раз, когда счётчик n̂ut увеличивается на δ = 1, в цикле
по всем словам w счётчик n̂wt увеличивается на Cwu δ. Чтобы это не занимало много
времени, матрица Cuv должна быть сильно разреженной. Для этого вводится эвристика IDFu > 3, возможно также вводить и другие эвристики, например, PMIuv > 0.
Модифицируем формулу ϕwt так, чтобы она приобрела стандартный вид, оставаясь в то же время реализацией обобщённой урновой схемы Пойя. Введём управляющий параметр τ и уберём параметр β, который является очевидным следствием
вездесущей регуляризации Дирихле:
X
ϕwt ∝ n̂wt + τ
Cuw n̂ut .
u∈W \w

Нетрудно показать, что эта формула соответствует регуляризатору
X X
R(Φ, Θ) = τ
Cuv n̂ut ln ϕvt → max,
t∈T (u,v)∈Q

который также несколько необычен тем, что коэффициенты n̂ut изменяются в ходе
итераций. Тем не менее, этот регуляризатор имеет совершенно ясную интерпретацию.
Возьмём Cuv = Nuv /Nv — оценку условной вероятности слова v при условии u. Тогда
данный регуляризатор минимизирует сумму дивергенций Кульбака-Лейблера
XX
KL(n̂v|t kϕvt ) → min
t∈T v∈W

между распределением ϕvt = p(v | t) и эмпирической оценкой n̂v|t частоты слова v
в теме t, вычисленной по оценкам частот всех когерентных слов:
X
X
n̂v|t = p̂(v | t)n̂t =
p̂(v | u)p̂(u | t)n̂t =
Cuv n̂ut .
u : (u,v)∈Q

u : (u,v)∈Q

Таким образом, алгоритм обобщённой урновой схемы Пойя [35] также можно
считать разновидностью регуляризации.

§5.6

Учёт связей между документами

Ещё до построения тематической модели может быть известно, что какие-то документы имеют схожую тематику. Это могут быть документы, относящиеся к одной
рубрике тематического рубрикатора, или документы, сгруппированные пользователем электронной библиотеки по тематике, или документы, где-то упоминавшиеся
вместе. В частности, это могут быть документы, ссылающиеся друг на друга [16].
Научные статьи ссылаются на другие статьи через списки литературы. Веб-страницы
или статьи Википедии используют для этого гиперссылки. Одна и та же ссылка может многократно упоминаться в тексте, и эту важную частотную информацию также
необходимо учитывать.

43

Тематические наборы документов. Обозначим через Dy ⊂ D тематические наборы документов, предположительно имеющих схожую тематику, где индекс y пробегает заданное конечное множество Y . Тематических наборов может быть много,
и они могут противоречить друг другу. Поэтому требование, чтобы документы d, d′
из одного тематического набора Dy имели похожие распределения θd , θd′ , должно
учитываться не жёстко. Один из возможных вариантов регуляризации — максимизация суммы ковариаций:
X
τX X
R(Φ, Θ) =
cov(θd , θd′ ) → max,
cov(θd , θd′ ) =
θtd θtd′ .
2 y∈Y ′
t∈T
d,d ∈Dy

Этот критерий не зависит от Φ, поэтому для ϕwt формулы M-шага не меняются.
Формула для θtd , согласно (5.2), принимает вид
X X
θtd ∝ n̂dt + τ θtd
θtd′ .
y∈Y d′ ∈Dy \d

Смысл этой формулы в том, что условные распределения θtd = p(t | d) документов d, d′, принадлежащих одной тематической подборке, в ходе итераций постепенно
приближаются друг к другу.
Найти похожие работы в литературе.

Ссылки и гиперссылки. Ковариационный регуляризатор легко обобщается на случай, когда на множестве документов задан направленный граф связей G = hD, Ei,
где E — множество рёбер графа. Ребро графа (d, c) ∈ E означает, что документ d
ссылается на документ c или цитирует его [16]. Будем считать, что в таком случае
тематика документа c близка к тематике документа d. При этом сходство тематики
пропорционально ndc — числу ссылок на документ c в документе d. Формализуем
эти предположения с помощью регуляризатора
X
R(Φ, Θ) = τ
ndc cov(θd , θc ) → max .
(d,c)∈E

В [16] предложена похожая модель LDA-JS, в которой вместо максимизации
ковариации минимизируется дивергенция Йенсена-Шеннона
между θd и θc .
	
Обозначим через Nd = c ∈ D : (d, c) ∈ E множество вершин, смежных с d.
Формула M-шага для θtd , согласно (5.2), принимает вид
X
θtd ∝ n̂dt + τ θtd
ndc θtc .
c∈Nd

Таким образом, условные распределения θtd = p(t | d) в ходе итераций приближаются к распределениям θtc документов, связанных с d.

§5.7

Траектория регуляризации

При использовании линейной комбинации регуляризаторов Ri возникает проблема выбора вектора коэффициентов τ = (τi )ni=1 . Аналогичная проблема эффективно решается в эластичных сетях (elastic net) при совмещении L1 - и L2 -регуляризации

ToDo18

44

для задач регрессии и классификации [70]. Там решение вычисляется одновременно
для множества векторов (regularization path), за время, сравнимое со временем решения одной задачи при фиксированном векторе коэффициентов [19]. Общие методы
многопараметрической регуляризации предложены в [23].
В задачах тематического моделирования регуляризаторы могут влиять друг
на друга. Разреживание влияет на большинство регуляризаторов. Согласно экспериментам, некоторые регуляризаторы могут ухудшать сходимость, если включать их
слишком рано или слишком резко. Поэтому предлагается увеличивать коэффициенты регуляризации постепенно и в определённой последовательности, выстраивая
в ходе итераций EM-алгоритма траекторию в пространстве коэффициентов регуляризации (regularization trajectory).
Есть ли в литературе такой подход? Внимательнее посмотреть [23].

6

Тематические модели классификации

Многие текстовые коллекции содержат для каждого документа d дополнительную информацию, называемую также метаинформацией, например:
—
—
—
—
—
—
—
—
—
—

время yd создания или публикации документа d;
список авторов Ad документа d;
список документов Dd′ , на которые ссылается d;
список авторов A′d , на которых ссылается d;
список документов Dd′′ , которые ссылаются на d;
список авторов A′′d , которые ссылаются на d;
список категорий Cd рубрикатора, к которым относится d;
список сущностей Ed , упоминаемых в документе d;
список ярлыков Ld , присвоенных пользователями документу d.
список пользователей Ud документа d.

Для этих и других подобных типов информации задача формализуется единообразно. Каждому документу соответствует набор элементов из конечного множества C, называемых классами (class), метками классов или просто метками (label).
Предполагается, что если документы имеют одинаковые метки, то они имеют также
схожую тематику. Поэтому учёт меток может улучшать интерпретируемость тем,
даже если между классами и темами нет однозначного соответствия.
Задача заключается в том, чтобы выявить связи между классами и темами,
улучшить качество тематической модели и построить алгоритм классификации новых документов, для которых метки ещё не проставлены.
Сложность задачи в том, что стандартные алгоритмы классификации показывают неудовлетворительные результаты на больших текстовых коллекциях с большим числом несбалансированных, пересекающихся, взаимозависимых классов [48].
Несбалансированность означает, что классы могут содержать как очень малое, так
и очень большое число документов. В случае пересекающихся классов документ может относиться как к одному классу, так и к очень большому числу классов. Взаимозависимые классы имеют схожие множества характерных терминов, и при классификации документа вступают в конкуренцию.

ToDo19

45

Тематические модели лучше справляются с такими задачами, поскольку они
учитывают все классы одновременно [48]. Кроме того, в процессе построения модели
они приписывают метки классов каждому термину w в каждом документе d, что
даёт полезную дополнительную информацию о структуре документов.
Далее мы рассмотрим несколько тематических моделей для классификации документов, в порядке усложнения и отказа от избыточно сильных ограничений.

§6.1

Моделирование классов темами

Начнём с простой тематической модели классификации, основанной на двух
довольно сильных ограничениях.
1. Темы отождествляются с классами, C ≡ T . Это сильное предположение
о классах, так как требование условной независимости p(w | d, c) = p(w | c), постулируемое для латентных тем, может не выполняться для наблюдаемых классов.
2. Для каждого документа d точно известно множество всех классов Cd ⊂ C,
к которым он относится. Это предположение подходит лишь для некоторых типов
задач. Для времени и авторов — подходит; для ссылок, категорий, пользователей
и большинства других типов — не подходит.
При сделанных предположениях можно использовать стандартную тематическую модель (1.2), в которой фиксирована структура разреженности матрицы Θ:
θcd = p(c | d) = 0 для всех c ∈
/ Cd ,
X
X
p(w | d) =
p(w | c)p(c | d) =
ϕwc θcd .
c∈Cd

c∈Cd

Формулы E-шага и M-шага также модифицируются не сильно: p(c | d, w), n̂dc ,
θcd вычисляются не для всех c ∈ C, а только для классов документа c ∈ Cd .
Для классификации новых документов d применяется стандартная модель
PLSA/LDA без ограничений на θcd . Документ относится к тем классам c, для которых условные вероятности θcd максимальны.
Данная модель известна в литературе как Flat-LDA [48] и Labeled-LDA [45].
Выразительные возможности данной модели существенно беднее, чем у PLSA/LDA,
так как значительная доля элементов матрицы Θ фиксированы и равны нулю.

§6.2

Моделирование классов распределениями тем

Далее будем полагать, что классы c ∈ C описываются не отдельными темами,
а неизвестными условными распределениями p(t | c) на множестве тем T .
Чтобы говорить о вероятностях классов, расширим вероятностное пространство
до множества D × W × T × C. Будем считать, что с каждым словом w в каждом
документе d связана не только тема t ∈ T , но и класс c ∈ C.
Рассмотрим стандартную тематическую модель (1.2), в которой распределение
вероятности тем документов p(t | d) описывается смесью распределений тем классов
p(t | c) и классов документов πcd = p(c | d), где новой неизвестной является матрица

46

классификаций документов Π = (πcd )C×D :
X
X
p(t | d) =
p(t | c)p(c | d) =
θtc πcd ;
c∈C

p(w | d) =

c∈C

X

p(w | t)

t∈T

X

p(t | c)p(c | d) =

c∈C

X
t∈T

ϕwt

X

θtc πcd .

(6.1)

c∈C

Для этой модели постулируются две гипотезы условной независимости:
p(w | t, c, d) = p(w | t) — распределение слов полностью определяется тематикой
документа и не зависит от самого документа и его классов;
p(t | c, d) = p(t | c) — тематика документа d зависит не от самого документа,
а только от того, каким классам он принадлежит;
p(c | t, d) = p(c | t) — условие, эквивалентное предыдущему — классификация
документа d зависит не от самого документа, а только от его тематики.
EM-алгоритм. Максимизация правдоподобия (1.6) для данной тематической модели p(w | d) приводит к следующим формулам E-шага и M-шага:
ϕwt θtc πcd
Hdwtc = p(t, c | d, w) =
;
p(w | d)
X
ϕwt ∝ n̂wt =
ndw Hdwtc ;
d,c

θtc ∝ n̂tc =

X

ndw Hdwtc ;

d,w

πcd ∝ n̂cd =

X

ndw Hdwtc .

w,t

Информация о классификации документов вводится в модель через матрицу Π,
которая может фиксироваться либо вычисляться в зависимости от задачи.
Рассмотрим три основных случая.
Фиксированная матрица классификаций. Если классификации Cd для каждого
документа d заданы точно и все классы равнозначны, то в качестве πcd можно взять
равномерные распределения на Cd :
p̂(c | d) =


1 
c ∈ Cd .
|Cd |

(6.2)

Этот случай в точности соответствует автор-тематической модели Author-Topic
Model (ATM) [47], в которой классами являются авторы документов, и каждый документ d связан с множеством его авторов Cd . Равномерное распределение p(c | d)
является формализацией предположения о равном вкладе всех авторов c ∈ Cd в создание документа d.
Для некоторых типов классов характерны частотные данные mdc — сколько раз
документ d был отнесён к классу c. Таким свойством, в частности, обладают:
— ссылки на авторов или документы в документе d;
— авторы или документы, ссылающиеся или цитирующие документ d;

47

— сущности, упоминаемые в документе d.
В таких случаях матрицу Π естественно заполнить частотными оценками:
X
πcd = mdc /md ;
md =
mdc .
c∈C

В EM-алгоритме элементы фиксированной матрицы классификаций Π не вычисляются и не хранятся. Величины Hdwtc , n̂cd , вычисляются только для классов
документа c ∈ Cd ; суммирования также производятся только по c ∈ Cd .
Модель Бернулли для бинарных данных mdc .

Матрица классификаций с фиксированными нулями. Если множества классов Cd для всех документов d заданы точно, но вероятности классов πcd = p(c | d)
неизвестны, то в матрице Π фиксируется структура разреженности, то есть только
нулевые значения πcd = 0 для всех (c, d) таких, что c ∈
/ Cd .
В EM-алгоритме величины Hdwtc , n̂cd , πcd вычисляются только для классов документа c ∈ Cd ; суммирования также производятся только по c ∈ Cd .
Регуляризация матрицы классификаций. Если множества классов Cd заданы
неточно или частично, то элементы матрицы Π не фиксируются. Вместо этого вводится регуляризатор, заставляющий распределения πcd = p(c | d) быть ближе к заданным распределениям p̂(c | d) = mdc /md :
X
X
R(Π) = τ
mdc
ln πcd → max .
d∈D

c∈Cd

В EM-алгоритме величины Hdwtc , n̂cd , πcd вычисляются для всех c ∈ C, суммирования также производятся по всем c ∈ C. Регуляризатор приводит к модификации
формулы M-шага только для переменных πcd , по аналогии с общей формулой (5.2),
πcd ∝ n̂cd + τ mdc .
Регуляризация хорошо подходит для задач классификации c частичным обучением, в которых не только про вероятности πcd ничего нельзя сказать заранее,
но даже нет уверенности, что множества Cd включают в себя все классы, которым документ d действительно принадлежит. Задача частичного обучения в том и состоит,
чтобы определить, каким ещё классам принадлежит каждый документ. Типичным
примером являются задачи категоризации текстов или определения пользователей,
которым можно порекомендовать данный документ.

§6.3

Частотный регуляризатор

В задачах классификации с большим числом классов или с несбалансированными классами хорошо зарекомендовала себя частотная регуляризация (label
regularization) [32].
Потребуем, чтобы
оценка безусловного распределения классов по тематической
P
1
1
модели p(c) = |D|
πcd была близка к наблюдаемым частотам классов p̂(c) = |D|
|Dc |,
d∈D

ToDo20

48

где Dc = {d ∈ D : c ∈ Cd } — множество документов, относящихся к классу c. Выразим
данное требование с помощью регуляризатора:
X
X
R(Π) = τ
|Dc | ln
πcd → max .
c∈C

d∈D

Формула M-шага для πcd , по аналогии с (5.2), принимает вид
|Dc |πcd
πcd ∝ n̂cd + τ P
.
πcd′
d′ ∈D

Частотная регуляризация использовалась в тематической модели Prior-LDA,
которая была предложена в [48] как улучшение модели Flat-LDA.

§6.4

Тематическая модель классификации

Тематическая модель (6.1) описывает распределение слов в документах p(w | d)
через распределения p(t | c) и p(c | d), связанные с классами. Если в задаче имеется сразу несколько типов классов, например, время, авторы, ссылки, категории,
то не совсем ясно, какую из классификаций предпочесть как основную, и уже совсем не ясно, как учесть классификации всех остальных типов.
Решение данной проблемы заключается в том, чтобы моделировать распределение классов документов p(c | d) через распределение тем документов θtd = p(t | d)
по аналогии с основной тематической моделью p(w | d):
X
X
p(c | d) =
p(c | t) p(t | d) =
ψct θtd ,
(6.3)
t∈T

t∈T

где новой неизвестной является матрица классов тем Ψ = (ψct )C×T . Для классов
постулируется гипотеза условной независимости p(c | t, d) = p(c | t), означающая, что
для классификации документа d достаточно знать только его тематику.
Будем полагать, что обучающая информация о принадлежности документов
классам задаётся числами mdc > 0, интерпретации которых могут быть различными
в зависимости от задачи, например:
— частота, сколько раз документ d отнесён к классу c;
— бинарный индикатор mdc = [c ∈ Cd ];
— оценка числа слов в документе, относящихся к классу c: mdc = nd [c ∈ Cd ]/|Cd |.
Для построения регуляризатора будем минимизировать KL-дивергенцию между моделью классификации p(c | d) и эмпирическим распределением p̂(c | d) ∝ mdc :
XX
X
R(Ψ, Θ) = τ
mdc ln
ψct θtd → max,
(6.4)
d∈D c∈C

t∈T

где коэффициент регуляризации τ необходим для «приведения к одному масштабу»
частот слов ndw и частот классов mdc .
Тематическая модель с таким регуляризатором в сочетании с частотным регуляризатором и регуляризатором Дирихле известна как Dependency LDA [48]. В оригинальной работе приводится намного более громоздкое обоснование этой модели
в рамках байесовского подхода, графических моделей и сэмплирования Гиббса.

49

EM-алгоритм. Задача решается по-прежнему с помощью EM-алгоритма.
′
На E-шаге дополнительно оценивается условная вероятность Hdct
= p(t | d, c):
ϕwt θtd
Hdwt = P
;
ϕws θsd
s∈T

ψct θtd
′
Hdct
= P
.
ψcs θsd
s∈T

На M-шаге оценки ϕwt вычисляются по прежним формулам; оценки ψct , как
и следовало ожидать, аналогичны ϕwt с точностью до замены терминов w на классы c
′
и Hdwt на Hdct
; оценки θtd агрегируют счётчики терминов и классов в документах:
X
ϕwt ∝ n̂wt ;
n̂wt =
ndw Hdwt ;
d∈D

ψct ∝ m̂ct ;

m̂ct =

X

′
mdc Hdct
;

d∈D

θtd ∝ n̂dt + τ m̂dt ;

n̂dt =

X

ndw Hdwt ;

m̂dt =

w∈W

X

′
mdc Hdct
.

c∈C

Вероятностная интерпретация. В нашей вероятностной модели с каждым словом
в документе (d, w) связана как тема t, так и класс c. Примем гипотезу условной независимости терминов и классов в документах: p(w, cQ
| d) =Qp(w | d) p(c | d). Благодаря
этому предположению логарифм правдоподобия ln d∈D w∈d p(d, w, c)ndw распадается на два похожих слагаемых — левое L(Φ, Θ) совпадает со стандартным функционалом (1.6), правое R(Ψ, Θ) соответствует регуляризатору с коэффициентом τ = 1,
в котором mdc есть число слов в документе, относящихся к классу c:
X
X
X
X
L(Φ, Θ) + R(Ψ, Θ) =
ndw ln
ϕwt θtd + τ
mdc ln
ψct θtd → max .
d,w

t

d,c

t

Если исходная обучающая информация mdc имеет частотную интерпретацию,
то регуляризатор является не только способом повышения устойчивости решения,
но и непосредственным следствием принципа максимума правдоподобия.
Разреживание. Если имеется гипотеза, что каждый класс связан c небольшим числом тем, то для распределений ψct вводится разреживающий регуляризатор:
X X

R′ (Ψ) = τ ′
ψct = 0 → max,
c∈C t∈T

для оптимизации которого можно использовать принудительное разреживание
(см. §5.3) с одним ограничением: в матрице Ψ не должно быть нулевых строк, так
как каждый класс содержит хотя бы один документ, следовательно, хотя бы одну
тему.
Возможность оценить распределения ψct = p(c | t) и связать каждую тему
с небольшим числом классов во многих приложениях является преимуществом данного типа моделей, так как позволяет лучше интерпретировать темы.

50

Тематическая модель цитирования документов LDA-post, предложенная в [16],
в точности соответствует описанной модели, если C = D и классами Cd являются
документы, процитированные в документе d.
Число ненулевых элементов в строке разреженной матрицы Ψ, соответствующей документу c ∈ D, интерпретируется как число тем, на которые документ c
оказывает существенное влияние. Это важный наукометрический показатель, если,
конечно, он вычисляется по достаточно полной коллекции научных публикаций.
Тематическая модель цитирования авторов Author Link Topic model (ALT) [24]
также аналогична описанной модели, но строится на основе автор-тематической модели (6.1). Классами в (6.1) являются авторы документов d коллекции D, а классами
в регуляризаторе — авторы документов, на которые ссылаются d.
Многофункциональные тематические модели, учитывающие много типов классификаций, строятся путём добавления регуляризаторов, по одному на каждый тип.
Каждому множеству классов C j , j = 1, . . . , J, ставится в соответствие модель классификации вида (6.3) и регуляризатор вида (6.4), строится ещё одна матрица классов
тем Ψj , на M-шаге добавляется вычисление её элементов и вводится ещё одно слагаемое в формулу θtd , на E-шаге оцениваются условные вероятности p(t | d, cj ), cj ∈ C j .
Таким образом, добавление ещё одного типа классификаций приводит к линейному
по числу классов |C j | увеличению затрат времени и памяти.

§6.5

Тематическая модель категоризации

Категоризация текстовых документов — это специальный случай классификации. Категории создаются с целью структуризации знаний. Каждая категория
объединяет схожие по смыслу документы. Категории могут разделяться на подкатегории, образуя иерархическую структуру [3]. Наиболее известными примерами
являются международный Универсальный десятичный классификатор УДК и российский Библиотечно-библиографический классификатор ББК. Это иерархические
классификаторы, содержащие десятки тысяч категорий, создававшиеся сотнями экспертов по всем областям знания на протяжения многих десятилетий. Большая часть
издаваемых книг расписаны по этим или другим подобным классификаторам. При
создании новых категорий к ним приписывают ключевые термины. Имеются словари
терминов по категориям, называемые алфавитно-предметными указателями.
Тематическая модель отдельных категорий. Понятия категории и темы во многом схожи, но не совпадают. Темы и категории имеют схожее назначение — группировать близкие по смыслу документы. Категории создаются экспертами на основе
слабо формализованных критериев. Темы определяются формально как дискретные
распределения на множестве терминов, удовлетворяющие гипотезе условной независимости на документах коллекции. Нет никаких гарантий, что категории являются
темами. В лучшем случае какие-то из категорий могут быть темами. Более разумной
представляется гипотеза, что распределение терминов p(w | c) в каждой категории c
является смесью распределений p(w | t) небольшого числа тем t.
Будем строить тематическую модель категоризации на основе модели (6.3), считая, что классы C — это категории, и |C| < |T |.

51

Обобщим регуляризатор (6.4), заменив логарифм на непрерывно дифференцируемую возрастающую функцию µ:
X

XX
R(Ψ, Θ) = τ
mdc µ
ψct θtd → max .
d∈D c∈C

t∈T

Оказывается, что при определённом выборе функции µ у каждой темы возникает единственная родительская категория c(t), следовательно, каждая категория
разбивается на непересекающиеся темы. Эта особенность модели позволяет получать
ответы на ряд практически важных вопросов:
— насколько однородны категории?
— какие категории пора разбивать на подкатегории?
— сколько подкатегорий можно было бы выделить в каждой категории?
Ответы на эти вопросы необходимы экспертам для улучшения качества категоризации, особенно в условиях постоянно растущей коллекции документов.
Ковариационный регуляризатор. При µ(z) = z и mdc = nd p̂(c | d) регуляризатор
R(Ψ, Θ) равен взвешенной сумме ковариаций дискретных распределений p̂(c | d)
и p(c | d). Возьмём в качестве p̂(c | d) равномерное распределение (6.2). Тогда регуляризатор будет нечувствителен к тому, в каких долях модель p(c | d) распределяет
документ d по категориям из Cd .
Перегруппируем слагаемые в регуляризаторе:
X
X
X
X
L(Φ, Θ) + R(Ψ, Θ) =
ndw ln
ϕwt θtd + τ
ψct
mdc θtd → max .
d,w

t

c,t

d

Решение по переменным ψct находится аналитически благодаря
P тому, что максимизируемый функционал линеен по ψct на симплексе ψct > 0,
ψct = 1.
c∈C

Чтобы функционал достигал максимума, для каждой темы t, независимо
от остальных тем, переменная ψct должна принимать максимальное значение, равное 1, при таком c = c∗ (t), для которого коэффициент при ψct максимален:
c∗ (t) = arg max
c∈C

X

mdc θtd = arg max
c∈C

d∈D

X [c ∈ Cd ]
d∈D

|Cd |

nd θtd .

В силу нормировки, значения ψct должны обращаться в нуль для остальных c. Следовательно, распределение ψct в каждом столбце матрицы Ψ является вырожденным:


ψct = c = c∗ (t) .

Таким образом, каждой теме t соответствует только одна родительская категория c∗ (t), и ковариационный регуляризатор действительно решает поставленную
задачу разбиения категорий на темы.
Подставим найденное решение ψct в регуляризатор, который теперь будет зависеть только от матрицы Θ:
R(Θ) = τ

X X [c∗ (t) ∈ Cd ]
t∈T d∈D

|Cd|

nd θtd → max .

52

Воспользуемся формулой M-шага (5.2). Формулы для ϕwt ничем не отличаются
от стандартных, поскольку регуляризатор не зависит от Φ. Формулы для θtd требуют
предварительно найти родительскую категорию c∗ (t) для каждой темы t:
θtd ∝ n̂dt + τ

nd ∗
[c (t) ∈ Cd ].
|Cd |

Данная формула имеет прозрачную интерпретацию: если документ d относится
к родительской категории темы t, то к нему добавляется «виртуальный термин»
темы t с частотой τ nd /|Cd |.
Категоризация новых документов после того, как построена тематическая модель, является тривиальной задачей. Она сводится к переходу от распределения
тем
P
в документе p(t | d) = θtd к распределению категорий в документе p(c | d) = t ψct θtd .
Разреженность матриц Ψ и Θ позволяет находить это распределение для любого
документа практически мгновенно.

7

Динамические тематические модели

Динамические (temporal) тематические модели учитывают дополнительную информацию о времени создания или публикации документов. Будем полагать, что
каждый документ d связан с моментом времени yd из заданного конечного линейно
упорядоченного множества Y . Для научных статей это может быть год публикации.
Для новостных сообщений используются более мелкие отсчёты времени — неделя,
день или час. При построении динамических моделей наряду со стандартными распределениями ϕwt = p(w | t) и θtd = p(t | d) оцениваются распределения каждой темы
во времени ξyt = p(y | t). Они нужны для того, чтобы отобразить динамику изменения
тем во времени. Примеры таких визуализаций можно найти в работах [68, 55].
Расширим вероятностное пространство до множества D × W × T × Y . Будем
считать, что с каждым словом в документе (d, w) связана не только тема t ∈ T ,
но и момент y ∈ Y . Введём обозначение Y+ для множества Y без начального (нулевого) момента времени. Обозначим через Ξ = (ξyt )Y ×T матрицу новых неизвестных.
Примем гипотезу условной независимости: p(y | d, t) = p(y | t), означающую, что
модель должна объяснять наблюдаемую отметку времени yd исходя только из тематики документа d.
Заметим, что, поскольку множество моментов времени Y дискретно, все вероятностные предположения с точностью до обозначений те же, что и в тематической
модели классификации.
Рассмотрим две динамические модели. Модель с фиксированной тематикой
основана на предположении, что темы не меняются со временем, p(w | t, y) = p(w | t).
Модель с медленно меняющейся тематикой ослабляет это предположение и в явном
виде оценивает распределения слов в темах p(w | t, y) как зависящие от времени.

53

§7.1

Модель с фиксированной тематикой

Запишем распределение моментов времени в произвольном документе d как
вероятностную смесь распределений моментов в темах и тем в документе:
X
X
p(y | d) =
p(y | t) p(t | d) =
ξyt θtd .
t∈T

t∈T

Будем считать, что момент времени yd является случайным наблюдением, выбранным из распределения p(y | d) для каждого слова документа d. Тогда эмпирические распределения p̂(y | d) = [y = yd ] являются вырожденными. Обозначим через
mdy = [y = yd ]nd число терминов документа d, относящихся к моменту времени y.
Возьмём в качестве регуляризатора логарифм правдоподобия модели p(y | d):
X X
X
X
R1 (Ξ, Θ) = τ1
nd
p̂(y | d) ln p(y | d) = τ1
mdy ln
ξyt θtd → max .
d∈D

y∈Y

d∈D

t∈T

Введённая модель p(y | d) с регуляризатором R1 ничем по сути не отличается
от модели классификации (6.3) с регуляризатором (6.4).
Второй регуляризатор более специфичен для динамических задач. Он формализует предположение, что тематика меняется медленно, поэтому вероятности ξyt
в последовательные моменты времени должны быть близки:
2
τ2 X X
R2 (Ξ) = −
ξyt − ξy−1,t → max .
2 y∈Y t∈T
+

Задачу максимизации функционала L(Φ, Θ) + R1 (Ξ, Θ) + R2 (Ξ) будем решать
с помощью EM-алгоритма.
На E-шаге дополнительно оценивается условная вероятность
ξyt θtd
ξyt θtd
′
.
= P
Hdyt
= p(t | d, y) =
p(y | d)
ξys θsd
s∈T

На M-шаге ϕwt оценивается по прежним формулам:
P
ndw Hdwt .
ϕwt ∝ n̂wt ,
n̂wt =
d∈D

′
Оценки ξyt аналогичны оценкам ϕwt после замены w на y и Hdwt на Hdyt
. Кроме
того, на оценки ξyt влияет регуляризатор R2 , выполняющий функцию сглаживания.
Он увеличивает значение ξyt , если оно меньше полусуммы соседних вероятностей
ξy−1,t , ξy+1,t , и уменьшает, если оно больше их полусуммы:

P
′
ξyt ∝ τ1 m̂yt + τ2 ξyt ξy−1,t + ξy+1,t − 2ξyt , m̂yt =
mdy Hdyt
.
d∈D

Оценки θtd агрегируют счётчики терминов и моментов времени:
P
P
′
θtd ∝ n̂dt + τ1 m̂dt ,
n̂dt =
ndw Hdwt ,
m̂dt =
mdy Hdyt
.
w∈d

y∈Y

Если есть основания полагать, что большинство тем существуют только в ограниченном интервале времени, то можно ввести дополнительный регуляризатор
X X

R3 (Ξ) = τ3
ξyt = 0 → max
y∈Y t∈T

и использовать принудительное разреживание (§5.3).

54

§7.2

Модель с медленно меняющейся тематикой

§7.3

Модели с непрерывным временем

8

Иерархические тематические модели
ToDo21

Этот раздел устарел и будет целиком переписан

Для больших коллекций текстовых документов естественно строить иерархии
вложенных друг в друга тем (называемых также категориями или рубриками), чтобы
упростить поиск документов. Иерархия — это общепринятый способ структуризации
знаний. Однако разделение тем на более узкие подтемы субъективно, неоднозначно
и часто вызывает споры среди специалистов.
В статье [66] приводится обзор иерархических тематических моделей и отмечается, что оптимизация структуры иерархии по коллекции документов является
открытой проблемой; более того, разработка объективной количественной оценки
качества иерархии — также открытая проблема.
Многие иерархические модели имеют те или иные неестественные ограничения:
либо фиксируется число уровней, либо фиксируется число подтем в каждой теме или
на каждом уровне, либо документ не может относиться к темам из различных ветвей
дерева, либо темы не могут иметь общую подтему, либо темам во внутренних узлах
не сопоставляется распределение на множестве терминов.

§8.1

Определение тематического дерева

Гипотеза о существовании тематического дерева. Рассмотрим дерево с множеством вершин V и корнем t0 ∈ V . Вершины дерева соответствуют темам. Каждой
теме t ∈ V соответствует множество её подтем — дочерних вершин в дереве St ⊂ V .
Каждое ребро дерева соответствует паре «тема–подтема» (t, s), s ∈ St . Если St = ∅,
то тема t называется терминальной или листом тематического дерева. Для каждой
вершины t в дереве V существует только одна родительская вершина, следовательно,
только один путь (t0 , . . . , t) от корня дерева t0 до темы t.
Ранее мы предполагали, что каждое вхождение термина w в документ d связано
только с одной темой t. Теперь примем за аксиому другие предположения:
1) если пара (d, w) связана с темой t, то она связана и со всеми темами выше
вершины t на пути до корня t0 ;
2) если пара (d, w) не связана с темой t, то она не связана и со всеми подтемами
в поддереве ниже вершины t.
Этих двух предположений, совершенно не вероятностного характера, достаточно, чтобы построить иерархическую вероятностную тематическую модель.
Вероятностная интерпретация отношения «тема–подтема». Каждому ребру
тематического дерева (t, s) соответствует условная вероятность p(s | t) того, что термин документа, связанный с темой t, связан также с подтемой s ∈ St :
p(s | t) =

p(t, s)
p(s)
=
.
p(t)
p(t)

(8.1)

55

Если рассматривать коллекцию документов как выборку троек (d, w, t), то частотной оценкой этой условной вероятности будет p̂(s | t) = ns /nt — доля троек, связанных с подтемой s, среди всех троек, связанных с темой t.
Условные вероятности подтем удовлетворяют ограничениям нормировки, которые, в силу (8.1), допускают две эквивалентные записи:
X
X
p(s | t) = 1,
p(s) = p(t), t ∈ V.
(8.2)
s∈St

s∈St

Обозначим через T множество тем, соответствующих терминальным вершинам
дерева V . Условие нормировки
X
p(t) = 1.
(8.3)
t∈T

выполняется именно для этого множества, а не для всего множества тем в дереве V .
Из (8.2) следует, что условие нормировки (8.3) останется в силе, если заменить любое
из множеств St ⊆ T его родительской темой t, а также если делать такие замены
многократно в произвольном порядке, вплоть до корневой темы t0 , p(t0 ) = 1.
При разделении темы t на подтемы s ∈ St условные распределения для подтем
ϕws = p(w | s) и θsd = p(s | d) должны удовлетворять требованиям нормировки
X
X
θsd = θtd , d ∈ D.
(8.4)
ϕws = 1, s ∈ St ;
w∈W

s∈St

p(s)
Распределения p(s | w) = ϕws p(w)
и p(d | s) = θsd p(d)
также должны быть нормиp(s)
рованы, откуда следуют ещё две серии тождеств:
X
X
ϕws p(s) = ϕwt p(t), w ∈ W ;
θsd p(d) = p(s), s ∈ St .
(8.5)
s∈St

d∈D

Документы во внутренних вершинах. В некоторых приложениях важно, чтобы
документы и термины могли относиться не только к терминальным вершинам, но
и к любым внутренним вершинам тематического дерева. В частности, это могут
быть документы, относящиеся сразу к нескольким подтемам, либо новые документы,
которые пока не выделились в отдельную подтему.
Для каждой внутренней вершины t ∈ V \T создаётся выделенная терминальная
вершина — подтема s0 ∈ St . Если документ или термин попадает в s0 , то считается,
что он остался в теме t. В терминах кластеризации выделенная подтема s0 — это
специальный «фоновый» кластер, к которому относится всё, что не удалось с уверенностью отнести к другим кластерам — подтемам темы t.
К выделенной подтеме s0 естественно предъявлять требование минимизации
числа документов и описывать её тем же распределением, что и родительскую тему t.

§8.2

Фиксированная иерархия

Рассмотрим случай, когда структура тематического дерева {St : t ∈ V } фиксирована. Чтобы каждая тема t ∈ T имела интерпретацию, к ней привязывается

56

множество документов Dt ⊂ D и множество терминов Wt ⊂ W . Одно из этих множеств может быть пустым. Таким образом, ставится задача частичного обучения
(semi-supervised learning) иерархической тематической модели.
Распределение θtd = p(t | d), полученное в результате тематического моделирования, непосредственно решает задачу категоризации — документ d относится к тем
темам (категориям), для которых вероятность θtd превышает заданный порог. Для
решения задач категоризации лучше подходят разреженные модели, в которых малые вероятности θtd обнуляются в процессе построения модели. Обнуление или игнорирование малых вероятностей в уже построенной модели может приводить к менее
адекватным результатам.
Для категоризации текстов часто применяется другой подход: для каждой пары
тема–подтема строится классификатор на два класса [49]. Каждый классификатор
обучается по выборке документов, относящихся к родительской теме, что требует
больших затрат времени и памяти. Иерархическая тематическая модель, очевидно,
является более естественным инструментом категоризации.
Иерархический Алгоритм 8.1 основан на онлайновом Алгоритме 2.4. Основное
отличие PLSA-HOEM в том, что для вычисления распределения θtd тем t в документе d производится спуск по дереву от корня к терминальным вершинам.
Модификация формул М-шага для иерархической модели. Пусть T ⊂ V —
подмножество вершин дерева, удовлетворяющее условию нормировки (8.3), и для
всех тем t ∈ T известны значения параметров ϕwt , θtd . Сначала T состоит из единственной корневой вершины t0 , для которой ϕwt0 = p(w) и θt0 d = 1. Спуск по дереву — это итерационный процесс, на каждом шаге которого выбирается некоторое
подмножество
тем R ⊆ T , и для каждой вершины s ∈ S из множества всех их подS
тем S =
St , вычисляются значения параметров ϕws , θsd . После этого множество T
t∈R

заменяется на (T \R) ∪ S и начинается следующий шаг. Спуск продолжается, пока T
не совпадёт с множеством терминальных вершин дерева.
Рассмотрим вероятностную модель (1.2) при ограничениях нормировки (8.4).
Параметры ϕwt и θtd для всех тем t ∈ T \R будем считать фиксированными. Обозначим через σdw фиксированную часть вероятностной тематической модели:
X
σdw =
ϕwt θtd , d ∈ D, w ∈ d.
t∈T \R

Задача оценивания параметров ϕws , θsd , s ∈ S сводится к максимизации логарифма правдоподобия,
(1.6), но оптимизируется только часть
 аналогично задаче

параметров ΦS = ϕws W ×S и ΘS = θsd S×D , связанных с темами из S:


XX
X
L(ΦS , ΘS ) =
ndw ln σdw +
ϕws θsd → max ;
d∈D w∈d

X

s∈S

ϕws = 1,

s ∈ S;

θsd = θtd ,

t ∈ R, d ∈ D.

w∈W

X

s∈St

ΦS ,ΘS

57

Как обычно, нужно записать лагранжиан, приравнять нулю его производные
по переменным ϕws и θsd , из полученных уравнений исключить двойственные переменные и выразить ϕws и θsd через Hdws :
ϕws θsd
P
, d ∈ D, w ∈ d, s ∈ S;
σdw +
ϕws′ θs′ d
′
P s ∈S
ndw Hdws
d∈D
= P P
, w ∈ W, s ∈ S;
ndw′ Hdw′s
w ′ ∈W d∈D
P
ndw Hdws
w∈d
= θtd P P
, d ∈ D, s ∈ St , t ∈ R;
ndw′ Hdw′s′

Hdws =

ϕws

θsd

s′ ∈St w ′ ∈d

или, в более компактной записи с использованием счётчиков:
n̂ws
,
n̂s
n̂ds
,
= θtd
n̂dt

ϕws =
θsd

n̂s =

P

n̂ws ,

n̂ws =

w∈W

n̂dt =

P

P

ndw Hdws .

(8.6)

ndw Hdws .

(8.7)

d∈D

n̂ds ,

n̂ds =

s∈St

P

w∈d

Таким образом, формулы M-шага и E-шага для иерархического алгоритма
лишь немногим отличаются от обычного PLSA-EM.
Инициализация и частичное обучение. Привязки терминов и документов к те0
мам задаются в виде начальных распределений ϕ0wt и θtd
, вычисляемых по формулам
из §2.5 c нормировкой (8.4). При инициализации они смешиваются с неразреженными случайными распределениями, на каждом шаге EM-алгоритма — с оценками (8.6)
и (8.7). Благодаря тому, что текущие приближения ϕwt и θtd немного притягиваются
0
к начальным распределениям ϕ0wt и θtd
, темы не уходят далеко от исходно заданных
интерпретаций. Сила этого «притяжения» регулируется параметрами λ и µ.
Регуляризация Дирихле может быть добавлена в Алгоритм 8.1 обычным образом:
частотные оценки условных вероятностей (8.6), (8.7) заменяются сглаженными:
ϕws = P

βw + n̂ws
β + n̂ws
= w
.
β0 + n̂s
βw′ + n̂w′ s

(8.8)

w ′ ∈W

θsd = θtd P

αs + n̂ds
α + n̂ds
 = θtd s
.
αt + n̂dt
αs′ + n̂ds′

(8.9)

s′ ∈St

Заметим, что при разделении темы t на множество подтем St расщепляются
также и гиперпараметры распределения Дирихле Dir(θd ; α), а их сумма α0 не меняется. Это следует из условий нормировки (8.4) и свойства (3.1):
X
X
X
θsd = θtd ⇒
Eθsd = Eθtd ⇒
αs = αt .
s∈St

s∈St

s∈St

58

Алгоритм 8.1. PLSA-HOEM: иерархический онлайновый EM-алгоритм.
Вход: коллекция документов D; параметры λ и µ,
множество тем V и структура тематического дерева {St : t ∈ V },
0
привязки терминов и документов к темам ϕ0wt и θtd
;
Выход: распределения Θ и Φ;
1
2
3
4
5
6
7
8

инициализировать ϕwt с учётом ϕ0wt для всех w ∈ W , t ∈ V ;
повторять
n̂wt := 0; n̂t := 0 для всех w ∈ W , t ∈ V ;
для всех d ∈ D
0
инициализировать θtd с учётом θtd
для всех t ∈ V ;
T := {t0 }; R := {t0 }; θt0 d = 1; S
пока множество подтем S :=
St не пусто
t∈R
P
σdw =
ϕwt θtd для всех w ∈ d;
t∈T \R

9

повторять
P
Zw := σdw +
ϕws θsd для всех w ∈ d;
s∈S
P
ns :=
ndw ϕws θsd /Zw для всех w ∈ d;
w∈d
P
n :=
ns ;

10
11
12

s∈S

0
θsd := µθsd
+ (1 − µ)θtd ns /n для всех s ∈ St , t ∈ R;
пока θsd не сойдутся для всех s ∈ S;
увеличить n̂ws , n̂s на ndw ϕws θsd /Zw для всех w ∈ d, s ∈ S;
T := (T \R) ∪ S; R := S;

13
14
15
16
17
18

ϕwt := λϕ0wt + (1 − λ)n̂wt /n̂t для всех w ∈ W , t ∈ V ;
пока Φ не сойдутся;

§8.3

9

Реконструкция иерархии

Многоязычные тематические модели

§9.1

Параллельные тексты

§9.2

Сопоставимые тексты

§9.3

Регуляризация матрицы переводов слов

10

Модели текста как последовательности слов

§10.1

Коллокации

§10.2

Марковские модели синтаксиса языка

§10.3

Выделение ключевых фраз

59

§10.4

11

Тематическая структура документа

Многомодальные тематические модели

§11.1

Коллаборативная фильтрация

§11.2

Модель научной социальной сети

§11.3

Персонализация рекламы в Интернете

12

Критерии качества тематических моделей
Этот раздел устарел и будет целиком переписан

§12.1

ToDo22

Внутренние оценки качества тематических моделей

Оценивание качества тематических моделей является нетривиальной проблемой. В отличие от задач классификации или регрессии здесь нет чёткого понятия
«ошибки» или «потери». Стандартные критерии качества кластеризации типа средних внутрикластерных или межкластерных расстояний или их отношений плохо подходят для оценивания «мягкой» совместной кластеризации документов и терминов.
Наиболее распространённым критерием является перплексия (perplexity), используемая для оценивания моделей языка в компьютерной лингвистике. Это мера
несоответствия или «удивлённости» модели p(w | d) терминам w, наблюдаемым в документах d коллекции D, определяемая через логарифм правдоподобия (1.6):



 1
1 XX
ndw ln p(w | d) .
(12.1)
P(D; p) = exp − L(Φ, Θ) = exp −
n
n
d∈D w∈d

Чем меньше эта величина, тем лучше модель p предсказывает появление терминов w в документах d коллекции D.
Интерпретация перплексии. Если термины w порождаются из равномерного распределения p(w) = 1/V на словаре мощности V , то перплексия модели p на таком
тексте сходится к V с ростом его длины. Чем сильнее распределение p отличается
от равномерного, тем меньше перплексия. Чем сильнее модель p отличается от генерирующего распределения, тем больше перплексия. В нашем случае в (12.1) используются условные вероятности терминов p(w | d), и интерпретация немного другая:
если каждый документ генерируется из V равновероятных терминов (возможно, различных в разных документах), то перплексия сходится к V . Опять-таки, чем сильнее
распределение отличается от равномерного, тем меньше перплексия.
Чтобы сравнение перплексии двух коллекций было корректным, необходимо, чтобы ToDo23
они имели один и тот же словарь.
Чтобы перплексия была характеристикой только качества модели, необходимо вво- ToDo24
дить нормировки, чтобы длины документов и эффективная мощность словаря не влияли
на перплексию.

60

Перплексия контрольной выборки. Обозначим через pD (w | d) модель, построенную по обучающей коллекции документов D. Перплексия обучающей выборки P(D; pD ) является оптимистично смещённой (заниженной) характеристикой качества модели из-за эффекта переобучения. Обобщающую способность модели принято оценивать перплексией контрольной выборки (hold-out perplexity) P(D ′ ; pD ).
Вопрос о том, как разделить исходную коллекцию на обучение D и контроль D ′ ,
не тривиален. К сожалению, детали этой процедуры во многих статьях опускаются. В [9] предлагается разделять все документы на обучающие и контрольные случайным образом в пропорции 9 : 1. Однако в силу гипотез «мешка слов» и «мешка
документов» более корректным было бы случайное разбиение каждого документа на
обучающую и контрольную части. С другой стороны, во многих приложениях важно
проверить способность тематической модели хорошо описывать новые документы.
Новые документы порождают две проблемы: во-первых, для них необходимо
оценивать θtd ; во-вторых, они могут содержать новые термины w, для которых придётся оценивать также ϕwt , увеличивать размерность векторов ϕt = (ϕwt )w∈W и перенормировать их. Такая процедура оценивания модели частично включает в себя
процедуру обучения, в результате чего оценка качества снова может оказаться оптимистично смещённой.
Частичное решение этой проблемы предлагается в [8]. После обучения модели pD векторы ϕt фиксируются, векторы θd контрольных документов d ∈ D ′ оцениваются по первой половине каждого документа, по вторым половинам вычисляется
контрольная перплексия. Что такое «половина», не уточняется. Простое разрезание
текста на две части может приводить к смещённым оценкам. Например, научные статьи обычно начинаются с введения и обзора, использующих общую терминологию,
затем идёт изложение частных результатов. Если в коллекции много таких текстов,
то оценка окажется пессимистично смещённой. Противоположный пример неслучайного разбиения текста — когда число вхождений каждого термина ndw делится ровно
пополам между обучающей и контрольной выборками. В таком случае обучающая
и контрольная половины документа будут неразличимы для тематической модели,
и оценка окажется оптимистично смещённой.
В наших экспериментах последовательность терминов {w1 , . . . , wnd } каждого
контрольного документа d ∈ D ′ после случайной перестановки разбивается на две
части равной длины. Новые слова, попадающие во вторую часть, игнорируются.
Ещё один выход — робастные модели. Новые редкие слова считаются шумом, описы- ToDo25
ваются униграммной моделью и почти не дают вклада в контрольную перплексию. Робастную модель трудно удивить новыми словами, т.к. она трактует их как шум.

Более сложные процедуры несмещённого оценивания правдоподобия предложены в [60] и улучшены в [10]. Они имеют трудоёмкость, квадратичную по длине документа, и в процессе оценивания используют ту же тематическую модель, качество
которой оценивается. Эти недостатки несколько ограничивают их применимость.

§12.2

Критерии условной независимости

Гипотеза условной независимости p(w | d, t) = p(w | t) чрезвычайно важна для
вероятностных тематических моделей. Именно она обеспечивает переход к компактному представлению данных F ≈ ΦΘ. Для её проверки не требуется выделять контрольную выборку, что является преимуществом данного типа критериев.

61

Оба распределения оцениваются в EM-алгоритме:
ndwt
, t ∈ T, d ∈ D;
n̂dt
n̂wt
p̂(w | t) =
, t ∈ T.
n̂t

p̂(w | d, t) =

Рассмотрим статистические тесты, проверяющие нулевую гипотезу о том, что
различия между этими распределениями незначимы, точнее, что выборка с эмпирическим распределением p̂(w | d, t) могла быть получена из генеральной совокупности
с распределением p̂(w | t).
Точный тест Фишера. Рассмотрим следующий статистический эксперимент, связанный с каждой тройкой (d, w, t). Имеется последовательность из nt терминов, в которой термин w встречается ровно nwt раз. Из этой последовательности случайно
и независимо выбираются ndt терминов. Какова вероятность, что термин w окажется в числе выбранных не более ndwt раз? В условиях истинности нулевой гипотезы
эта вероятность описывается функцией гипергеометрического распределения:
Pdwt =

n
dwt
X
i=0

−i
Cni wt Cnntdt−n
wt
.
Cnntdt

дописать

ToDo26

Критерий X 2 Пирсона основан на вычислении статистики хи-квадрат, которая
является естественной мерой различия двух распределений:

X (Edwt − ndwt )2
X p̂(w | t) − p̂(w | d, t) 2
2
Xdt
=
= n̂dt
,
Edwt
p̂(w | t)
w∈W
w∈W
dt

dt

где Edwt = n̂dt p̂(w | t) — ожидаемое
число вхождений
термина w в документ d, свя
	
занных с темой t, Wdt = w ∈ W : Edwt > 0 .
2
Если значение Xdt
превышает (1−α)-квантиль распределения хи-квадрат χ2k,1−α
с числом степеней свободы k = |Wdt | − 1, то нулевая гипотеза отвергается.
Условием применимости асимптотики χ2k считается наличие достаточного числа наблюдений во всей выборке, n̂dt > 50, а также достаточного ожидаемого числа
наблюдений каждого термина, Edwt > 5. Второе требование в типичном случае не выполняется для большинства терминов w, так как распределение p̂(w | t), как правило,
разрежено, более того, мощность словаря Wdt может превышать длину документа n̂dt .
Таким образом, в нашем случае критерий Пирсона применять нельзя. Для случая
разреженных распределений больше подходят статистики G2 и D 2 .
Статистика G2 определяется через дивергенцию Кульбака–Лейблера, и для неё также справедливо асимптотическое распределение χ2k с тем же числом степеней свободы, но при менее жёстких требованиях к числу наблюдений:
X
ndwt
G2dt = 2
ndwt ln
.
Edwt
w∈W
dt

62

Статистика D 2 — это поправка к статистике X 2 , предложенная Зельтерманом в [67]
специально для случая разреженных распределений:
X (Edwt − ndwt )2 − ndwt
2
Ddt
=
.
Edwt
w∈W
dt

Эта статистика имеет асимптотически нормальное распределение. Особенности её
применения обсуждаются в [53, 25].
Семейство функций расстояния Кресси–Рида. Для сравнения эмпирической
функции вероятности p̂(w), оцененной по выборке длины n, с истинной функцией
вероятности p(w) принято использовать функции расстояния, придающие больший
вес малым вероятностям:
X
p̂(w)
KL(p̂ k p) =
p̂(w) ln
— дивергенция Кульбака–Лейблера;
(12.2)
p(w)
w

X p(w) − p̂(w) 2
X 2 (p̂, p) =
— ненормированная χ2 -статистика;
(12.3)
p(w)
w
2
Xp
p
2
H (p̂, p) =
p(w) − p̂(w) — расстояние Хеллингера.
(12.4)
w

Эти и другие «разумные» функции расстояния обобщаются (с точностью до константного множителя) параметрическим семейством дивергенций Кресси–Рида [13,
46]:

λ

X
p̂(w)
2
p̂(w)
CRλ (p̂ : p) =
−1 .
λ(λ + 1) w
p(w)
дописать, сделать эксперименты

Перестановочный тест основан на использовании эмпирического распределения
статистики, полученного путём сэмплирования большого числа выборок в условиях
истинности нулевой гипотезы. Перестановочные тесты применяются в тех случаях,
когда функция распределения статистики неизвестна или имеет слишком сложный
вид или её известные асимптотики не достаточно точны.
Пусть S — одна из статистик X 2 , G2 , D 2 . Зафиксируем тему t. Сгенерируем
N независимых выборок терминов из распределения p̂(w | t). Для каждой из них вычислим эмпирическое распределение p̂(w) и значение статистики S. По выборке значений статистики {S1 , . . . , SN } построим эмпирическое распределение F̂t (S) и найдём
его (1 − α)-квантиль F̂t,1−α . Число N должно быть порядка 103 при α = 0.05.
Обозначим через Sdt значение статистики S, вычисленное по распределению
p̂(w | d, t) для заданных t ∈ T и d ∈ D. Поскольку распределение F̂t (S) построено
в условиях истинности нулевой гипотезы, неравенство Sdt > F̂t,1−α является критерием отклонения нулевой гипотезы для документа d на уровне значимости α.
Заметим, что квантиль F̂t,1−α достаточно вычислить один раз для каждой темы t и использовать для всех документов d ∈ D, что даёт значительную экономию
времени. Однако при изменении распределения p̂(w | t) распределение F̂t (S) и его
квантиль придётся пересчитать заново.

ToDo27

63

Оценки средней несогласованности для документов и тем. Введём индикатор
события «тема t не согласована в документе d при уровне значимости α»:


Bdt (α) = Sdt > F̂t,1−α ;
Определим среднюю несогласованность темы, документа и тематической модели в целом при уровне значимости α:
Bt (α) =

X n̂dt
d∈D

n̂t

Bdt (α) — средняя несогласованность темы t;

X n̂dt

Bdt (α) — средняя несогласованность документа d;
n
d
t∈T
X X n̂dt
B(α) =
Bdt (α) — средняя несогласованность модели.
n
d∈D t∈T

Bd (α) =

Это нормированные величины, принимающие значения из отрезка [0, 1]. Чем
меньше средняя несогласованность, тем лучше модель описывает соответствующую
тему t, документ d или всю коллекцию в целом.
Критерий условной независимости. В [34] предлагается ещё один критерий,
оценивающий степень несоответствия темы t ∈ T гипотезе условной независимости. Он основан на дивергенции Кульбака–Лейблера и может быть легко вычислен
в EM-алгоритме на каждом проходе коллекции:


 Xn
ndwt

dwt
KLt = KL p̂(d, w | t) 
 p̂(d | t) p̂(w | t) =
ln
.
n̂
E
t
dwt
d,w
Статистика G2t =

P

G2dt = 2n̂t KLt имеет асимптотически распределение χ2k
P
с числом степеней свободы k =
|Wdt | − |W | − |D| + 1. В силу разреженности расd∈D

d∈D

пределения p̂(d, w | t) вместо критерия хи-квадрат лучше применять перестановочный тест. Гипотеза условной независимости принимается для темы t, когда значение
статистики G2t меньше критического.

Выделение несогласованных тем. Статистические критерии позволяют находить
«неудачные» темы, которые целесообразно разбивать на подтемы, непосредственно
во время итераций EM-алгоритма. Темы можно ранжировать и сравнивать по значениям средней согласованности Bt (α) или статистики G2t . Заметим, что сравнивать
темы по значению дивергенции KLt некорректно, так как только после умножения
на «длину темы» n̂t получается величина G2t = 2n̂t KLt , имеющая (асимптотически)
одинаковое распределение для всех тем.
Эксперименты Влады Целых

§12.3

Критерии качества классификации документов

Оценивание качества тематической модели упрощается в тех случаях, когда
она строится с целью классификации или поиска документов. Каждый документ

ToDo28

64


описывается |T |-мерным вектором тем θd = p(t | d) t∈T . Качество модели определяется тем, насколько хорошо классифицируются документы, представленные этими
векторами.
Пусть каждый документ d ∈ D относится к классу yd ∈ Y , алгоритм классификации a : R|T | → Y относит документ d к классу ad = a(θd ). В задачах информационного поиска и категоризации текстов качество классификации принято измерять
в терминах точности и полноты [49].
Точность (precision) относительно класса y ∈ Y определяется как доля правильно классифицированных документов среди всех документов, отнесённых алгоритмом a к классу y:

	
# d ∈ D : ad = yd = y

	 .
Py (a) =
# d ∈ D : ad = y

Полнота (recall) относительно класса y ∈ Y определяется как доля правильно
классифицированных документов среди всех документов класса y:

	
# d ∈ D : ad = yd = y

	 .
Ry (a) =
# d ∈ D : yd = y

Чем больше значения точности и полноты, тем выше качество классификации.
В задачах информационного поиска обычно рассматривают два класса — документ либо «релевантен», либо «нерелевантен»; точность и полноту определяют
только относительно класса релевантных документов.
Задачи категоризации, как правило, являются многоклассовыми, |Y | ≫ 2. В таких случаях точность и полноту усредняют по всем классам.
В качестве агрегированного показателя, объединяющего точность P и полноту R, принято использовать F1 -меру:
F1 =

§12.4

2P R
.
P +R

Критерии качества тематического поиска

Описать идею разбиения каждого документа на части и поиска одних частей по дру- ToDo29
гим. Качество поиска может измеряться с помощью Mean Average Precision.

§12.5

Интерпретируемость тем

§12.6

Когерентность

Тема называется когерентной, если термины, наиболее частые в данной теме,
неслучайно часто совместно встречаются рядом в документах коллекции [41, 42].
Когерентность может оцениваться по сторонней коллекции (например, по Википедии) [39], либо по той же коллекции, по которой строится модель [35].
Предлагалось несколько оценок когерентности. Сначала использовалась поточечная взаимная информация (pointwise mutual information, PMI) [41, 42]:
PMI(t) =

k−1 X
k
X
i=1 j=i

log

N(wi , wj )
,
N(wi )N(wj )

65

где wi — i-й термин в порядке убывания ϕwt , N(w) — число документов, в которых
термин w встречается хотя бы один раз, N(w, w ′ ) — число документов, в которых
термины w, w ′ встречаются рядом хотя бы один раз, число k обычно полагается
равным 10. «Встречаются рядом» означает — в окне заданной ширины h, которая
является параметром, обычно h = 10.
Затем в экспериментах [35] было показано, что более адекватной мерой когерентности является логарифм условной вероятности (log conditional probability,
LCP), оценивающая вероятность менее частого слова при условии более частого:
LCP(t) =

k−1 X
k
X
i=1 j=i

§12.7

log

N(wi , wj )
,
N(wi )

Точность восстановления модельных данных

Алгоритм 1.1 можно использовать для генерации модельных данных по заданным распределениям p(w | t) и p(t | d). Это крайне полезно на стадии тестирования
методов обучения тематических моделей, решающих задачу (1.6). Хороший метод
должен быть способен восстановить по данным ту самую модель, которая эти данные породила. Модельные данные можно генерировать различной длины n; можно
добавлять в них шум — случайные пары (di , wi ) из распределения, заведомо плохо
приближаемого моделью (1.2); можно задавать распределения p(w | t), p(t | d) более
различными или более похожими, тем самым делая задачу восстановления модели
более лёгкой или более трудной; задавать различное число тем |T |, а восстанавливать
модель при другом числе тем, либо пытаться его определить. Эксперименты с варьированием модели данных позволяют исследовать устойчивость метода и узнать
границы его применимости. Только в случае модельных данных известно, какая тема ti на самом деле связана с каждой парой (di , wi ), что позволяет оценивать качество
восстановления модели по данным как долю правильно угаданных тем или как расстояние между восстановленными и истинными распределениями p(w | t), p(t | d).
Показать эксперименты на модельных данных

ToDo30

Лирическое отступление: задача о назначениях и венгерский алгоритм.

ToDo31

13
§13.1

Эксперименты с тематическими моделями
Экспериментальные текстовые коллекции

Коллекция RuDis содержит |D| = 2000 авторефератов диссертаций на русском языке. Суммарная длина коллекции n ≈ 8.7 · 106 слов. Объём словаря |W | ≈ 3 · 104 . Контрольная коллекция D ′ содержит 200 авторефератов. Предварительно сделана лемматизация и отброшены стоп-слова.
Коллекция NIPS содержит |D| = 1566 текстов статей научной конференции Neural
Information Processing Systems на английском языке. Суммарная длина коллекции
n ≈ 2.3 · 106 слов. Объём словаря |W | ≈ 1.3 · 104 . Контрольная коллекция D ′ содержит 174 документов. Предварительно сделан стемминг и отброшены стоп-слова.

66

§13.2

Неустойчивость LDA
(Глушаченков В. В.)

Задача тематического моделирования (1.6) является некорректно поставленной.
Её решение не единственно, так как стохастическое матричное разложение определено с точностью до невырожденного преобразования: ΦΘ = (ΦS)(S −1 Θ). Её решение
неустойчиво, так как выбор преобразования S в EM-подобных алгоритмах никак
не контролируется и зависит от начального приближения.
Устойчивость решения может повышаться, если исходные данные удовлетворяют гипотезе разреженности. Чем больше нулевых значений в матрицах Φ и Θ, тем
меньше остаётся линейных преобразований S столбцов Φ, при которых преобразование S −1 над строками Θ оставляет все элементы матрицы Θ неотрицательными. При
сильной разреженности разложение может оказаться единственным с точностью до
перестановки тем.
Цель эксперимента. Показать, что алгоритмы PLSA и LDA-GS дают неустойчивые
решения, зависящие от случайных начальных приближений. Показать, что устойчивость повышается с ростом разреженности исходных матриц Φ и Θ, порождающих
коллекцию документов. Проверить, позволяет ли алгоритм постепенного принудительного разреживания повысить устойчивость решения или правильнее определить
структуру разреженности матриц Φ и Θ.
Исходные данные и условия эксперимента. Все эксперименты проводились
на модельных коллекциях, порождаемых известными матрицами Φ и Θ при
|D| = 500, |W | = 1000, |T | = 30, длина документов nd выбиралась случайно из равномерного распределения на [100, 600].
Для генерации столбцов исходных матриц Φ и Θ применялось два подхода:
1) симметричные распределения Дирихле: ϕt ∼ Dir(β), θd ∼ Dir(α);
2) равномерные распределения с последующим обнулением заданной доли элементов Rϕ , Rθ в каждом столбце матриц Φ и Θ соответственно.
На рис. 3 показана зависимость степени разреженности (доли нулевых элементов) модельных матриц Θ и Φ от гиперпараметров распределения Дирихле. Сильная
разреженность (более 60% нулей) возникает при значении гиперпараметра менее 0.1.
Начальные приближения ϕwt и θtd задавались путём обхода всей коллекции,
при этом каждой паре (d, w) назначалась случайная тема t из равномерного распределения и вычислялись частотные оценки ϕwt и θtd согласно (1.4).
Использовался алгоритм принудительного разреживания (см. §5.3, §13.4) с эвристикой упрощённой робастности (см. §4.3). Принудительное разреживание начиналось с i0 -го прохода коллекции, i0 = 10; доля обнуляемых наименьших вероятностей
в каждом распределении не превышала r = 0.1; сумма обнуляемых значений не превышала SΦ = 0.001, SΘ = 0.1.
Отклонение восстановленных распределений p̂(i | j) от модельных p(i | j) измерялось средним расстоянием Хеллингера
v
m u X
2
X
p
u 1 n p
1
t
p̂(i | j) − p(i | j) ,
H(p̂, p) =
m j=1 2 i=1

67

1
Θβ
Φα

0.8

0.6

0.4

0.2

0
0

0.2

0.4

0.6

0.8

1

α,β

1.2

1.4

1.6

1.8

2

Рис. 3. Зависимость степени разреженности (доли нулевых элементов) модельных матриц Θ и Φ от
гиперпараметров распределения Дирихле.

0.5

0.55

DΦΘ

0.45

D
D

DΘ

0.4

Θ

0.35

DΦ

0.45

Φ

0.4

DΦΘ

0.5

0.35
0.3
0.3
0.25

0.25

0.2

0.2

0.15

0.15

0.1

0.1

0.05
0

0.1

0.2

0.3

0.4

0.5

0.6

β, α = 0,01

0.7

0.8

0.9

1

0.05
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

β, α = 0.01

LDA-GS

PLSA-EM

0.8

0.8

DΦΘ
0.7

0.7

D

Φ

D

Θ

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

DΦΘ
D

0.1
0

0.1

0.2

0.3

0.4

0.5

0.6

β, α = 0.5

LDA-GS

0.7

0.8

0.9

1

0.1
0

Φ

DΘ

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

β, α = 0.5

PLSA-EM

Рис. 4. Зависимость точности восстановления матриц Φ, Θ и ΦΘ от разреженности матрицы Φ
порождающей модели при фиксированной разреженности матрицы Θ с параметрами α = 0.01, 0.5,
для алгоритмов LDA-GS и PLSA-EM.

68

как для самих матриц Φ и Θ, так и для их произведения:
DΦ (Φ̂, Φ) = H(Φ̂, Φ);
DΘ (Θ̂, Θ) = H(Θ̂, Θ);
DΦΘ (Φ̂Θ̂, ΦΘ) = H(Φ̂Θ̂, ΦΘ).
Поскольку матрицы Φ̂, Θ̂ восстанавливаются с точностью до перестановки тем,
перед сравнением к ним применялся венгерский алгоритм [37], который ищет перестановочную матрицу Π, минимизирующую функционал
f (Π) = DΦ (Φ̂Π, Φ) + DΘ (Π−1 Θ̂, Θ).
Качество восстановления структуры разреженности матриц Φ и Θ измерялось
долей ошибок первого рода
SΦ1 =



1 X
ϕ̂wt > 0 ϕwt = 0 ,
|W ||T | w,t

SΘ1 =



1 X
θ̂dt > 0 θdt = 0 ,
|D||T | d,t

и долей ошибок второго рода
SΦ2 =



1 X
ϕ̂wt = 0 ϕwt > 0 ,
|W ||T | w,t

SΘ2 =



1 X
θ̂dt = 0 θdt > 0 .
|D||T | d,t

Результаты. Графики на рис. 4 и рис. 5 показывают зависимости точности восстановления матриц Φ, Θ и ΦΘ от разреженности матриц Φ и Θ в порождающей модели.
Сравнивались алгоритмы LDA-GS и PLSA-EM. Гиперпараметры α и β в алгоритме
LDA-GS полагались равными тем же значениям, которые использовались при генерации модельных данных. Оба алгоритма хорошо восстанавливают произведение ΦΘ.
Однако сами матрицы Θ и Φ хорошо восстанавливаются только когда они сильно
разрежены. При уменьшении разреженности оба алгоритма неустойчивы, причём
PLSA-EM менее устойчив.
Графики на рис. 6 и рис. 7 показывают зависимости точности восстановления
матриц Φ, Θ и ΦΘ от разреженности матриц Φ и Θ в порождающей модели. Теперь
сравниваются два варианта алгоритма PLSA-EM: стандартный и с принудительным
разреживанием. Оба алгоритма хорошо восстанавливают матрицы Φ и Θ только
когда они сильно разрежены (на 80% или более). Чем менее разрежены исходные
матрицы Φ и Θ, тем меньшую точность даёт принудительное разреживание, что
вполне естественно.
Преимущество принудительного разреживания проявляется в том случае, когда
требуется восстановить структуру разреженности — узнать, какие именно элементы
матриц Φ и Θ равны нулю.
Графики на рис. 8 и рис. 9 показывают зависимости числа ошибок первого и
второго рода при определении нулевых элементов в матрицах Φ, Θ. Разреживающий
EM-алгоритм действительно лучше восстанавливает структуру разреженности почти
на всем интервале. Ошибки первого рода меньше, следовательно разреживающий
алгоритм правильнее определяет нулевые элементы. При сильной разреженности
исходных данных (80% и более) ошибки второго рода малы у всех алгоритмов.

69

0,7

0.8

D

D

ΦΘ

0,6

0.7

D

Φ

Φ

D

0,5

0.6

Θ

0,4

0.5

0,3

0.4

0,2

0.3

0,1

0.2

0

0.2

ΦΘ

D

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.1
0

D

Θ

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

1.4

1.6

1.8

2

α, β = 0.01

α, β = 0.01

LDA-GS

PLSA-EM

0.8

0.8

D

D

ΦΘ

0.7

ΦΘ

0.7

D

Φ

D

0.6

0.6

Θ

DΦ
D

Θ

0.5
0.5
0.4
0.4
0.3
0.3

0.2

0.2

0.1

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.1
0

0.2

0.4

0.6

0.8

1

1.2

α, β = 0.1

α, β = 0.1

LDA-GS

PLSA-EM

0,7

0.7

D

ΦΘ

0,6

0.6

DΦ
DΘ

0,5

0.5

D

ΦΘ

0,4

DΦ

0.4

DΘ

0,3
0.3
0,2
0.2

0,1

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

0.1
0

0.2

0.4

0.6

0.8

1

1.2

α, β = 0.5

α, β = 0.5

LDA-GS

PLSA-EM

1.4

1.6

1.8

2

Рис. 5. Зависимость точности восстановления матриц Φ, Θ и ΦΘ от разреженности матрицы Θ порождающей модели при фиксированной разреженности матрицы Φ с параметрами β =
= 0.01, 0.1, 0.5, для алгоритмов LDA-GS и PLSA-EM.

70

0,9

0.9

DΦΘ

D
0.8
0.7

ΦΘ

0.8

DΦ

0.7

D

DΦ
DΘ

Θ

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

Φ

0.4

0.5

0.6

0.7

0.8

0.9

1

RΦ, RΘ = 0.1

R , R = 0.1
Θ

без разреживания

с разреживанием

Рис. 6. Зависимость точности восстановления матриц Φ, Θ и ΦΘ от разреженности RΦ при фиксированной разреженности RΘ = 0.1.

0,9
0.8

0.9

DΦΘ

0.8

D
0.7
0.6

Φ

0.7

DΘ

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.1

0.2

DΦ
DΘ

0.6

0.5

0

DΦΘ

0.3

0.4

0.5

0.6

0.7

R , R = 0.1
Θ

Φ

без разреживания

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

RΘ, RΦ = 0.1

с разреживанием

Рис. 7. Зависимость точности восстановления матриц Φ, Θ и ΦΘ от разреженности RΘ при фиксированной разреженности RΦ = 0.1.

71

0.9

0,45
1

0.8
0.7
0.6

S1

SΦ
S1

Θ
S2
Φ
S2
Θ

S1

0,35

S2

0,3

SΘ

Θ

0.5

0,25

0.4

0,2

0.3

0,15

0.2

0,1

0.1

0,05

0

0.1

Φ

0,4

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

Φ
2

0.1

0.2

0.3

R , R = 0.1
Φ

0.4

0.5

0.6

0.7

0.8

0.9

1

R , R = 0.1

Θ

Φ

без разреживания

Θ

с разреживанием

Рис. 8. Зависимость доли ошибок первого и второго рода при определении нулевых элементов в матрицах Φ, Θ при фиксированной разреженности RΘ = 0.1.

0.8

0.8
S1

0.7
0.6

1

SΦ

Φ
1
SΘ
S2
Φ
S2
Θ

0.7
0.6

Φ
Θ

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.1

S2
S2

0.5

0

1

SΘ

0.2

0.3

0.4

0.5

0.6

0.7

RΘ, RΦ = 0.1

без разреживания

0.8

0.9

1

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

R , R = 0.1
Θ

Φ

с разреживанием

Рис. 9. Зависимость доли ошибок первого и второго рода при определении нулевых элементов в матрицах Φ, Θ при фиксированной разреженности RΦ = 0.1.

72

Выводы. Произведение ΦΘ восстанавливается устойчиво и практически с одинаковой точностью всеми алгоритмами (PLSA-EM и LDA-GS, с разреживанием и без),
независимо от степени разреженности исходных данных.
Восстановление матриц Φ и Θ устойчиво только при условии, что справедлива гипотеза разреженности, то есть когда истинные матрицы Φ и Θ, породившие
коллекцию документов, разрежены на 80% или более.
Алгоритм постепенного принудительного разреживания не улучшает точность
восстановление матриц Φ и Θ по метрике Хеллингера, но уменьшает число ошибок
при определении структуры разреженности матриц Φ и Θ.

§13.3

Сравнение PLSA, LDA и SWB
(Потапенко А. А.)

Цель эксперимента: определить, какая из эвристик важнее — сглаживание, сэмплирование или робастность. Эти три эвристики могут комбинироваться в любых
сочетаниях, поэтому сравниваются 8 алгоритмов тематического моделирования.
Исходные данные и условия эксперимента. Эксперимент проводился на коллекциях RuDis и NIPS. Качество алгоритмов оценивалось перплексией обучающей
и контрольной коллекции.
При вычислении перплексии на документах d контрольной коллекции D ′ параметры ϕwt и фон πw оценивались по обучающей коллекции D, параметры θtd и νd
оценивались по первой половине документа d′ , параметры шума πdw оценивались для
каждой пары (d, w) согласно (4.7). Перплексия вычислялась по вторым половинам d′′
контрольных документов.
Сэмплирование (2.6) применялось с параметром s = ndw .
Сглаживание (3.3)–(3.4) применялось с параметрами αt = 0.5, βw = 0.01.
Робастность применялась с параметрами γ = 0.3, ε = 0.01.
Число тем |T | = 100.
Результаты представлены на рис. 10.
Выводы. Для обеих задач нет существенного различия перплексии между сглаженными моделями (LDA) и несглаженными (PLSA).
Робастные алгоритмы существенно превосходят неробастные и гораздо меньше
переобучаются.
Сэмплирование (2.6) сходится быстрее, но в итоге оказывается немного хуже
пропорционального распределения (2.2).
Сэмплирование без сглаживания может приводить к увеличению перплексии.
Величина переобучения (разность перплексии на обучающей и контрольной
выборке) больше зависит от задачи, чем от алгоритма. Сравнение алгоритмов по
перплексии на обучающей выборке приводит к тем же качественным выводам, что
и их сравнение по перплексии на контрольной выборке. По всей видимости, для
сравнения алгоритмов не нужна столь сложная методика разделения контрольных
документов для вычисления перплексии; вполне достаточно вычислять перплексию
только на обучающей выборке.

73
перплексия

перплексия

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800
600
0 2 4 6

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

0 2 4 6

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

P

PD

S

SD

P

PD

S

SD

PR

PDR

SR

SDR

PR

PDR

SR

SDR

RuDis

NIPS

Рис. 10. Зависимость контрольной перплексии от числа итераций для всевозможных сочетаний
эвристик: D — сглаживание Дирихле (αt = 0.5, βw = 0.01); R — робастность (γ = 0.3, ε = 0.01); S —
сэмплирование (s = ndw ), P — пропорциональное распределение (2.2); |T | = 100. Тонкие кривые без
точек — перплексия обучающей выборки.

§13.4

Разреживание матриц Φ и Θ
(Потапенко А. А.)

Согласно гипотезе разреженности, подавляющее большинство вероятностей
ϕwt = p(w | t) и θtd = p(t | d) равны нулю. Однако стандартный EM-алгоритм не позволяет оптимизировать структуру разреженности моделей PLSA и LDA, то есть узнать,
какие именно вероятности равны нулю. В PLSA структура разреженности фиксируется начальным приближением. В LDA априорные распределения Дирихле запрещают вероятностям ϕwt и θtd и гиперпараметрам βw и αt принимать нулевые значения.
Парадокс LDA заключается в том, что при βw → 0 и αt → 0 распределение Дирихле порождает сильно разреженные распределения ϕt , θd , в пределе стремящиеся
к вырожденным. В литературе часто встречается утверждение, что модель LDA более разрежена, чем PLSA. Однако при оценивании параметров модели по выборке
модель LDA, наоброт, оказывается менее разреженной, чем PLSA. Это с очевидностью следует из сравнения несмещённых частотных оценок PLSA (2.3)–(2.4) со сглаженными оценками LDA (3.4)–(3.3).
Известные подходы к разреживанию LDA требуют введения дополнительных
параметров и усложнения EM-алгоритма. В [17] предлагается хранить не сами значения ϕwt и θtd , а только их разности с фоновыми распределениями. В [61] предполагается, что каждая тема описывается распределением Дирихле на подмножестве слов,
заданном бинарными переменными bwt из распределения Бернулли. Сглаженность
и разреженность регулируется независимо параметрами распределения Дирихле и
распределения Бернулли. Недостатком данной модели является большое число дополнительных скрытых переменных, которые усложняют обучение. В [27] вводится
распределение псевдо-Дирихле, которое строится путём расширения области определения распределения Дирихле и имеет ограниченную плотность, в то время как
распределение Дирихле не ограничено в случае α < 1, что и приводит к запрету
нулевых значений ϕwt и θtd .

74

Целью эксперимента является исследование более простых стратегий принудительного разреживания, см. §5.3, стр. 38. Идея разреживания заключается в том,
чтобы в конце каждой итерации (полного прохода всей коллекции D) обнулять некоторое количество наименьших значений ϕwt и θtd . Теоретические обоснования разреживания приводятся в §5.3.
Считается, что сглаживание в моделях языка необходимо для описания новых
и редких слов. Мы предполагаем, что вместо сглаживания можно использовать противоположную эвристику — разреживание, а описание новых и редких слов возложить на робастную модель. Целью эксперимента является проверка этой гипотезы.
Для этого сравнивается разреживание робастных и неробастных моделей.
Исходные данные и условия эксперимента. Эксперимент проводился на коллекциях RuDis и NIPS. Качество моделей оценивалось по контрольной перплексии.
Предварительные эксперименты показали, что одновременное обнуление большого числа параметров слишком резко изменяет структуру модели, может приводить к снижению её качества и неравномерному разреживанию, когда некоторые
распределения оказываются сильно разреженными, тогда как другие почти не разреживаются. Поэтому предлагается разреживать матрицы Φ и Θ постепенно, придерживаясь одной из следующих стратегий.
Простая стратегия: в каждом из распределений ϕt , θd обнуляется заданная
доля r наименьших ненулевых значений. После обнуления производится перенормировка распределений. Число обнуляемых значений сокращается от итерации к итерации, поскольку доля берётся от числа ненулевых значений. Обнуления прекращаются, когда в распределении остаётся ⌊r −1 ⌋ ненулевых значений. Недостатком этой
стратегии является стремление к выравниванию доли ненулевых значений во всех
распределениях, что представляется довольно странным ограничением.
Сложная стратегия устраняет этот недостаток. В каждом из распределений
ϕt , θd обнуляется максимальное число наименьших значений, так, чтобы оно не превышало r|W | и r|T | соответственно, и сумма обнуляемых значений не превышала
заданного порога Sϕ или Sθ для распределений ϕt или θd соответственно. В экспериментах эта стратегия показала лучшие результаты. Задание параметров Sϕ , Sθ
эквивалентно заданию коэффициентов регуляризации τϕ , τθ в (5.5).
Начинать обнуления малых вероятностей можно и не дожидаясь сходимости.
После нескольких первых проходов коллекции становится ясно, что самые малые
вероятности останутся малыми на всех последующих итерациях. Поэтому разреживания включаются, начиная с итерации i0 , и производятся не на каждой итерации,
чтобы модель успевала восстановить адекватность. В экспериментах [2] разреживания включались на итерациях с номерами i = i0 + kδ, k = 1, 2, . . . , где i0 и δ —
параметры стратегии разреживания.
Под агрессивным разреживанием понимается уменьшение δ до 1 или уменьшение i0 до 1 или применение сложной стратегии, когда число обнуляемых значений
не уменьшается с итерациями.
В каждом распределении p(w | t) и p(t | d) должно оставаться хотя бы одно ненулевое значение. Если в результате разреживания все вероятности p(t | d, w) оцениваются как нулевые, то термин w считается нетематическим в документе d. Поэтому
разреживание применяется совместно с робастной моделью (4.1), либо с упрощённой
робастной моделью (4.8).

75

итерации

разреженность

4 200
4 000
3 800
3 600

итерации

разреженность

1.0

3 200

1.0

0.9

3 000

0.9

0.8

2 800

0.8

2 600

3 400
3 200

0.7

3 000

0.6

2 800
2 600

0.5

2 000

0.5

0.4

1 800

0.4

2 000

0.3

1 600

0.3

1 800
1 600

0.2

1 400
1 200

0.1

0.7
2 400

2 400
2 200

0.6

2 200

1 400

0.2

1 200

1 000

0.1

1 000

0
0

5

0%

10

15

15:2:10%

20

25

30

10:2:15%

35

0

40

0

10:2:20%

0%

RuDis, разреживание через 2 итерации
итерации

5

разреженность

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

10

15

15:2:10%

20

25

30

10:2:15%

35

40

10:2:20%

NIPS, разреживание через 2 итерации
итерации

разреженность

1.0

3 200

1.0

0.9

3 000

0.9

0.8

2 800

0.8

2 600

0.7

2 400

0.7

0.6

2 200

0.6

0.5

2 000

0.5

1 800
0.4

1 600

0.4

0.3

1 400

0.3

0.2

1 200

0.2

1 000
0.1
0
0

5

15:1:15%

10

15

20

1:2:10%

25

30

35

0

15:2:15%, th:10%, ph:0.1%

5

15:1:15%

RuDis, агрессивное разреживание
итерации

0

600

40

10

15

20

1:2:10%

25

30

35

40

15:2:15%, th:10%, ph:0.1%

NIPS, агрессивное разреживание

разреженность

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

0.1

800

итерации

разреженность

1.0

4 500

1.0

0.9

4 000

0.9

0.8

3 500

0.7

0.8
0.7

3 000

0.6

0.6
2 500

0.5

0.5
2 000
0.4
0.3
0.2

1 000

0.1

500

0
0

0%

5

10

15:2:5%

15

20

25

15:2:10%

30

35

10:2:15%

RuDis, SEM, через 2 итерации

40

0.4
1 500

0.3
0.2
0.1

0

0
0

0%

5

10

15:2:5%

15

20

25

15:2:10%

30

35

40

10:2:15%

NIPS, SEM, через 2 итерации

Рис. 11. Зависимость перплексии (◦) и разреженности матриц Φ (△) и Θ (△) от числа итераций
для рационального и стохастического EM-алгоритма при различных параметрах разреживания,
обозначаемых i0 :δ:r, th:Sθ , ph:Sϕ . Число тем |T | = 100.

76

итерации

разреженность

итерации

разреженность

1.0

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

1.0
3 000

0.9

0.9

2 800

0.8

2 600

0.8

0.7

2 400

0.7

0.6

2 200

0.6

2 000
0.5

1 800

0.5

0.4

1 600

0.4

0.3

1 400

0.3

1 200
0.2

0

5

0%

10

15

15:2:10%

20

25

30

10:2:15%

35

800

0

600

40

0.1
0
0

10:2:20%

5

0%

RuDis, разреживание через 2 итерации
итерации

0.2

1 000

0.1

разреженность

10

15

15:2:10%

20

25

30

10:2:15%

35

40

10:2:20%

NIPS, разреживание через 2 итерации
итерации

разреженность

1.0

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

1.0
3 000

0.9

0.9

2 800

0.8

2 600

0.8

0.7

2 400

0.7

0.6

2 200

0.6

2 000
0.5

0.5

1 800

0.4

1 600

0.4

0.3

1 400

0.3

0.2

1 200

0.2

1 000
0.1
0
0

5

15:1:15%

10

15

20

1:2:10%

25

30

35

600

40

0
0

15:2:15%, th:10%, ph:0.1%

5

15:1:15%

RuDis, агрессивное разреживание
итерации

0.1

800

10

15

20

1:2:10%

25

30

35

40

15:2:15%, th:10%, ph:0.1%

NIPS, агрессивное разреживание

разреженность

итерации

разреженность

1.0

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

1.0
3 000

0.9

0.9

2 800

0.8

2 600

0.8

0.7

2 400

0.7

0.6

2 200

0.6

2 000
0.5

0.5

1 800

0.4

1 600

0.4

0.3

1 400

0.3

0.2

1 200

0.2

1 000
0.1
0
0

0%

5

10

15:2:5%

15

20

25

15:2:10%

30

35

10:2:15%

RuDis, SEM, через 2 итерации

40

0.1

800

0

600
0

0%

5

10

15:2:5%

15

20

25

15:2:10%

30

35

40

10:2:15%

NIPS, SEM, через 2 итерации

Рис. 12. Зависимость перплексии (◦) и разреженности матриц Φ (△) и Θ (△) от числа итераций для
рационального и стохастического робастного EM-алгоритма с параметрами робастности γ = 0.3,
ε = 0.01 и параметрами разреживания i0 :δ:r, th:Sθ , ph:Sϕ . Число тем |T | = 100.

77

итерации

разреженность

итерации

разреженность

1.0

4 200
4 000

0.9

3 800

1.0
3 000

0.9

2 800

3 600

0.8

0.8
2 600

3 400
0.7

3 200

0.7
2 400

3 000

0.6

0.6
2 200

2 800
2 600

0.5

2 400

0.4

2 200
0.3

2 000
1 800
1 600
1 400
1 200
0

5

0%

10

15

15:2:10%

20

25

30

10:2:15%

35

0.4

1 800

0.3

1 600

0.2

1 400

0.2

0.1

1 200

0.1

0

1 000

40

0
0

10:2:20%

5

0%

RuDis, разреживание через 2 итерации
итерации

0.5

2 000

разреженность

10

15

15:2:10%

20

25

30

10:2:15%

35

40

10:2:20%

NIPS, разреживание через 2 итерации
итерации

разреженность

1.0

4 200

1.0
3 000

4 000

0.9

3 800

0.9
2 800

3 600

0.8

0.8
2 600

3 400

0.7

3 200

0.7
2 400

3 000

0.6

2 800
2 600

0.5

2 400

0.4

2 200
0.3

2 000
1 800

0.2

1 600

0.1

1 400
1 200

0
0

5

15:1:15%

10

15

20

1:2:10%

25

30

35

0.4

1 800

0.3

1 600

0.2

1 400

0.1

1 200

0

1 000
0

15:2:15%, th:10%, ph:0.1%

5

15:1:15%

RuDis, агрессивное разреживание
итерации

0.5

2 000

40

10

15

20

1:2:10%

25

30

35

40

15:2:15%, th:10%, ph:0.1%

NIPS, агрессивное разреживание

разреженность

4 200

0.6

2 200

итерации

разреженность

1.0

4 000

0.9

3 800

1.0
3 000

0.9

0.8

2 800

0.8

0.7

2 600

0.7

0.6

2 400

0.6

0.5

2 200

0.5

0.4

2 000

0.4

0.3

1 800

0.3

1 800

0.2

1 600

0.2

1 600

0.1

3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000

1 400

0.1

1 400

0

1 200
0

0%

5

10

15:2:5%

15

20

25

15:2:10%

30

35

10:2:15%

RuDis, SEM, через 2 итерации

40

0
0

0%

5

10

15:2:5%

15

20

25

15:2:10%

30

35

40

10:2:15%

NIPS, SEM, через 2 итерации

Рис. 13. Зависимость перплексии (◦) и разреженности матриц Φ (△) и Θ (△) от числа итераций
для рационального и стохастического робастного EM-алгоритма при малой априорной вероятности
шума γ = 0.01, ε = 0.01, с параметрами разреживания i0 :δ:r, th:Sθ , ph:Sϕ . Число тем |T | = 100.

78

Результаты экспериментов [2]. На рис. 11 показаны зависимости контрольной перплексии и разреженности матриц Φ и Θ от числа итераций при различных стратегиях
разреживания. На рис. 12 показаны аналогичные зависимости для робастных моделей с априорной долей шума γ = 0.3, на рис. 13 — с априорной долей шума γ = 0.01.
Наименьшая перплексия и одновременно наибольшая разреженность матрицы Φ до 99.4% для RuDis и 99.6% для NIPS достигается при использовании упрощённого робастного стохастического EM-алгоритма с параметрами r = 0.15, Sθ = 0.1,
Sϕ = 0.001, i0 от 15 до 20, δ = 1 или 2, рис. 11. Разреживание распределений ϕt свыше 99% при числе тем T = 100 означает, что каждый термин в среднем относится
только к одной теме.
В робастных алгоритмах с шумом и фоном (SWB) разреживание почти не влияет на перплексию и позволяет достигать сопоставимой разреженности, рис. 12.
В неробастных алгоритмах агрессивное разреживание может приводить к обратному результату — снижению разреженности ϕt до 80%.
При недостаточном априорном уровне шума γ = 0.01 агрессивное разреживание
может приводить к расходимости EM-алгоритма, рис. 13. Тонкие кривые без точек,
проходящие чуть ниже кривых контрольной перплексии, соответствуют перплексии
на обучающей выборке. Они показывают, что расходимость возникает синхронно на
контроле и обучении. Поэтому её легко обнаружить на стадии обучения, непосредственно во время итераций EM-алгоритма.
Для упрощённой робастной модели расходимость ни разу не наблюдалась.
Эвристика разреживания плохо совместима со сглаживанием и применяется
только к PLSA, то есть при βw = 0, αt = 0.
Выводы. Робастные модели с разреживанием не нуждаются в сглаживании. Для
практического применения рекомендуется упрощённый робастный EM-алгоритм
с достаточно высоким априорным уровнем шума γ и агрессивным разреживанием.

§13.5

Разреживание распределений тем p(t | d, w)
(Потапенко А. А.)

Цель эксперимента. Недостатком Алгоритма 2.2 является необходимость хранить
массив значений ndwt = ndw p(t | d, w), t ∈ T , для каждого термина (d, w). Расход памяти объёма O(n|T |) может оказаться неприемлемым даже при небольшом числе
тем. С другой стороны, согласно гипотезе разреженности, этот массив должен состоять преимущественно из нулей. Сэмплирование решает данную проблему, однако остаётся вопрос, не вносит ли генератор случайных чисел дополнительный шум,
и не существует ли более простого способа разреживания распределений p(t | d, w).
Стратегия максимального разреживания распределений p(t | d, w) представляется наиболее естественной. Для каждого термина (d, w) остаются только s тем с наибольшими значениями ndwt , вероятности остальных темы обнуляются.
Целью эксперимента является сравнение различных стратегий разреживания.
Исходные данные и условия эксперимента. Эксперимент проводился на коллекциях RuDis и NIPS. Качество моделей оценивалось по контрольной перплексии.

79

Результаты. Стратегия максимального разреживания приводит к накоплению систематической ошибки и расходимости EM-алгоритма, рис. 14. На первых же итерациях возникает сильная (свыше 90%) разреженность распределений ϕwt = p(w | t),
которые к этому моменту ещё не сошлись. Значения ϕwt , оказавшиеся равными нулю,
далее так и остаются нулевыми.
Включение разреживания с 10-й итерации даёт лучшие результаты, но также
может приводит к расходимости (средний ряд графиков на рис. 14). При этом наблюдается интересный положительный эффект: сразу после разреживания перплексия
улучшается скачком, даже при малых s = 1, 2.
Эвристика сглаживания даёт ещё лучшие результаты (нижний ряд графиков
на рис. 14). Расходимость не возникает, но качество получаемой модели всё же хуже,
чем при s = |T |, то есть когда разреживание не применяется. Снова наблюдается
эффект скачкообразного падения перплексии сразу после разреживания.
Максимальное разреживание при s = 1 можно интерпретировать как применение оптимального байесовского решающего правила в задаче классификации терминов в документах (d, w) на |T | классов-тем. Оно даёт адекватный результат только
после достижения сходимости в EM-алгоритме, рис. 15. При этом перплексия незначительно ухудшается, разреженность ϕt и θd достигает 0.8–0.9.
Менее требовательной к настройке параметров оказалась стратегия постепенного разреживания, когда в каждом распределении p(t | d, w) обнуляется заданная
доля r наименьших ненулевых значений и производится перенормировка. При r 6 0.2
расходимость не возникает, и финальная перплексия мало отличается от случая
r = 0, см. рис. 16. При включении разреживаний, начиная с 10-й итерации, темп
разреживания r можно увеличить (средний ряд графиков на рис. 16). При этом
постепенно увеличивается разреженность распределений θtd (до 0.5 и выше) и распределений ϕwt (немного ниже 0.5).
Робастные алгоритмы более устойчивы к постепенному разреживанию распределений p(t | d, w). У них расходимость не наблюдалась, темп разреживания можно
увеличивать до r = 0.7, (нижний ряд графиков на рис. 16). При этом разреженность ϕwt достигает почти 0.9, разреженность θtd достигает 0.7.
Выводы. Стратегии максимального и постепенного разреживания требуют хранения слабо разреженного массива ndwt или p(t | d, w), хотя бы на ранних итерациях.
Этим они уступают стохастическому EM-алгоритму.
Максимальное разреживание может использоваться по окончании итераций
для жёсткого присвоения тем каждому термину в каждом документе (d, w), практически без ухудшения перплексии.
Максимальное разреживание даёт резкое улучшение перплексии непосредственно после его применения. Возникает гипотеза, что максимальное разреживание на отдельных промежуточных итерациях EM-алгоритма может улучшать его
сходимость. Возможно также, что непосредственно после итераций разреживания
придётся применять сглаживание.

§13.6

Экономное сэмплирование
(Потапенко А. А.)

80

перплексия

перплексия
6 500

4 200
6 000

4 000
3 800

5 500

3 600
3 400

5 000

3 200
3 000

4 500

2 800

4 000

2 600
3 500

2 400
2 200

3 000

2 000
2 500

1 800
1 600

2 000

1 400
0 2 4 6

s=1

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

s=2

s=10

ndw

s=3

s=4

0 2 4 6

s=5

s=1

|T|

s=10

RuDis, разреживание с 1-й итерации
перплексия

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

s=2
ndw

s=3

s=4

s=5

|T|

NIPS, разреживание с 1-й итерации
перплексия
4 200

4 200
4 000

4 000

3 800

3 800

3 600

3 600

3 400

3 400

3 200
3 000

3 200

2 800

3 000

2 600

2 800

2 400

2 600

2 200
2 000

2 400

1 800

2 200

1 600

2 000

1 400
0 2 4 6

s=1

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

s=2

s=10

ndw

s=3

s=4

0 2 4 6

s=5

s=1

|T|

s=10

RuDis, разреживание с 10-й итерации

ndw

s=3

s=4

s=5

|T|

NIPS, разреживание с 10-й итерации

перплексия

перплексия

4 200

3 200

4 000

3 100

3 800

3 000

3 600

2 900

3 400

2 800

3 200

2 700

3 000

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

s=2

2 600

2 800

2 500

2 600

2 400

2 400
2 200

2 300

2 000

2 200

1 800

2 100

1 600

2 000

1 400
0 2 4 6

s=1
s=10

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

s=2
ndw

s=3

s=4

s=5

|T|

RuDis, с 10-й итерации, сглаживание LDA

0 2 4 6

s=1
s=10

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40

s=2
ndw

s=3

s=4

s=5

|T|

NIPS, с 10-й итерации, сглаживание LDA

Рис. 14. Зависимость перплексии от числа итераций в рациональном EM-алгоритме при максимальном разреживании p(t | d, w). Параметр разреживания: s = 1, 2, 3, 4, 5, 10, ndw , при s = |T |
разреживания нет. Параметры сглаживания: αt = 0.5, βw = 0.01. Число тем |T | = 100.

81

перплексия

разреженность

4 200

1.0

4 000

0.9

перплексия

разреженность

3 200

1.0

3 100

0.9

3 000

3 800
3 600
3 400
3 200

0.8

2 900

0.8

0.7

2 800

0.7

0.6

3 000

2 700

0.6

2 600

2 800

0.5

2 600

0.4

2 400
0.3

2 200

0.5

2 500
2 400

0.4

2 300

0.3

2 200

2 000

0.2

1 800
1 600
1 400
0

5

10

15

20

25

30

35

40

45

50

55

0.1

2 000

0

1 900

60

0
5

10

15

20

4 000

30

35

40

45

50

55

60

NIPS, разреживание с 40-й итерации

разреженность

4 200

25

s=1, начиная с 40-й итерации

без разреживания

RuDis, разреживание с 40-й итерации
перплексия

0.1

0

s=1, начиная с 40-й итерации

без разреживания

0.2

2 100

перплексия

разреженность

1.0

3 200

1.0

0.9

3 100

0.9

3 000

3 800

0.8

3 600
3 400
3 200

0.8

2 900

0.7

2 800

0.7

0.6

2 700

0.6

3 000

2 600

2 800

0.5

2 600

0.4

2 400

0.4

0.3

2 300

0.3

2 400
2 200

0.5

2 500

2 200

2 000

0.2

1 800
1 600
1 400
0

5

10

15

без разреживания

20

25

30

35

40

45

50

55

0.1

2 000

0

1 900

60

s=1, начиная с 40-й итерации

RuDis, с 40-й итерации, сглаживание LDA

0.2

2 100

0.1
0
0

5

10

15

без разреживания

20

25

30

35

40

45

50

55

60

s=1, начиная с 40-й итерации

NIPS, с 40-й итерации, сглаживание LDA

Рис. 15. Зависимость перплексии (◦) и разреженности матриц Φ (△) и Θ (△) от числа итераций при
максимальном разреживании s = 1 после достижения сходимости в рациональном EM-алгоритме.
Параметры сглаживания: αt = 0.5, βw = 0.01. Число тем |T | = 100.

82

перплексия

разреженность

перплексия

разреженность

1.0

4 200

1.0

0.9

3 600

0.9

0.8

3 400

0.8

3 400

0.7

3 200

0.7

3 200

0.6

3 000

0.6

2 800

0.5

2 800

0.5

2 600

0.4

4 000
3 800
3 600

3 000

2 400

0.3

2 200

0.4

2 600

0.3

2 400

0.2

2 000

0.2
2 200

1 800

0.1

1 600
0

1 400
0

5

10

r=0

r=0.1

фи

тета

15

20

25

r=0.2

30

35

r=0.3

40

0
0

r=0.5

RuDis, разреживание с 1-й итерации
перплексия

0.1
2 000
5

10

r=0

r=0.1

фи

тета

15

20

25

r=0.2

30

35

r=0.3

40

r=0.5

NIPS, разреживание с 1-й итерации

разреженность

перплексия

разреженность

4 200

1.0

3 200

1.0

4 000

0.9

3 100

0.9

3 800

0.8

3 600
3 400
3 200

3 000

0.8

2 900

0.7

2 800

0.7

0.6

2 700

0.6

3 000

2 600

2 800

0.5

2 600

0.4

2 400

0.4

0.3

2 300

0.3

0.5

2 500

2 400
2 200
2 000

0.2

2 200

0.2

2 100

1 800

0.1

1 600

0

1 400
0

5

r=0

10

15

r=0.1

r=0.7

20

25

r=0.2

фи

30

35

r=0.5

0.1

2 000

0

1 900

40

0

r=0.6

5

r=0

RuDis, разреживание с 10-й итерации
перплексия

15

r=0.1

r=0.7

тета

10

20

25

r=0.2

фи

30

35

r=0.5

40

r=0.6

тета

NIPS, разреживание с 10-й итерации

разреженность

перплексия

разреженность

1.0

4 200
4 000
3 800
3 600
3 400
3 200
3 000
2 800
2 600
2 400
2 200
2 000
1 800
1 600
1 400
1 200
1 000
800

1.0
3 000

0.9

0.9

2 800

0.8

2 600

0.8

0.7

2 400

0.7

0.6

2 200

0.6

2 000
0.5

1 800

0.5

0.4

1 600

0.4

0.3

1 400

0.3

0.2

0

r=0
r=0.7

5

10

r=0.1
фи

15

20

r=0.2

25

30

r=0.5

35

1 200

800

0

600

40

r=0.6

тета

RuDis, с 10-й итерации, робастный PLSA

0.2

1 000

0.1

0.1
0
0

r=0
r=0.7

5

10

r=0.1
фи

15

20

r=0.2

25

30

r=0.5

35

40

r=0.6

тета

NIPS, с 10-й итерации, робастный PLSA

Рис. 16. Зависимость перплексии (◦) и разреженности матриц Φ (△) и Θ (△) от числа итераций
в рациональном EM-алгоритме при постепенном разреживании p(t | d, w) с параметром r от 0 до 0.7.
Параметры робастности: ε = 0.01, γ = 0.3. Число тем |T | = 100.

83

§13.7

Частота обновления параметров ϕwt и θtd
(Потапенко А. А.)

§13.8 Оптимизация параметров робастного алгоритма
(Потапенко А. А.)
§13.9

Онлайновые алгоритмы
(Китов В. В., Потапенко А. А.)

§13.10

Категоризация: тематическая модель против SVM
(Гаврилюк К. А.)

§13.11

Качество категоризации для иерархических моделей

Список литературы
[1] Воронцов К. В., Потапенко А. А. Регуляризация, робастность и разреженность
вероятностных тематических моделей // Компьютерные исследования и моделирование. — 2012. — Т. 4, № 4. — С. 693–706.
[2] Воронцов К. В., Потапенко А. А. Модификации EM-алгоритма для вероятностного тематического моделирования // Машинное обучение и анализ данных. —
2013 (в печати).
[3] Гиляревский Р. С., Шапкин А. В., Белоозеров В. Н. Рубрикатор как инструмент
информационной навигации. — СПб.: Профессия, 2008. — 352 с.
[4] Лукашевич Н. В. Тезаурусы в задачах информационного поиска. — Издательство МГУ имени М. В. Ломоносова, 2011.
[5] Маннинг К. Д., Рагхаван П., Шютце Х. Введение в информационный поиск. —
Вильямс, 2011.
[6] Павлов А. С., Добров Б. В. Метод обнаружения массово порожденных неестественных текстов на основе анализа тематической структуры // Вычислительные методы и программирование: новые вычислительные технологии. — 2011. —
Т. 12. — С. 58–72.
[7] Тихонов А. Н., Арсенин В. Я. Методы решения некорректных задач. — М.: Наука, 1986.
[8] Asuncion A., Welling M., Smyth P., Teh Y. W. On smoothing and inference for topic
models // Proceedings of the International Conference on Uncertainty in Artiﬁcial
Intelligence. — 2009.
[9] Blei D. M., Ng A. Y., Jordan M. I. Latent Dirichlet allocation // Journal of Machine
Learning Research. — 2003. — Vol. 3. — Pp. 993–1022.
[10] Buntine W. L. Estimating likelihoods for topic models // 1st Asian Conference on
Machine Learning: Advances in Machine Learning. — 2009. — Pp. 51–64.

84

[11] Celeux G., Chauveau D., Diebolt J. On stochastic versions of the EM algorithm:
Tech. Rep. RR-2514: INRIA, 1995.
[12] Chemudugunta C., Smyth P., Steyvers M. Modeling general and speciﬁc aspects
of documents with a probabilistic topic model // Advances in Neural Information
Processing Systems. — MIT Press, 2007. — Vol. 19. — Pp. 241–248.
[13] Cressie N., Read T. R. C. Multinomial goodness-of-ﬁt tests // Journal of the Royal
Statistical Society, Series B. — 1984. — Vol. 46, no. 3. — Pp. 440–464.
[14] Daud A., Li J., Zhou L., Muhammad F. Knowledge discovery through directed
probabilistic topic models: a survey // Frontiers of Computer Science in China. —
2010. — Vol. 4, no. 2. — Pp. 280–301.
[15] Dempster A. P., Laird N. M., Rubin D. B. Maximum likelihood from incomplete
data via the EM algorithm // J. of the Royal Statistical Society, Series B. — 1977. —
no. 34. — Pp. 1–38.
[16] Dietz L., Bickel S., Scheﬀer T. Unsupervised prediction of citation inﬂuences //
Proceedings of the 24th international conference on Machine learning. — ICML ’07. —
New York, NY, USA: ACM, 2007. — Pp. 233–240.
[17] Eisenstein J., Ahmed A., Xing E. P. Sparse additive generative models of text //
ICML’11. — 2011. — Pp. 1041–1048.
[18] Feng Y., Lapata M. Topic models for image annotation and text illustration //
Human Language Technologies: The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguistics. — Association for
Computational Linguistics, 2010. — Pp. 831–839.
[19] Friedman J. H., Hastie T., Tibshirani R. Regularization paths for generalized linear
models via coordinate descent // Journal of Statistical Software. — 2010. — Vol. 33,
no. 1. — Pp. 1–22.
[20] Grün B., Hornik K. Topicmodels: An R package for ﬁtting topic models // Journal
of Statistical Software. — 2011. — Vol. 40, no. 13. — Pp. 1–30.
[21] Hoﬀman M. D., Blei D. M., Bach F. R. Online learning for latent dirichlet
allocation // NIPS. — Curran Associates, Inc., 2010. — Pp. 856–864.
[22] Hofmann T. Probabilistic latent semantic indexing // Proceedings of the 22nd annual
international ACM SIGIR conference on Research and development in information
retrieval. — New York, NY, USA: ACM, 1999. — Pp. 50–57.
[23] Ito K., Jin B., Takeuchi T. Multi-parameter Tikhonov regularization // The
Computing Research Repository (CoRR). — 2011. — Vol. abs/1102.1173.
[24] Kataria S., Mitra P., Caragea C., Giles C. L. Context sensitive topic models
for author inﬂuence in document networks // Proceedings of the Twenty-Second
international joint conference on Artiﬁcial Intelligence — Volume 3. — IJCAI’11. —
AAAI Press, 2011. — Pp. 2274–2280.

85

[25] Kim S.-H., Choi H., Lee S. Estimate-based goodness-of-ﬁt test for large sparse
multinomial distributions // Computational Statistics and Data Analysis. — 2009. —
Vol. 53, no. 4. — Pp. 1122 – 1131.
[26] Krestel R., Fankhauser P., Nejdl W. Latent dirichlet allocation for tag
recommendation // Proceedings of the third ACM conference on Recommender
systems. — ACM, 2009. — Pp. 61–68.
[27] Larsson M. O., Ugander J. A concave regularization technique for sparse mixture
models // Advances in Neural Information Processing Systems 24 / Ed. by J. ShaweTaylor, R. Zemel, P. Bartlett, F. Pereira, K. Weinberger. — 2011. — Pp. 1890–1898.
[28] Lau J. H., Baldwin T., Newman D. On collocations and topic models // ACM
- Transactions on Speech and Language Processing. — 2013. — Vol. 10, no. 3. —
Pp. 10:1–10:14.
[29] LeCun Y., Denker J., Solla S., Howard R. E., Jackel L. D. Optimal brain damage //
Advances in Neural Information Processing Systems II / Ed. by D. S. Touretzky. —
San Mateo, CA: Morgan Kauﬀman, 1990.
[30] Li X.-X., Sun C.-B., Lu P., Wang X.-J., Zhong Y.-X. Simultaneous image
classiﬁcation and annotation based on probabilistic model // The Journal of China
Universities of Posts and Telecommunications. — 2012. — Vol. 19, no. 2. — Pp. 107–
115.
[31] Lu Y., Mei Q., Zhai C. Investigating task performance of probabilistic topic models:
an empirical study of PLSA and LDA // Information Retrieval. — 2011. — Vol. 14,
no. 2. — Pp. 178–203.
[32] Mann G. S., McCallum A. Simple, robust, scalable semi-supervised learning via
expectation regularization // Proceedings of the 24th international conference on
Machine learning. — ICML ’07. — New York, NY, USA: ACM, 2007. — Pp. 593–600.
[33] Masada T., Kiyasu S., Miyahara S. Comparing LDA with pLSI as a dimensionality
reduction method in document clustering // Proceedings of the 3rd International
Conference on Large-scale knowledge resources: construction and application. —
LKR’08. — Springer-Verlag, 2008. — Pp. 13–26.
[34] Mimno D., Blei D. Bayesian checking for topic models // 11th Conference
on Empirical Methods in Natural Language Processing. — Association for
Computational Linguistics, 2011. — Pp. 227–237.
[35] Mimno D., Wallach H. M., Talley E., Leenders M., McCallum A. Optimizing
semantic coherence in topic models // Proceedings of the Conference on Empirical
Methods in Natural Language Processing. — EMNLP ’11. — Stroudsburg, PA, USA:
Association for Computational Linguistics, 2011. — Pp. 262–272.
[36] Minka T. P. Estimating a dirichlet distribution: Tech. rep.: 2000 (revised 2003, 2009,
2012).

86

[37] Munkres J. Algorithms for the assignment and transportation problems // Journal
of the Society for Industrial and Applied Mathematics. — 1957. — Vol. 5, no. 1. —
Pp. 32–38.
[38] Neal R. M., Hinton G. E. A view of the EM algorithm that justiﬁes incremental,
sparse, and other variants // Learning in graphical models / Ed. by M. I. Jordan. —
Cambridge, MA, USA: MIT Press, 1999. — Pp. 355–368.
[39] Newman D., Bonilla E. V., Buntine W. L. Improving topic coherence with
regularized topic models // Advances in Neural Information Processing Systems 24 /
Ed. by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, K. Weinberger. — 2011. —
Pp. 496–504.
[40] Newman D., Karimi S., Cavedon L. External evaluation of topic models //
Australasian Document Computing Symposium. — December 2009. — Pp. 11–18.
[41] Newman D., Lau J. H., Grieser K., Baldwin T. Automatic evaluation of topic
coherence // Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics. — HLT
’10. — Stroudsburg, PA, USA: Association for Computational Linguistics, 2010. —
Pp. 100–108.
[42] Newman D., Noh Y., Talley E., Karimi S., Baldwin T. Evaluating topic models
for digital libraries // Proceedings of the 10th annual Joint Conference on Digital
libraries. — JCDL ’10. — New York, NY, USA: ACM, 2010. — Pp. 215–224.
[43] Nigam K., McCallum A. K., Thrun S., Mitchell T. Text classiﬁcation from labeled
and unlabeled documents using EM // Machine Learning. — 2000. — Vol. 39, no. 23. — Pp. 103–134.
[44] Pecina P., Schlesinger P. Combining association measures for collocation
extraction // Proceedings of the COLING/ACL on Main conference poster
sessions. — Association for Computational Linguistics, 2006. — Pp. 651–658.
[45] Ramage D., Hall D., Nallapati R., Manning C. D. Labeled lda: a supervised topic
model for credit attribution in multi-labeled corpora // Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing: Volume 1 Volume 1. — EMNLP ’09. — Stroudsburg, PA, USA: Association for Computational
Linguistics, 2009. — Pp. 248–256.
[46] Read T., Cressie N. Goodness-of-Fit Statistics for Discrete Mutivariate Data. —
Springer, New York, 1988.
[47] Rosen-Zvi M., Griﬃths T., Steyvers M., Smyth P. The author-topic model for
authors and documents // Proceedings of the 20th conference on Uncertainty in
artiﬁcial intelligence. — UAI ’04. — Arlington, Virginia, United States: AUAI Press,
2004. — Pp. 487–494.
[48] Rubin T. N., Chambers A., Smyth P., Steyvers M. Statistical topic models for multilabel document classiﬁcation // Machine Learning. — 2012. — Vol. 88, no. 1-2. —
Pp. 157–208.

87

[49] Sebastiani F. Machine learning in automated text categorization // ACM Computing
Surveys. — 2002. — Vol. 34, no. 1. — Pp. 1–47.
[50] Steyvers M., Griﬃths T. Finding scientiﬁc topics // Proceedings of the National
Academy of Sciences. — 2004. — Vol. 101, no. Suppl. 1. — Pp. 5228–5235.
[51] Steyvers M., Griﬃths T. Probabilistic Topic Models // Handbook of Latent Semantic
Analysis / Ed. by T. Landauer, D. Mcnamara, S. Dennis, W. Kintsch. — Lawrence
Erlbaum Associates, 2007.
[52] Tan Y., Ou Z. Topic-weak-correlated latent dirichlet allocation // 7th International
Symposium Chinese Spoken Language Processing (ISCSLP). — 2010. — Pp. 224–228.
[53] Taneichi N., Sekiya Y., Imai H. Improvements of goodness-of-ﬁt statistics for sparse
multinomials based on normalizing transformations // Annals of the Institute of
Statistical Mathematics. — 2003. — Vol. 55. — Pp. 831–848.
[54] Teh Y. W., Newman D., Welling M. A collapsed variational bayesian inference
algorithm for latent dirichlet allocation // NIPS. — 2006. — Pp. 1353–1360.
[55] TextFlow: Towards better understanding of evolving topics in text. / W. Cui, S. Liu,
L. Tan, C. Shi, Y. Song, Z. Gao, H. Qu, X. Tong // IEEE transactions on visualization
and computer graphics. — 2011. — Vol. 17, no. 12. — Pp. 2412–2421.
[56] Varadarajan J., Emonet R., Odobez J.-M. A sparsity constraint for topic models
— application to temporal activity mining // NIPS-2010 Workshop on Practical
Applications of Sparse Modeling: Open Issues and New Directions. — 2010.
[57] Vulić I., Smet W., Moens M.-F. Cross-language information retrieval models based
on latent topic models trained with document-aligned comparable corpora //
Information Retrieval. — 2012. — Pp. 1–38.
[58] Wallach H. Structured Topic Models for Language: Ph.D. thesis / Newnham College,
University of Cambridge. — 2008.
[59] Wallach H., Mimno D., McCallum A. Rethinking LDA: Why priors matter //
Advances in Neural Information Processing Systems 22 / Ed. by Y. Bengio,
D. Schuurmans, J. Laﬀerty, C. K. I. Williams, A. Culotta. — 2009. — Pp. 1973–1981.
[60] Wallach H., Murray I., Salakhutdinov R., Mimno D. Evaluation methods for topic
models // 26th International Conference on Machine Learning, Montreal, Canada. —
2009. — Pp. 1105–1112.
[61] Wang C., Blei D. M. Decoupling sparsity and smoothness in the discrete hierarchical
dirichlet process // NIPS. — Curran Associates, Inc., 2009. — Pp. 1982–1989.
[62] Wang Y. Distributed Gibbs sampling of latent dirichlet allocation: The gritty
details. — 2008.
[63] Wu Y., Ding Y., Wang X., Xu J. A comparative study of topic models for topic
clustering of chinese web news // Computer Science and Information Technology
(ICCSIT), 2010 3rd IEEE International Conference on. — Vol. 5. — july 2010. —
Pp. 236–240.

88

[64] Yeh J.-h., Wu M.-l. Recommendation based on latent topics and social network
analysis // Proceedings of the 2010 Second International Conference on Computer
Engineering and Applications. — Vol. 1. — IEEE Computer Society, 2010. — Pp. 209–
213.
[65] Yi X., Allan J. A comparative study of utilizing topic models for information
retrieval // Advances in Information Retrieval. — Springer Berlin Heidelberg, 2009. —
Vol. 5478 of Lecture Notes in Computer Science. — Pp. 29–41.
[66] Zavitsanos E., Paliouras G., Vouros G. A. Non-parametric estimation of topic
hierarchies from texts with hierarchical Dirichlet processes // Journal of Machine
Learning Research. — 2011. — Vol. 12. — Pp. 2749–2775.
[67] Zelterman D. Goodness-of-ﬁt tests for large sparse multinomial distributions //
Journal of the American Statistical Association. — 1987. — Vol. 398, no. 82. —
Pp. 624–629.
[68] Zhang J., Song Y., Zhang C., Liu S. Evolutionary hierarchical Dirichlet processes
for multiple correlated time-varying corpora // Proceedings of the 16th ACM
SIGKDD international conference on Knowledge discovery and data mining. —
2010. — Pp. 1079–1088.
[69] Zhang Z., Iria J., Brewster C., Ciravegna F. A comparative evaluation of term
recognition algorithms // Proceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC08). — 2008.
[70] Zou H., Hastie T. Regularization and variable selection via the elastic net // Journal
of the Royal Statistical Society B. — 2005. — Vol. 67. — Pp. 301–320.


