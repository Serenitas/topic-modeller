Одним из основных научно-технических направлений деятельности НИИ «Центрпрограммсистем» (г. Тверь) является разработка АСУ в защищенном исполнении. При этом под защищенным исполнением АСУ понимается наличие в ее составе организационных и программно-технических средств защиты информации от несанкционированного доступа.  Разработка автоматизированных систем выполняется, как правило, в виде опытно-конструкторских работ (ОКР) и регламентируется в основном ГОСТами 34-й серии и РВ 15.203-2001. Эти стандарты определяют перечень этапов ОКР, последовательность и порядок их выполнения, формы отчетных документов и т.п. Необходимо отметить, что приведенные ГОСТы в части, касающейся разработки ПО, являются весьма устаревшими.  Действительно, за десятилетия, прошедшие с даты ввода их в действие, технология разработки ПО ушла далеко вперед. Так, классическая каскадная модель разработки ПО в последнее время признана неэффективной из-за недостаточной гибкости в угоду формальному управлению разработкой. Недостаточная гибкость процесса разработки обусловливает неспособность реагировать на изменения требований к системе и может в конечном итоге привести к невыполнению технического задания (ТЗ). В связи с этим возникает противоречие между технологическим уровнем разработки современных АСУ и устаревшими требованиями руководящих документов.  Разрешение указанного противоречия возможно путем интеграции современных гибких методов проектирования на этапах каскадной модели жизненного цикла разработки ПО. Полученные таким образом комбинированные модели представляют большой интерес и с научной точки зрения – для совершенствования методических подходов к разработке ПО, и с практической – для получения качественного ПО, соответствующего заданным требованиям.  Гибкие (agile) методы активно применяются в области разработки ПО в последние несколько лет. Они позволяют устранить ряд недостатков классического каскадного метода (метода водопада), заключающихся в недостаточной гибкости и формальном управлении проектами в ущерб срокам, стоимости и качеству. Недостаточная гибкость процесса разработки обусловливает неспособность реагировать на возникающие изменения требований к системе и может в конечном итоге привести к превышению бюджета, срыву сроков и невостребованности продукта. Причинами таких изменений могут быть корректировка требований по результатам демонстрации ПО заказчику, конечным пользователям или экспертам в автоматизируемой предметной области, ошибки, допущенные при формулировании требований ранее, изменения в самой предметной области и т.д. Ряд зарубежных авторов отмечают, что изменения функциональных требований к ПО в общем случае практически неизбежны и являются частью объективной реальности, с которой следует не бороться, препятствуя появлению изменений, а наоборот, приветствовать их даже на завершающих стадиях проекта, потому как скорректированные требования являются важнейшим условием создания действительно востребованного изделия, удовлетворяющего актуальные потребности заказчика. Принятие изменений требований даже в конце разработки является одним из принципов гибкой методологии, определенных в Agile Manifesto.  Внедрение гибких методов разработки ПО обеспечивает постепенную итерационную реализацию проекта с постоянной обратной связью от конечного пользователя. Проект развивается небольшими итерациями, в рамках каждой из которых в общем случае реализуется относительно небольшое приращение функционала системы, представляющее ценность для пользователя. Каждая итерация заканчивается выпуском новой версии ПО, которая демонстрируется пользователю (заказчику) с целью получения обратной связи. Объем программного кода, разработанного в рамках очередной итерации, оказывается относительно небольшим, что обеспечивает возможность его корректировки в рамках текущего проекта в случае вероятных изменений функциональных требований. В результате необходимые изменения вносятся в программный код своевременно.  Однако при управлении объемными проектами формализация имеет большую ценность, так как обеспечивает прозрачность процесса разработки. Известный американский стандарт по управлению требованиями Project Management Body of Knowledge (PMBOK) в своей третьей версии формально закреплял только методику каскадной модели, а в 2009 г. Институт управления проектами (Project Management Institute, PMI) предложил гибридную методологию, позволяющую сочетать достоинства классического метода водопада и современных гибких методов разработки.  Учитывая положительный мировой опыт использования гибких методов, задача их внедрения в области разработки систем специального назначения представляется актуальной.  Внедрение гибких методов в разработку ПО является комплексной и достаточно трудоемкой задачей. Она связана с изменением основных технологических и организационных процессов разработки ПО, способа взаимодействия с заказчиком, планирования и контроля разработки и т.д. Итерационная разработка задает новый ритм работы не только коллективу разработчиков, но и другим отделам организации и даже ее руководству. Нередко для внедрения гибких методов разработки в организацию приглашаются сертифицированные и дорогостоящие специалисты.  В НИИ «Центрпрограммсистем» при выполнении ряда ОКР была предпринята попытка использовать гибкие методы в рамках действующих нормативных документов и государственных стандартов разработки систем специального назначения.  Из известных гибких методов был выбран Scrum. Следование его основным принципам позволяет предоставлять конечному пользователю (заказчику) работающее ПО с новыми возможностями в фиксированные и относительно короткие итерации, называемые спринтами (sprints). При этом сбор требований, проектирование, разработка ПО и его демонстрация заказчику могут выполняться в каждом спринте. Однако в соответствии с ГОСТ РВ 15.203-2001 этапы ОКР в общем случае выполняются последовательно: этапу разработки рабочей конструкторской документации предшествуют этапы эскизного и технического проектирования, корректировка опытного образца выполняется после испытаний и т.д. Этапы выполнения ОКР зачастую длятся месяцами и даже годами. Кроме того, эффективная итерационная разработка ПО требует постоянного вовлечения экспертов в автоматизируемой предметной области в процесс разработки, а не только в моменты сдачи этапов, защиты эскизного и технического проектов и т.д. В связи с этим целесообразно сформировать группу экспертов в предметной области для обеспечения сбора требований, получения обратной связи и выполнения ряда других работ в рамках ОКР. Например, при разработке автоматизированных систем и программных комплексов военного назначения в группу экспертов могут войти должностные лица органов военного управления и специалисты организаций, выполняющих военно-научное сопровождение.  Важными аспектами внедрения гибких методов являются сбор и управление требованиями. Гибкая методология разработки ориентируется на динамическое формирование требований. Это дает заказчику право и возможность по результатам очередной итерации формулировать новые требования. Однако такой порядок формирования требований может привести к тому, что вновь сформированные требования будут противоречить уже реализованным и в некоторых случаях требовать решений, нарушающих архитектуру разработанного ранее ПО. Кроме того, динамическое формирование требований усложняет процесс управления ими, снижает предсказуемость процесса разработки и затрудняет планирование работы. Динамическое формирование требований может означать изменение объема работы, сроков проекта или его стоимости. Однако ОКР, как правило, выполняются по заранее согласованному и утвержденному ТЗ, а сроки и стоимость работы зачастую жестко определяются контрактными документами. При этом четко заданный в ТЗ перечень задач, решение которых должно обеспечить разрабатываемое ПО, ограничивает возможность изменения объема работы.  Способом управления объемом работы по реализации функциональных требований, предъявляемых к ПО специального назначения, является разработка постановок задач. Постановки задач позволяют конкретизировать и детализировать требования, описать порядок их реализации (порядок решения задач), используемые алгоритмы, форматы данных, определить формы входных и выходных документов и пр. Уточнение и конкретизация функциональных требований в постановках задач фактически позволяют варьировать трудоемкость выполнения задач ТЗ, а утверждение их заказчиком обеспечивает им юридическую силу.  Одним из современных подходов к формированию функциональных требований является разработка вариантов использования (прецедентов). Варианты использования утверждают поведение системы при обработке запросов пользователя (основного действующего лица) и в общем случае являются простыми текстовыми описаниями последовательности взаимодействия пользователя с системой. При необходимости в описание функциональных требований можно добавить диаграммы на формальных языках типа UML, BPMN, IDEF и других, поясняющие схемы, рисунки и прочие материалы. Однако, как показывает практика и отмечают некоторые авторы изданий на тему гибкой методологии и современных методов формирования требований, наибольшую ценность представляют именно текстовые описания прецедентов. Разработанные варианты использования целесообразно включить в соответствующие постановки задач в раздел «Порядок решения задачи». При необходимости следует изменить способ нумерации расширений описаний прецедентов. Классическим способом нумерации расширений является добавление к порядковому номеру расширяемого шага основного сценария прецедента литер «а», «б», «в» (a, b, c) и т.д. Например, основной сценарий прецедента содержит действие «3. Система выводит документ на печать». Поведение системы в случае сбоя печати описывается в расширениях прецедента, например: «3а. В принтере нет бумаги – система предлагает добавить бумагу в лоток и повторить попытку печати». Такой способ нумерации расширений прецедентов, приведенных в постановках задач, зачастую является непривычным для экспертов в предметной области и может быть заменен на более естественную формулировку в повествовательном стиле.  Трудоемкость разработки, последующего согласования и утверждения постановок задач кардинально сокращается в случае применения гибких методов разработки ПО.  При «классическом» выполнении ОКР постановки задач разрабатываются и утверждаются до начала непосредственной разработки ПО. Это зачастую затягивает разработку постановок и приводит к их «раздуванию», спровоцированному перепроектированием, обусловленным стремлением экспертов в предметной области включить в постановки задач побольше требований «на всякий случай». Однако по ходу развития проекта большая часть подобных требований может потерять свою актуальность, потребовать существенной корректировки, выйти за границы системы или вовсе оказаться надуманной.  В случае итерационного подхода актуальность сформулированных функциональных требований и корректность их реализации может быть определена в конце каждой итерации по результатам демонстрации разработанного ПО группе экспертов в предметной области. При этом трудоемкость разработки постановок задач, их согласования и утверждения заказчиком сокращается. Это происходит прежде всего потому, что постановки задач содержат уже реализованные функциональные требования. К моменту согласования постановок эксперты в предметной области и конечные пользователи уже имели возможность апробировать на практике реализацию сформулированных в них требований. В случае изменений требований постановки задач могут быть скорректированы и утверждены повторно даже на этапе разработки рабочей конструкторской документации и более поздних.  Наличие формально заданных в ТЗ функциональных требований к ПО, с одной стороны, и требований, определенных в виде описания прецедентов, с другой, обусловливают актуальность задачи обеспечения их трассировки и непротиворечивости. Разработанные описания прецедентов не должны противоречить формальным требованиям ТЗ. Напротив, они должны их уточнять, детализировать и конкретизировать. Кроме того, неэффективная трассировка требований в сочетании с недостаточным планированием ближе к завершению этапа разработки рабочей конструкторской документации может привести к авральной реализации формальных задач с низким приоритетом, отложенных «на потом».  Учитывая это, представляется целесообразной реализация дополнительного инструмента, позволяющего отслеживать соответствие реализуемых функциональных требований (прецедентов) формальным задачам, сформулированным в ТЗ. Это обеспечит трассировку требований и их непротиворечивость.  Как показала практика, таким инструментом может быть простая таблица, содержащая два столбца: в одном указываются задачи ТЗ, в другом – соответствующие им прецеденты. Она может быть дополнена столбцом с указанием названия соответствующей постановки или постановок задач.  Такая таблица обеспечивает трассировку требований от формальных задач, определенных в ТЗ,через соответствующие постановки к конкретным описаниям прецедентов. Трассировка позволяет «не забыть» про формальные требования ТЗ, увлекшись динамическим формированием бэклога проекта по результатам работы с экспертами в предметной области.  Указание постановки задач в таблице трассировки может быть особенно актуально в случае, когда на одну задачу из ТЗ разрабатываются две и более постановок. Оформление такой таблицы, за исключением столбца с перечнем прецедентов в виде формального документа, согласование и утверждение его у заказчика существенно упрощают задачу проверки реализации требований к математическому обеспечению на этапах испытаний опытного образца конечного изделия. Значительно упростить процесс разработки функциональных требований, повысить их организацию и обеспечить соответствие формальным задачам ТЗ позволяет формирование карт воздействий (Impact Mapping) и карт историй (story mapping).  Impact Mapping рассматривает функциональные требования в контексте бизнес-целей и делает акцент на тех изменениях деятельности конечных пользователей, которые должна обеспечить система для их достижения. Фрагмент упрощенной карты воздействий приведен на рисунке. Как видно из рисунка, основной целью автоматизации является обеспечение оперативности формирования и представления сведений по движению пациентов в медицинских организациях. Эта цель может быть достигнута изменением деятельности двух специалистов: специалиста медицинского управления и специалиста медицинской организации. Изменение деятельности первого заключается в повышении оперативности сбора данных и расчетов по движению пациентов в подчиненных медицинских организациях, а также в обеспечении автоматизированного контроля получаемых данных.  Повышение оперативности сбора данных и расчетов может быть обеспечено за счет автоматизированного сбора данных от подчиненных медицинских организаций и автоматизации расчета динамики движения пациентов. Аналогичным образом можно интерпретировать всю карту воздействий. Выстраивание логической цепочки от целей через пользователей системы до конкретных функциональных требований, описываемых в виде пользовательских историй, прецедентов или вариантов использования, обеспечивает визуализацию решения актуальной задачи их трассировки. Другим хорошо зарекомендовавшим себя на практике подходом к формированию функцио нальных требований является составление карты историй (story map). Такой подход позволяет в простой табличной форме представить сценарий применения системы и отобразить в нем место функциональных требований в виде вариантов использования или прецедентов.  Строка таблицы «Сценарий применения системы» описывает последовательность действий пользователей – специалиста медицинской организации и управления с использованием разрабатываемой системы. Последовательность состоит из отдельных шагов, расположенных слева направо и конкретизируемых в описаниях соответствующих прецедентов. В строке таблицы «Скелет системы» отображаются функциональные требования, реализация которых позволит сформировать скелет действующей системы. Скелетная реализация не позволяет выполнить все запланированные функциональные требования и формальные задачи ТЗ в полном объеме, но она обеспечивает выполнение прогона системы по сценарию применения от начала до конца, хотя и с ограничениями. Указание формальной задачи (или цели) вверху таблицы дает возможность упростить и визуализировать трассировку требований.  Следующими важными аспектами разработки ПО специального назначения являются планирование и отслеживание выполнения работы. Одним из недостатков гибких методов является недостаточная формализация планирования, снижающая предсказуемость выполнения проекта, в то время как действующие стандарты выполнения ОКР, напротив, предписывают разрабатывать планирующие документы, например, единый сквозной план создания изделия, сетевой план-график и т.д. Помимо формальных требований стандартов, эксперты в предметной области и их руководство зачастую требуют разработки и согласования планирующих документов с целью организации своей деятельности.  Упростить задачу планирования гибкой разработки позволяет небольшая фиксированная длительность итераций. Следуя принципам метода Scrum, к реализации в каждом новом спринте выбираются функциональные возможности, для которых определен наибольший приоритет. Требования для реализации в очередном спринте выбираются в его начале и в общем случае не меняются до его завершения. При этом фиксированная длительность спринта обеспечивает ему некоторую предсказуемость и упрощает процесс планирования.  При планировании работ следует учитывать, что сбор требований и проектирование их реализации в общем случае должны опережать разработку соответствующего ПО. Это позволяет избежать простоев в работе программистов. Целесообразная длительность такого опережения составляет одну-две итерации.  С целью поддержки планирования разработки ПО специального назначения, помимо формальных планирующих документов, предусмотренных действующими государственными стандартами, может быть применен адаптированный вариант бэклога проекта, предусмотренного методом Scrum.  В первом столбце таблицы указываются функциональные требования. Их приоритет оценивается в баллах от одного до трех и указывается во втором столбце. Сложность реализации каждого функционального требования оценивается аналогично и указывается в третьем столбце таблицы. Далее приводится ссылка на прототип графического интерфейса, разработанного дизайнером (проектировщиком). Знак в пятом столбце означает наличие приемочных тестов, подготовленных группой тестировщиков. Столбец «Результат тестирования» позволяет уяснить наличие и общий характер выявленных дефектов (ошибок). Например, условное обозначение, закрашенное красным цветом, означает, что ошибки были выявлены при проведении приемочных тестов вручную, а – при выполнении автоматического приемочного тестирования. Кроме того, в этом столбце можно указывать и другие условные обозначения результатов тестирования, по тем или иным причинам представляющие интерес для руководителя проекта.  Функциональные требования, запланированные к реализации в одной и той же итерации, отображаются в соседних строках таблицы. Группа требований одной итерации отчеркивается двойной линией от остальной части бэклога. Сверху указываются номер итерации, даты ее начала и окончания и версия выпуска (релиза) ПО, которое будет продемонстрировано заказчику по завершении итерации. Итерации располагаются в бэклоге проекта в хронологическом порядке сверху вниз.  Приведенная реализация бэклога проекта не просто содержит перечень функциональных требований, но и позволяет планировать их выполнение и отслеживать результаты. Бэклог проекта обобщенно отображает результаты работы системных аналитиков, программистов и тестировщиков. Непротиворечивость используемого бэклога проекта формальным планирующим документам обеспечивается за счет учета и отображения в нем формальных сроков этапов и работ выполняемой ОКР.  Еще одним решением, позволяющим упростить планирование гибкой разработки, является приоритизация требований.  Некоторые зарубежные специалисты в области гибких методов советуют оценивать приоритеты функциональных требований в баллах, например, от одного до трех по мере возрастания приоритета. Единых подходов к определению приоритета функциональных требований нет, приоритеты оцениваются умозрительно, исходя из личных представлений и опыта того или иного привлеченного эксперта. Применение упомянутых ранее Impact Mapping и карт историй позволяет облегчить процесс приоритизации требований за счет визуализации их влияния на достижение поставленных бизнес-целей и сценарий применения системы соответственно. Кроме того, при необходимости бизнес-цели, достижение которых обеспечивается реализацией соответствующих функциональных требований, могут быть отображены в виде вех или приведены как дополнительные описания итераций в бэклоге проекта. Аналогичным образом в бэклоге могут быть отображены формальные задачи, определенные в ТЗ на ОКР.  Метод гибкой разработки Scrum предлагает отслеживать процесс выполнения работы при помощи диаграмм сгорания задач (Burndown chart). Такая диаграмма графически отображает уже вложенные трудозатраты и трудоемкость оставшейся работы и может быть составлена на одну или несколько итераций. По мере выполнения работы кривая оставшихся трудозатрат, отображаемая на графике, будет стремиться к нулю. Однако, учитывая, что перечень функциональных требований формируется динамически, уже реализованные требования могут корректироваться заказчиком по итогам демонстрации очередной версии ПО, скорость работы команды разработчиков меняется от итерации к итерации, диаграмма сгорания задач на практике позволяет сделать лишь очень приблизительный прогноз сроков завершения работы. Скорее, она может быть использована для отслеживания расчетного запаздывания реализации проекта. Более точные оценки процесса разработки ПО могут быть получены при совокупном использовании бэклога, карт историй и способа Impact Mapping. При этом целесообразно сосредоточиться на достижении заранее спланированных контрольных точек, приведенных в планирующих документах.  Внедрение гибких методов в процесс разработки ПО специального назначения, безусловно, является трудоемким и обладает множеством особенностей, не рассмотренных в данной статье. Однако ряд положительных результатов, полученных НИИ «Центрпрограммсистем» при сочетании гибких методов разработки ПО с классической каскадной моделью выполнения ОКР в специальных областях, позволяет сделать вывод о возможности и актуальности применения принципов гибкой методологии в рамках действующих нормативных документов и государственных стандартов. Способы применения гибких методов, рассмотренные в данной статье, представляют собой решения, позволяющие организовать процесс разработки ПО, сочетающий преимущества современных гибких методов и классической каскадной модели жизненного цикла ПО. Гибридный процесс разработки способствует разрешению противоречия между технологическим уровнем разработки современных АСУ и устаревшими требованиями руководящих документов.
Большое разнообразие видов информации и способов ее представления, тенденция к росту сложности процессов обработки и в то же время стремление к унификации и повышению технологичности этих процессов, желание снизить стоимость хранения и обработки информации – эти и многие другие факторы все чаще приводят к необходимости создания и использования распределенных информационных систем (ИС) различного рода. Для объединения отдельных компонентов таких ИС используются разнообразные каналы связи (в зависимости от масштабов и открытости системы это могут быть локальные вычислительные сети (ЛВС), открытые и закрытые выделенные каналы, общие каналы – Интернет), поэтому возникает возможность влияния на работу ИС или ее данные с помощью различного рода сетевых механизмов, то есть возникает угроза сетевых вторжений. В связи с этим для ИС все более важным становится вопрос обнаружения и (по возможности) предотвращения таких вторжений. Это особенно актуально, если ИС имеет дело с такими видами информации, как персональные данные, коммерческая и государственная тайна. Для решения данного вопроса в рамках системы защиты информации ИС используются различные системы обнаружения вторжений (СОВ). При этом обнаружение сетевых вторжений может осуществляться различными способами: на основании первичных данных – путем анализа (сигнатурного, эвристического, статистического) сетевого трафика – для выявления непосредственно вторжения, и на основании вторичных данных – путем контроля поведения сетевых процессов на узлах ИС и анализа аудита узлов ИС – для выявления следов или последствий вторжения . На текущий момент практически все новые ИС разрабатываются с учетом необходимости обнаружения вторжений, однако существует множество ранее созданных систем, при разработке которых этому вопросу не уделялось достаточно внимания либо разработка проводилась вообще без его учета. Одному из аспектов внедрения СОВ в такие ИС и посвящена статья. Прежде всего обозначим условия, в рамках которых рассматривается проблема:  в качестве контролируемого объекта рассматриваем ЛВС, объединяющую ЭВМ, – узлы ИС;  к ЛВС могут быть подключены один или более внешних каналов связи;  в ЛВС не должны использоваться средства преобразования сетевых адресов – NAT и т.п.; должна обеспечиваться однозначная идентификация каждого узла сети с любого другого ее узла;  на контролируемом объекте отсутствует система обнаружения вторжений. Как видим, подобные условия характеризуют большое число уже имеющихся ИС. Учитывая существующую для таких условий угрозу сетевых вторжений, необходимо обеспечить обнаружение этих вторжений, минимизировав при этом затраты на внедрение средств обнаружения и влияние на функционирование исходной системы. При этом предполагается возможность воздействия на контролируемый объект со стороны как внешнего, так и внутреннего злоумышленника; под внешним в данном случае будем понимать злоумышленника, воздействующего на ИС извне, через внешние каналы связи; под внутренним – злоумышленника, воздействующего на ИС (либо на внешние системы) с одного или нескольких узлов самой ИС. Эффективное обнаружение сетевых вторжений – комплексный процесс, осуществляемый на различных уровнях (контролируемый объект в целом, ЛВС, сегмент ЛВС, отдельный узел сети) и различными способами (сигнатурный, эвристический, статистический анализ). В данной статье предметом рассмотрения является статистический анализ сетевого трафика (в частности, формирование такой статистики) с целью обнаружения таких типов вторжений, как DoS/DDoS-атаки, сканирование портов и узлов сети. Статистику сетевого трафика будем рассматривать на уровне потоков; под потоком в данном случае понимаем однонаправленную последовательность IP-пакетов, проходящих за определенный период времени в точке наблюдения и обладающих общим набором свойств (ключей потока – flow keys) . Обычно в качестве ключей потока выступает следующий набор данных:  транспортный протокол;  IP-адрес и сетевой порт отправителя;  IP-адрес и сетевой порт получателя. Указанный в условиях рассмотрения проблемы запрет на использование средств преобразования сетевых адресов связан как раз с тем, что при их наличии набор ключей одного и того же потока будет отличаться в различных частях сети (например, перед сервером NAT и за сервером NAT ). DoS/DDoS-атаки, сканирование портов и узлов сети характеризуются тем, что атакующим за небольшой период времени выполняется множество соединений с атакуемым узлом (узлами). Таким образом, во время вторжения происходит резкое увеличение числа активных потоков , что позволяет выявлять указанные типы атак с помощью простейшего анализа количества активных потоков. Существуют два подхода по сбору статистики сетевого трафика для обнаружения вторжений:  сбор в ключевых точках сети;  сбор непосредственно на узлах сети. Сбор статистики в ключевых точках сети подразумевает использование специфичных аппаратных средств (интеллектуального коммутационного оборудования, аппаратных сенсоров сетевого трафика и т.п.) или выделенных ЭВМ (шлюзов, межсетевых экранов). В этом случае классическими средствами для учета сетевого трафика и получения статистики по нему являются протоколы NetFlow , IPFIX , sFlow  и тому подобные протоколы, оперирующие данными сетевого и транспортного уровней базовой эталонной модели взаимодействия открытых систем (ГОСТ Р ИСО/МЭК 7498-1-99). Их использование предполагает наличие сенсоров, непосредственно собирающих статистику (обычно в этом качестве выступает L3-коммутатор или маршрутизатор, но существуют и соответствующие специализированные устройства); полученные сенсорами данные собираются и сохраняются коллектором и впоследствии используются анализатором для оценки и формирования отчетов. Стоит отметить, что описанное выше оборудование для сбора статистики достаточно редко изначально закладывается в ИС (по причине его дороговизны, из-за отсутствия требований по обнаружению вторжений на момент проектирования ИС и т.п.). Замена уже используемого в ИС коммутационного оборудования на интеллектуальное и добавление аппаратных сенсоров зачастую весьма проблематичны по причинам экономического, технического и организационного характера. Помимо вышесказанного, данный подход является весьма зависимым от топологии сети. Рассмотрим простейший вариант ЛВС, в котором сеть формируется единственным сетевым коммутатором (выполняющим также роль сенсора), к которому подключены все узлы сети, а также внешний канал связи. Через этот коммутатор проходит весь трафик, как внутрисетевой, так и внешний. Очевидно, что в этом случае собираемая коммутатором статистика будет полной для данной сети. Если же рассмотреть более сложный вариант ЛВС, содержащий хотя бы два коммутатора, то легко увидеть, что в этом случае часть трафика будет проходить через один коммутатор и учитываться однократно, а другая часть – через два или более коммутаторов и учитываться несколько раз (пример потоков в такой ЛВС изображен на рисунке). Как следствие, в таких случаях для формирования полной (суммарной) статистики по сети приходится на коллекторе выполнять операцию дедупликации собираемых данных, при которой полученные из нескольких источников сведения об одном и том же трафике объединяются, исключая дублирование. Данная операция весьма ресурсоемка, что приводит к необходимости использования в качестве коллектора высокопроизводительного оборудования (например, в системных требованиях к программной версии коллектора SteelCentral Flow Gateway Virtual Edition фирмы Riverbed рекомендуется использование 4 центральных процессоров и (в зависимости от конфигурации) от 4 до 32 Гб ОЗУ ). Это приводит к значительному росту стоимости внедрения системы обнаружения вторжений. Альтернативой описанному подходу является сбор статистики непосредственно на узлах сети. В этом случае он обычно осуществляется с помощью ПО, установленного на все узлы сети контролируемого объекта. Такая схема позволяет собирать статистику сетевого трафика без использования интеллектуального сетевого оборудования и аппаратных сенсоров, что положительно влияет на стоимость внедрения системы обнаружения вторжений. Кроме того, поскольку такая схема функционирует исключительно на уровне оконечных точек (узлов) сети, она фактически не зависит от конкретной топологии сети, то есть количества и взаимосвязей коммутационного оборудования (в рамках указанных в статье условий к контролируемому объекту). Еще одним доводом в пользу данного подхода является необходимость собирать информацию о сетевом трафике, проходящем через контролируемые узлы ИС, указанная в требованиях ФСТЭК России от 06.03.2012 г. к системам обнаружения вторжений уровня узла начиная уже с минимального, шестого, класса защиты. В качестве входных данных для анализа с целью выявления рассматриваемых типов вторжений (DoS/DDoS-атаки, сканирование портов и узлов сети) выступает количество активных потоков за единицу времени. Сбор такой статистики (заключающийся в выделении отдельных потоков и подсчете их количества) является тривиальной, требующей минимальных ресурсов задачей и, следовательно, минимально влияет на производительность контролируемых узлов сети. Необходимо отметить, что анализ статистики для одного узла сети позволяет обнаруживать только те виды вторжений (из рассматриваемых в данной статье), которые направлены на конкретный узел сети. Сканирование узлов сети (направленное обычно на выявление состава узлов и топологии сети и являющееся источником данных для организации последующих вторжений на конкретные узлы сети) таким образом не обнаружить. Тем не менее, если сформировать суммарную статистику по сети, можно выявить аномальное количество активных потоков с одного (внутреннего или внешнего по отношению к сети) узла ко множеству других узлов сети; таким образом появляется возможность обнаружения и этого типа вторжений. Однако, если на каждом узле сети контролировать и входящий, и исходящий трафики, то при формировании суммарной статистики, как и в случае сбора статистики в ключевых точках сети, возникает необходимость дедупликации собираемых данных, так как один и тот же поток может фиксироваться один раз, если идет обмен с внешними по отношению к сети узлами и поток фиксируется на одном узле сети, либо два раза, если идет внутрисетевой обмен в режиме «точка-точка» и поток фиксируется как у отправителя, так и у получателя; широковещательные либо групповые (multicast) рассылки при этом стоит рассматривать как множество независимых потоков. Соответственно, как и для коллектора при сборе в ключевых точках сети, для формирования суммарной статистики потребуется высокопроизводительное оборудование. Далее для приведения рассматриваемой схемы к более однородному виду каждый внешний канал связи, имеющийся в данной сети, представим в виде специального, «канального», узла сети, контроль трафика которого может осуществляться на шлюзе или на межсетевом экране этого канала. Чтобы исключить такую ресурсоемкую операцию, как дедупликация собираемых данных, и снизить требования к оборудованию для формирования суммарной статистики, предлагается для сбора статистики на узлах сети контролировать трафик, проходящий только в одном направлении (то есть только входящий или только исходящий). При условии, что таким образом будет контролироваться каждый узел сети (как реальный, так и «канальный»), при сборе статистики будут учтены все возможные потоки – как внутрисетевые, так и с внешними узлами. В результате того, что на каждом узле контролируется только одно направление трафика, любой поток, как внутрисетевой, так и с внешними узлами, оказывается учтенным только один раз – либо на узле получателя, либо на «канальном» узле. Таким образом, множества записей о потоках на отдельных узлах Fi оказываются непересекающимися и формирование суммарного множества записей о потоках всей сети Fsum выполняется путем объединения всех этих множеств. Соответственно, ключевой показатель – суммарное количество активных потоков для сети – может быть вычислен как сумма количества активных потоков для всех узлов либо как мощность множества Fsum в случае, если с узлов сети для получения статистики предоставляются только записи о потоках. Последующий анализ суммарных статистических данных оказывается полностью аналогичным анализу статистических данных для отдельных узлов, то есть нетребовательным к вычислительным ресурсам. Рассмотрим, как влияет выбор контролируемого направления трафика на возможность и достоверность выявления различных типов вторжений. Контроль исходящего трафика позволяет локально (то есть непосредственно на узле) обнаруживать все рассматриваемые типы вторжений, если они осуществляются извне (в этом случае обнаружение происходит при контроле внешнего канала связи – например, на шлюзе или межсетевом экране) либо с одного из контролируемых узлов сети (вторжение обнаруживается на узле – источнике вторжения). Исключениями являются внутренние, выполняемые с нескольких узлов сети DDoS-атаки, которые могут быть выявлены только с помощью суммарной статистики по сети, а также вторжения любого типа, осуществляемые с нелегально подключенных злоумышленником к сети узлов, на которых, очевидно, отсутствуют рассматриваемые средства контроля. Стоит отметить также, что для легальных узлов сети, с которых осуществляет вторжение злоумышленник, вполне закономерны сомнения в подлинности выдаваемых средствами контроля трафика сведений; более того, в результате вмешательства злоумышленника в работу узла эти средства вообще могут оказаться отключенными. Контроль входящего трафика узла, в свою очередь, позволяет локально обнаруживать вторжения, направленные на данный узел, независимо от расположения источника вторжения. При этом вторжения, ориентированные на группу узлов (сканирование узлов), не могут быть выявлены локально; однако использование суммарной статистики по сети позволит обнаруживать и такой тип вторжений. При контроле входящего трафика легальность или нелегальность присутствия в сети узла – источника вторжения не играет роли. Учитывая вышесказанное, можно сделать вывод, что контроль входящего трафика узла обладает более высокой степенью доверия к анализируемым данными и обеспечивает более эффективное обнаружение вторжений, чем контроль исходящего трафика. Использование мониторинга сетевого трафика непосредственно на узлах сети и анализ статистики только входящего трафика узлов позволяет осуществлять выявление вторжений в локальных вычислительных сетях со стороны как внешних, так и внутренних нарушителей, в том числе нелегально подключенных к контролируемой сети. При этом предложенный подход не требует больших вычислительных ресурсов ни от узлов сети, ни от вспомогательного оборудования (коллектор и анализатор суммарной статистики сети). Кроме того, возможность применения данного подхода практически не зависит от топологии и внутренней структуры контролируемой сети, что значительно упрощает его внедрение в уже существующие информационные системы. Ограничением на использование данного подхода является требование возможности установки программных сенсоров на все узлы сети.
Задачи обеспечения заданной боевой готовности ВМФ можно успешно решить только при эффективном функционировании системы военно-морского образования и системы боевой подготовки. В свою очередь, эффективность образования и боевой подготовки зависит от наличия учебно-тренировочных средств (УТС), соответствующих современному состоянию ВМФ . В своем развитии УТС прошли несколько этапов. Наиболее значимый этап связан с внедрением современных информационных технологий . НИИ «Центрпрограммсистем» (г. Тверь) разрабатывает УТС для ВМФ более 25 лет. Это одно из ключевых направлений деятельности предприятия, которое прошло путь от разработки первой автоматизированной системы обучения (АСО) ВМФ с использованием первых локальных вычислительных сетей персональных компьютеров (АСО 101, 1992 г.) до создания тактических тренажерных комплексов, предназначенных для подготовки экипажей кораблей ВМФ (2013 г.). Общие технические решения и применяемые технологии:  использование средств вычислительной техники Intel общепромышленного исполнения и локальных вычислительных сетей, в том числе в защищенном исполнении, с применением интерфейсов Ethеrnet;  кроссплатформенная программная реализация ;  защита от несанкционированного доступа;  использование моделирующих систем и технологии распределенного моделирования;  практически неограниченные возможности по модернизации за счет как разработки и внедрения нового ПО, так и замены аппаратной части на более современные программно совместимые средства;  унификация программно-технических решений, используемых при разработке УТС, позволяющая полностью или частично включать их в состав других изделий;  использование унифицированного аппаратно-программного комплекса руководства обучением и комплексирования (АПК РОК) ;  использование инструментальных программных средств собственной разработки, которые постоянно совершенствуются, а их перечень пополняется новыми средствами. Работа предприятия по созданию УТС для ВМФ ведется по четырем основным направлениям: АСО, специализированные тренажеры, комплексные тренажеры, тренажерные комплексы. Автоматизированные системы обучения Данные системы предназначены для обеспечения процесса группового теоретического обучения под руководством преподавателя . В общем случае АСО – это среда, в которой автоматизированные учебные занятия реализуются как процесс функционирования единой системы, объединяющей обучающего, обучающихся и средства автоматизации. Обеспечиваются создание занятий любой тематики и различных видов (теоретические, практические, контрольные), их проведение, изменение плана проведения занятия и дополнение их новым учебным материалом. Средства поддержки проведения занятий позволяют преподавателю обеспечить индивидуальный подход к обучающемуся и оперативно воздействовать на процесс обучения.В основу разработки АСО положен подход, предполагающий проведение автоматизированных учебных занятий на базе компьютерных обучающих программ, которые представляют собой набор информационных файлов, обеспечивающих в процессе занятий выдачу учебной информации, интерактивное управление ею, а также контроль (входной, промежуточный, выходной) в соответствии с алгоритмом, заданным разработчиком сценария компьютерных обучающих программ. Разработчиком сценария являлся специалист в конкретной предметной области с достаточным педагогическим опытом (как правило, преподаватель учебного заведения). В общем виде специальное ПО (СПО) АСО включает комплексы программ инструментальных средств разработки компьютерных обучающих программ, поддержки проведения автоматизированных учебных занятий, поддержки учебного процесса . В состав АСО включен набор опорных (базовых) компьютерных обучающих программ, на которые могут ориентироваться пользователи при разработке средствами АСО собственных компьютерных обучающих программ. С помощью инструментальных средств предприятием создана база компьютерных обучающих программ различной тематики, состоящая на данный момент из нескольких тысяч компьютерных обучающих программ, которые обеспечивают информационную поддержку теоретического обучения практически по всем военно-учетным специальностям ВМФ. Работы по созданию компьютерных обучающих программ по заказу МО РФ продолжаются. В НИИ «Центрпрограммсистем» разработано несколько поколений АСО, которые приняты на снабжение ВС РФ и широко используются при подготовке специалистов ВМФ. К ним относятся АСО-101 (1993 г., поставлено 62 серийных образца), «Обзор-АСО» (2012 г., поставлено 13 образцов) и АСО МСК (2014 г., поставлен 1 образец). Основные технические решения:  технология единой информационно-дидактической среды теоретического обучения;  технология создания электронного образовательного ресурса (компьютерные обучающие программы, электронные учебники, интерактивная электронная документация) с использованием оригинальных инструментальных программных средств, рассчитанная на широкий круг пользователей;  сертификация программных комплексов для разработки и использования информации ограниченного доступа;  наличие специального редактора тестов для разработки контрольных вопросов, обеспечивающего возможность проведения входного, промежуточного и итогового контроля;  применение специальных шаблонов и библиотеки учебно-информационных фрагментов, позволяющих снизить трудоемкость разработки компьютерных обучающих программ;  поддержка всех распространенных форматов данных и форм представления информации, в том числе 3D-графики, перечень которых постоянно расширяется;  возможность подключения внешних программ (например, отдельных частей программных имитаторов пультовых приборов образцов военного вооружения и специальной техники (ВВСТ) для предтренажерной подготовки);  открытость системы как для конечного пользователя, так и для модернизации, имеющей гибкую настройку, что позволяет потребителю использовать нужную комплектацию. Специализированные тренажеры Данные тренажеры предназначены для практической подготовки к эксплуатации образцов ВВСТ, то есть к техническому обслуживанию, ремонту, борьбе за живучесть технических средств и решению функциональных задач управления. Основной целью практической подготовки является отработка, в первую очередь, моторных навыков операторов, поэтому АРМ обучающихся (АРМО) в специализированных тренажерах основаны на использовании корабельного оборудования или, как правило, аппаратно-программных имитаторов (действующих макетов) пультовых приборов образцов ВВСТ, достоверно воспроизводящих эргономику штатного оборудования. Для обеспечения подготовки по техническому обслуживанию в состав специализированных тренажеров входят имитаторы инженерных панелей и пультов, выполненные с использованием или действующих макетов, или программных комплексов в виде программных имитаторов. Для подготовки к использованию отдельных образцов ВВСТ, особо сложных для имитации (моделирования) в обеспечение выработки умений и навыков по их эксплуатации, допускается применение специализированных тренажеров, обеспечивающих отработку только операторских навыков по решению функциональных задач управления. В этом случае подготовка по техническому обслуживанию и другим вопросам выполняется в режиме теоретического обучения (режим АСО) с помощью программных имитаторов инженерно-технических панелей, пультов и с имитацией режимов автоматических систем технической диагностики (АСТД). Специализированные тренажеры обеспечивают быструю адаптацию обучающихся к реальным образцам ВВСТ за счет высокой степени достоверности моделирования физических сред, имитаторов пультовых приборов образцов ВВСТ и реальных действий при их использовании. В последние годы НИИ «Центрпрограммсистем» является основным российским производителем специализированных тренажеров радиоэлектронных средств подводных лодок ВМФ. Линейка разработанных специализированных тренажеров включает специализированные тренажеры радиолокационных комплексов дизельных и атомных подводных лодок 4-го поколения. Основные технические решения:  технология создания аппаратно-программных имитаторов образца ВВСТ, которые включают макет(ы) пульта(ов) имитируемого изделия, устройство сопряжения с оборудованием и управляющую ПЭВМ; устройство сопряжения обеспечивает обмен данными по протоколу ТСР/IP с управляющей ПЭВМ о положениях органов управления и состоянии индикаторов; в качестве управляющих программ используется доработанный в части обмена данными с устройством сопряжения программный имитатор;  использование программных имитаторов и программных комплексов «подыгрыша» для имитации функционирования штатных периферийных приборов и взаимодействующих образцов ВВСТ;  имитация работы встроенных систем технической диагностики, инженерных панелей, технологических пультов, моделирование включения диагностических тестов, позволяющих контролировать параметры и обнаруживать неисправности;  использование ПО АСО для обеспечения теоретической подготовки. Комплексные тренажеры Подобные тренажеры предназначены для подготовки расчетов боевых постов и командных пунктов различных уровней по решению тактических и оперативных задач. Отличительным признаком комплексных тренажеров является их направленность на отработку взаимодействия и слаженности между специалистами в составе командных пунктов, боевых и корабельных боевых расчетов по вопросам решения тактических задач, использования оружия и образцов ВВСТ, а также решения задач по борьбе за живучесть корабля и его технических средств. Для отработки операторских (без сенсомоторных) навыков обучающихся применяются программные имитаторы пультовых приборов образцов ВВСТ, а также стилизованные интерфейсы для отработки навыков функциональной деятельности. Комплексные тренажеры могут быть универсальными. Направленность универсальных тренажеров – подготовка по образцам ВВСТ одинакового функционального назначения, по общим видам деятельности в составе боевого расчета, в том числе командира боевого расчета, командного пункта (ГКП, ФКП, БИЦ и др.), дежурной смены. Универсальность достигается перестройкой имитационных моделей, сменой лицевых панелей АРМО, возможностью программными методами, исходя из учебных целей, конфигурировать состав и различные сочетания АРМО . За последние годы НИИ ЦПС разработало для МО РФ несколько комплексных тренажеров, три из которых приняты на снабжение. Их серийные образцы поставляются в учебные классы, центры и заведения МО, к ним относятся комплексные тренажеры боевого использования РТС «Обзор» (2012 г., поставлено 13 образцов), радиотехнического подразделения «Тест» (2013 г., поставлено 14 образцов), а также тактический тренажер ПВО «Охта» (2012 г., поставлено 7 образцов). Основные технические решения:  технология создания программных имитаторов образцов ВВСТ, в которых органы управления и индикации имитируются на дисплеях компьютеров, при этом использование органов управления осуществляется с помощью стандартных манипуляторов или сенсорных мониторов; предприятие разработало большое количество программных имитаторов образцов ВВСТ надводных кораблей, подводных лодок и морской авиации ВМФ, используемых в различных тренажерах и тренажерных комплексах ;  технология создания моделей радиоэлектронных средств наблюдения на основе универсального имитатора функционирования, который позволяет достаточно легко настраивать модель под конкретные характеристики реальных прототипов;  технология эмуляции программно-технического окружения для обеспечения использования штатного функционального ПО сложных образцов ВВСТ (например боевые информационно-управляющие системы), что позволяет значительно снизить материальные и временные затраты на разработку имитаторов данных средств и обеспечить их полную адекватность;  возможность создания замкнутых контуров управления с «подыгрышем» за отсутствующие источники и/или потребители. Тренажерные комплексы Построение единой информационно-моделирующей среды и обеспечение единого управления на основе архитектуры распределенного моделирования – главная проблема при интеграции в тренажерные комплексы УТС различного функционального назначения и различных производителей. В «Центрпрограммсистем» разработана концепция построения архитектуры распределенного моделирования и создания единого информаци-онномоделирующего пространства, которые обеспечивают интеграционные процессы, направленные на объединение специализированных и тактических тренажеров, созданных различными производителями, в тренажерные комплексы (системы), функционирующие под единым управлением на единой тактической обстановке. Системообразующим элементом тренажерных комплексов является АПК РОК , который обеспечивает  управление конфигурацией комплекса, составом и размещением рабочих мест обучающихся и преподавателей в зависимости от цели учебного мероприятия;  проведение учебных мероприятий в едином информационно-моделирующем пространстве, в едином масштабе времени и под единым руководством;  интеграцию отдельных тренажеров в комплекс и взаимодействие комплекса и его составных частей с внешними УТС;  мониторинг состояния технических средств комплекса при подготовке и в процессе проведения учебного мероприятия. АПК РОК включает три подсистемы: моделирования, руководства обучением, комплексирования. Подсистема моделирования предназначена для имитационного моделирования объектов тактической обстановки, процессов и явлений, функционирования технических средств корабля и развития аварийных ситуаций в соответствии с исходной обстановкой и действиями обучающихся, а также для обеспечения управления и документирования учебного мероприятия. При моделировании тактической обстановки осуществляется моделирование  земной поверхности с использованием картографической информации, включая очертания береговой черты, глубину, рельеф местности, грунт, температуру и соленость морской воды;  гидрометеорологических условий, включая облачность, волнение моря, ледовую обстановку, течение и ветер;  радиолокационной среды с учетом облачности, осадков, состояния морской поверхности и прочих гидрометеорологических условий;  гидроакустической среды с учетом заданных гидрометеорологических и гидроакустических условий, биологических шумов и шумов дальнего судоходства;  радиотехнической обстановки с учетом излучающих средств наблюдения, средств связи и радиоэлектронной борьбы;  взаимодействующих и противодействующих сил, алгоритмов их поведения с возможностью использования средств обнаружения, применения средств радиоэлектронной борьбы и оружия в соответствии с их моделями и тактико-техническими характеристиками. Подсистема руководства обучением предназначена для формирования задания на учебное мероприятие, управления ходом мероприятия и тактической обстановкой, ручного и автоматического управления взаимодействующими и противодействующими силами, а также при проведении разбора демонстрации развития тактической ситуации и действий обучающихся в реальном, замедленном или ускоренном времени. Подсистема комплексирования тренажеров предназначена для проведения тренировок на нескольких тренажерах одновременно на базе единой тактической обстановки и под единым управлением. Основные технические решения и технологии:  универсальный вычислительно-моделирующий комплекс (реализует сервер тактической обстановки, обеспечивает доступ к тактической обстановке по универсальному протоколу, управление и документирование);  универсальный пост руководства обучением (обеспечивает создание заданий на тренировки, их проведение и разбор);  универсальный редактор БД моделей (позволяет создавать и корректировать БД тактико-технических характеристик объектов и их образцов ВВСТ, параметров среды и др.);  модульный принцип построения системы моделирования с возможностью расширения номенклатуры моделей, их адаптации к конкретным учебным задачам;  организация моделей в единую иерархическую структуру с произвольным числом уровней; вершиной иерархии является модель тактической обстановки, в которую в качестве дочерних элементов включаются модели объектов и групповых формирований; иерархическая структура моделей обеспечивает механизм построения сложных моделей объектов с использованием набора более простых моделей, при этом количество уровней структуры определяет глубину моделирования;  множество готовых базовых компонентов для реализации как отдельных моделей, так и целых рабочих мест;  гибкий настраиваемый современный пользовательский интерфейс программ подсистемы руководства обучением;  возможность программного задания не только различных вариантов строев, ордеров, боевых и походных порядков объектов тактической обстановки, движения их по различным типам траекторий и профилям полетов, но и различных алгоритмов развития тактической обстановки ;  система автоматизированного управления объектами обстановки на основе планов, созданных пользователем из команд, и широкого спектра готовых алгоритмов тактических действий ;  эффективная система записи и воспроизведения проведенных тренировок (с точки зрения объема сохраняемых данных и быстродействия);  гибкая система протоколирования различных событий тренировки с возможностью включения в протоколы изображений и звуковых файлов, сформированных на различных УТС комплекса;  минимальные требования к комплексированию УТС, возможность различных вариантов комплексирования (с помощью сетевого протокола либо готового программного клиента);  системы диагностирования программных сбоев и автосохранения, обеспечивающие надежность проведения длительных тренировок;  использование открытых протоколов и форматов обмена данными (xml, json);  максимальная унификация, модульность и масштабируемость системных и прикладных программных средств, а также использование унифицированных программных интерфейсов;  создание АПК РОК как унифицированного комплекса, предназначенного для использования как в комплексах (системах), так и в отдельных УТС с целью обеспечения их автономной работы. Концепция использования АПК РОК впервые была реализована в 2013 г. при создании НИИ «Центрпрограммсистем» тактического тренажерного комплекса Учебного центра ВМФ (г. Обнинск). В дальнейшем данная технология прошла апробацию и нашла применение и развитие в работах по созданию новых тренажерных комплексов и систем.
В настоящее время невозможно представить подготовку военнослужащих ВМФ без компьютерных тренажеров со специальными обучающими программами. Принято считать, что полный набор компьютерных обучающих программ (КОП) по одной теме представляет курс обучения. Обучение в период подготовки можно подразделить на следующие этапы: освоение и систематизация знаний, овладение умениями, развитие умений в навыки, приобретение опыта и его развитие . При обучении с помощью компьютерной техники используются уже общепринятые виды занятий: лекция, практические и контрольные занятия, самостоятельная подготовка. При разработке КОП учитываются характерные для каждого вида занятий методические и дидактические особенности их построения . Занятие с применением компьютерной техники, как и любое учебное занятие, должно иметь свою структуру и сценарий. Структура всех учебных занятий неизменна и состоит из трех частей: вводной, основной, заключительной. Согласно документам Министерства образования, любое учебное занятие должно иметь структуру, представленную в таблице 1.   Современные КОП разрабатываются на основании сценариев. Сценарий КОП должен содержать достаточно полное однозначно понимаемое описание   содержания и форм представления информации, предъявляемой обучающемуся в процессе исполнения КОП;  порядка следования (очередности), регламента и способов предъявления (скрытия) учебной, контрольной и управляющей информации на средствах отображения АРМ обучаемого (АРМО);  правил, порядка и форм проведения контроля усвоения обучающимися полученных знаний, навыков и умений, включая описание критериев оценки знаний, пороговых значений величин, используемых для оценки, и изложение методик определения оценок;  способов, правил и порядка ввода информации (команд управления, ответов на контрольные вопросы) в процессе исполнения КОП, устанавливаемых для обучающихся. Рассмотрим разработку сценария КОП на примере группового теоретического занятия или лекции. Лекция как форма теоретической подготовки проводится с целью расширения области знания, формирования понятий и единообразных представлений у группы обучающихся по наиболее общим вопросам изучаемого материала. Материал преподносится в текстовом, табличном, графическом, видеоформатах и компонуется в виде отдельных кадров. Сценарий – это последовательное изложение содержания учебного занятия с указанием порядка и времени длительности каждой его части с распределением действий всех участников процесса обучения.  Предлагаемые сейчас сценарии КОП не соответствуют сценариям учебных занятий, принятым в учебных заведениях ВМФ. Сценарии современных КОП – это просто порядок предъявления кадров с учебной информацией. У них нет четкой структуры и не определены действия участников процесса обучения. Сценарии содержат кадры с темой и учебными вопросами, кадры с изучаемым материалом и с контрольными вопросами, но в них нет кадров с резюме после каждого изученного учебного вопроса, а также заключительных кадров с выводами по представленному материалу . Согласно методическим рекомендациям по проведению любых занятий, после каждого изученного вопроса преподаватель обязан сделать вывод, и в конце занятия им должно быть сделано заключение с обобщением. Современные компьютерные тренажеры создаются на базе персональных компьютеров, то есть изначально предусматривается, что участники процесса обучения (преподаватель и обучающиеся) взаимодействуют друг с другом как очно, так и посредством персональных компьютеров, объединенных в единую образовательную сеть на основе КОП. Из таблицы 2 видно, что варианты одной и той же КОП, которые подаются на экраны преподавателя и обучающегося, различаются между собой. Например, в Кратком анализе готовности на экран обучающегося выводятся итоги ответов на контрольные вопросы по пройденному ранее материалу. На экран преподавателя должны выводиться уже итоги по всем обучаемым, присутствующим на занятии. Разница в вариантах КОП должна быть и в подаче материала. Варианты КОП для преподавателя и для обучающегося назовем партитурами. Из вышесказанного следует, что сценарий учебного занятия должен иметь более одной партитуры. Одна партитура подается на компьютер (АРМ, ноутбук, планшетник) преподавателя, вторая – обучающегося. Если, кроме компьютеров, используется и интерактивная доска, на нее выводится версия обучающегося. Партитура, подготовленная преподавателем для себя, в данном случае может выступать и как план занятия, и как методическая разработка. Объем учебного материала, выделенного для проработки в рамках одной КОП, должен отвечать требованиям необходимой достаточности. Время работы с КОП не должно превышать двух академических часов (90 мин.) с учетом перерыва. Превышение данного времени может привести к резкому снижению усвояемости материала обучающимися. На самом начальном этапе компоновки материала следует предусмотреть его разбивку на логически завершенные части (абзацы, разделы) таким образом, чтобы к моменту перерыва в работе у обучающегося могла сформироваться целостная картина проработанной части материала .  Причем следует учитывать, что на классической лекции подача информации идет по визуальному и слуховому каналам и закрепляется в памяти путем создания образа, переформулирования полученной информации и записи ее в конспект. При этом работает еще и механическая память. В настоящее время при использовании компьютерной техники конспекты чаще всего не пишут. Это следует учитывать при разработке сценариев для аудиторной партитуры КОП, значит, должна увеличиться информационная нагрузка на зрительный и слуховой каналы восприятия информации. Партитура занятия обучающегося под руководством преподавателя не должна содержать много текстовых поясняющих вставок – только определения, таблицы, схемы и диаграммы. Все пояснения дает преподаватель. Другими словами, партитура обучающегося – это опорный конспект. Текст большого объема будет отвлекать обучающегося, и он отстанет от темпа занятия. Это объясняется тем, что большинство контрактников – мужчины, а в силу устройства мужской психики большинство из них не могут выполнять два дела одновременно, даже таких, как читать и слушать . Партитура КОП для преподавателя должна содержать визуальный ряд партитуры обучающегося, а также текст лекции и сноски на дополнительные материалы, чтобы преподаватель всегда был готов ответить на любые вопросы по теме лекции. Партитура КОП для занятий по самоподготовке должна содержать не только визуальный ряд, который подавался во время лекции на экран обучающемуся, но и текст лекции, а также сноски на дополнительные материалы. В этом партитура для самоподготовки схожа с партитурой преподавателя. Получается, что сценарий на одну тему должен иметь три партитуры: для преподавателя, для обучающегося в учебной аудитории и для обучающегося самостоятельно, в которой предусмотрены ссылки на справочный и другой поясняющий материал. В настоящее время при написании сценариев для КОП чаше всего используют режим предъявления обучающей программы без учета времени, что в корне неправильно, так как любое занятие, особенно на корабле, занимает ограниченное время. Как уже говорилось, по умолчанию считается (было принято при написании первых российских КОП для учебных заведений ВМФ командой преподавателей ВМУЭР им. А.С. Попова, работающих в «Пассате» (г. Санкт-Петербург)), что КОП рассчитан на два академических часа работы обучающегося. Поэтому следует оговорить условия закрытия программы:  принудительное с помощью кнопки закрытия; при этом должны сохраняться следующие данные: дата работы с КОП, кто работал, по какой теме, по каким вопросам, если были ответы на тесты, то и результаты работы с тестом;   Временной режим работы с КОП для обучающихся определяет преподаватель. 1. При формировании КОП занятия преподаватель может задать длительность каждого элемента занятия самостоятельно или отключить его совсем. 2. Если время не будет задано, на каждый элемент занятия будет отведено свое значение согласно руководству. 3. Время может быть отключено совсем при проведении самостоятельных занятий. 4. Во время самостоятельной подготовки обучающийся должен иметь возможность отключения времени работы КОП, чтобы работать в своем скоростном режиме. Во время занятия ведущей партитурой является партитура преподавателя. По мере его перехода с кадра на кадр происходит синхронный переход кадров на экране обучающегося. В режиме самоподготовки переход с кадра на кадр осуществляет сам обучающийся. Как видно из таблицы 3, материал, который предполагается использовать в трех разных партитурах, пересекается. Значит, создание партитур достигается ветвлением алгоритма программы. Сделать это еще на этапе проектирования КОП могут преподаватель совместно с программистом. Сценарий для КОП, разработанный на основе предложенного способа трех партитур, во-первых, помогает представить взаимодействие всех участников процесса обучения во времени, что важно для преподавателя, во-вторых, это может помочь программистам при написании самих КОП. В настоящее время при написании методического наполнения КОП и при их программном воплощении это все подразумевается, но точно никем не прописывается, а программирование не терпит приблизительности в описании. Если необходимо получить хорошие работающие КОП, то нужно учитывать все аспекты взаимодействия преподавателя, обучающегося и самой компьютерной программы.
Постоянный рост требований к быстродействию управляющих воздействий в сложных многофункциональных системах вызывает необходимость применения в процессе разработки функционального ПО (ФПО) для встроенных систем на кристалле (ВСнК) гибридных методов объектно-ориентированного проектирования (ООП) и языков реализации этих систем, ориентированных на поддержку исполняемых моделей в нотации UML . Одним из важнейших этапов ООП ФПО, обеспечивающего функционирование ВСнК в режиме реального времени (РВ), является оптимальная по определенным критериям декомпозиция предметной модели системы на взаимодействующие подсистемы . В существующих методологиях в качестве критерия выделения подсистемы используется пространственная обособленность некоторого множества объектов от остальной части проектируемой системы . В настоящее время при создании ВСнК РВ все большее применение находят решения на основе гетерогенных вычислительных платформ, включающих процессорный компонент с различной архитектурой набора команд, с поддержкой вычислений с фиксированной или плавающей запятой и беспроцессорный компонент интеллектуальных функциональных ядер перепрограммируемой логики. Процессорные элементы гетерогенных вычислительных платформ характеризуются значительными объемами основной и внешней памяти, гибкостью реализуемых алгоритмов управления и форматов данных. Элементы ядер обеспечивают существенно большее быстродействие, разнообразные внешние интерфейсы, обладают сопоставимыми с программируемыми процессорами возможностями адаптации алгоритмов работы к изменяющимся требованиям в процессе функционирования. В связи с этим при проектировании ФПО РВ актуальность решения задачи декомпозиции на подсистемы, реализуемые программно на процессорных компонентах и на конфигурируемых аппаратных ядрах, очевидна. Следует учитывать, что подсистемы, реализуемые на языках описания аппаратных средств (HDL), характеризуются низким и практически постоянным временем отклика. В то же время они имеют ограниченные ресурсы по реализации сложных аналитических алгоритмов обработки данных. Таким образом, сильно связанные объекты, выполняющие задачи принятия решения и управления, потребляющие большие объемы оперативной памяти с требуемым временем реакции, достижимым под управлением ОС РВ, целесообразно выделять в программную подсистему в кодах на языках высокого и низкоуровневого программирования, имплементация которой возлагается на процессор, а объекты, решающие критичные по времени вычислительно сложные задачи (например, цифровая обработка сигналов), выделять в «аппаратную» подсистему, реализуемую на языках HDL в логических блоках ВСнК. При использовании гетерогенной вычислительной платформы в качестве критериев декомпозиции ФПО на программную и аппаратную подсистемы в процессе ООП следует выбрать требуемое время реакции подсистемы на события, а также требуемую скорость и точность вычислений. Рассмотрим вариант применения критериев декомпозиции при проектировании распределенной системы управления технологическим процессом (ТП) РВ, представленной на рисунке 1. Подсистема формирования задач управления (ПФЗУ) периодически формирует блоки данных заданий для подсистемы формирования сигналов управления (ПФСУ), являющейся объектом проектирования. При этом ПФЗУ использует модели функционирования объекта управления, которым является ТП РВ, а формируемые задачи сформулированы в параметрах и величинах предметной области. Во временной области функциональные задачи привязаны к единому времени, которое отсчитывается по двум синхронизированным шкалам: T с периодом порядка единиц миллисекунд (формирование заданий управления) и D с периодом порядка единиц микросекунд (формирование управляющих сигналов для ТП РВ). ПФСУ является встроенной вычислительной системой, пространственно отдаленной от ПФЗУ и ТП РВ. Аппаратные средства ВСнК включают в себя универсальные 32-разрядные целочисленные RISC-процессоры средней мощности и блоки программируемой логики. Поступающие в соответствии с синхронизирующими импульсами шкалы T задания управления подвергаются дополнительной обработке, заключающейся в формировании последовательностей управляющих сигналов на основе параметров задания. Данный этап должен быть завершен до поступления следующего импульса шкалы времени T. Поскольку время обработки одного задания на имеющихся аппаратных средствах сопоставимо с длительностью периода синхросигнала шкалы T, время реакции на поступление задания от ПФЗУ должно быть на порядок меньше этого периода и составлять сотни микросекунд. Последовательности управляющих сигналов выдаются ТП РВ в соответствии с импульсами синхросигнала шкалы D. Таким образом, время реакции ПФСУ на импульсы синхронизации D должно быть на порядок меньше периода данной шкалы, то есть составлять не более сотен наносекунд. В качестве критерия декомпозиции проектируемого элемента на программную и аппаратную подсистемы используется требуемое время реакции на события, которыми являются фронты синхросигналов временных шкал T и D. Подсистема элемента ПФСУ, реализуемая как программа для процессорного ядра, функционирует под управлением ОС РВ на основе Linux; задачи, использующие дисциплину планирования FIFO, под ее управлением обладают временем отклика от нескольких десятков до нескольких сотен микросекунд. Таким образом, программный компонент использует только синхросигналы шкалы T. Результат выполнения декомпозиции ПФСУ на компоненты формирования управляющих последовательностей (ФУП) и выдачи управляющих последовательностей (ВУП) приведен на рисунке 2. Компонент ФУП включает в себя активный объект «Формирователь», который исполняется как задача с низким приоритетом и осуществляет формирование массивов управляющих сигналов, соответствующих поступающим от ПФЗУ заданиям. Активный объект «Планировщик», в свою очередь, исполняется как задача с высоким приоритетом и осуществляет передачу аппаратным средствам массивов управляющих сигналов в течение периода синхросигнала шкалы T, предшествующего периоду исполнения управления. Компонент ВУП состоит из двойного буфера, каждая часть которого хранит массив управляющих сигналов на один период шкалы T, и активного объекта «Автомат выдачи», представляющего собой конечный автомат, диаграмма состояний которого приведена на рисунке 3.  Событиями, вызывающими изменения состояния автомата, являются синхросигналы шкал T и D, а дополнительными условиями – состояния частей двойного буфера. Обобщенные состояния S1 и S2 связаны с использованием в работе соответственно первой и второй половин двойного буфера управляющих сигналов. Состояния W1 и W2 связаны с ожиданием поступления данных от компонента ФУП, а состояния V1 и V2 – с последовательной выдачей управляющих сигналов ТП РВ под управлением синхронизирующих импульсов шкалы D. На рисунке 4 изображена временная диаграмма функционирования ПФСУ. Параллельное выполнение активных объектов подсистемы обеспечивает совмещение по времени различных стадий обработки поступающих задач управления и режим работы подсистемы, аналогичный вычислительному конвейеру. Интервал шкалы T от импульса 2 до 3 характеризуется максимальным использованием ресурсов встроенной вычислительной системы. В течение этого периода высокоприоритетный «Планировщик» инициирует передачу аппаратным средствам массива управляющих сигналов З2, сформированного в предыдущем периоде. При этом фактическую запись управляющих последовательностей в свободную часть двойного буфера компонента ВУП производит контроллер прямого доступа к памяти (DMA), что освобождает процессор для работы низкоприоритетного «Формирователя» на большую часть периода. Одновременно автомат выдачи компонента ВУП осуществляет исполнение управляющей последовательности З1, сформированной в течение периода 0 и записанной в соответствующую часть буфера в течение периода 1. При планировании задач формирователя и планировщика в ОС РВ с использованием алгоритма монотонных частот условие выполнения временных ограничений системой определяется теоремой о верхней границе использования процессора, где Сф – время выполнения одного цикла задачи формирователя (обработки одного задания); Сп – время выполнения одного цикла задачи планировщика (выдачи аппаратным средствам одной управляющей последовательности); Т – период синхросигнала шкалы Т. Предлагаемый метод декомпозиции ФПО ВСнК позволяет успешно реализовать требования режима реального времени при управлении многофункциональными системами.
В настоящее время теория латентных параметров находит применение в педагогике, психологии, в маркетинге для исследования пожеланий потребителей, поведения поставщиков, направлений проведения досуга, эффективности рекламы, спортивной информации, при анкетировании в здравоохранении, обосновании программ реабилитации и т.п. -. Насчитывается несколько сотен коммерческих и свободно распространяемых программных продуктов, обеспечивающих оценивание латентных параметров. Значительное число исследований посвящено изучению моделей Раша, составляющих основу IRT -, и их расширений, а также методам оценивания латентных параметров -. Тестирование применяют для измерения латентного параметра, например знаний испытуемых. Оно состоит из этапов составления тестовых заданий, непосредственно проведения тестирования и последующей обработки результатов, которая дает оценку латентного параметра тестируемых – уровня подготовленности тестируемых и трудности заданий, надежности, валидности, дискриминационных возможностей и других параметров теста -. Обработка результатов тестирования представляет собой решение статической задачи: собирается массив данных, оцениваются трудность заданий и уровень подготовленности испытуемых. Процесс тестирования знаний по своей сути является динамическим. В начальном состоянии система имеет лишь экспертные оценки трудности заданий и параметров групп тестируемых. По мере проведения тестирования в различные интервалы времени происходит корректировка тестов: какието задания оставляют прежними, какие-то заменяют с учетом их трудности для применения на последующих сеансах тестирования. Важную роль в совершенствовании системы тестирования знаний играет обратная связь: результат функционирования системы тестирования влияет на параметры, от которых зависит функционирование этой системы. Поэтому актуальна разработка алгоритмического и программного обеспечения, в котором система тестирования рассматривается как динамическая с обратной связью. Постановка задачи Организатор тестирования проектирует базу заданий для многих вариантов тестов, задает и корректирует образовательный стандарт для различных классов испытуемых. Математическая модель динамической системы тестирования состоит в том, что в каждый момент проведения тестирования t исследуют однородную группу испытуемых с индексом класса в количестве  человек, которым предлагают тест, состоящий из  заданий с трудностями. По результатам тестирования составляется матрица ответов, которая показывает подготовленность испытуемых  и трудность заданий . Среди заданий теста в момент времени  могут быть узловых заданий, применявшихся ранее с трудностями где индексы  включены в множество индексов . Впервые используемые задания имеют начальные трудности, равные значениям, полученным из экспертных оценок. Узловые задания (ранее применявшиеся) имеют трудности, оцененные на последний момент времени применения каждого из них. Кроме трудностей  из БД извлекаются дополнительные атрибуты (значения достаточной статистики и соответствующей подготовленности) – вычисленные в момент последнего применения соответствующего задания параметры, необходимые для рекуррентных вычислений в момент времени t. Если в момент времени принято решение уменьшить число заданий в тесте, тест считается прежним, когда в матрице ответов по этим заданиям элементы соответствующих столбцов были равны только 0 или 1. В противном случае считается, что это новый тест. Увеличение количества заданий за счет добавления заданий по теме теста или замена заданий на задания с аналогичными трудностями не приводит к изменению теста. Все множество испытуемых, для которых создают тесты, естественно разбить на классы, соответствующие тематике тестов и их назначению. Предположим, что подготовленности испытуемых в логитах в каждом классе характеризуются гауссовым распределением N(Mg, σg). Параметры класса в начальный момент определяются экспертными оценками и уточняются по мере обработки результатов тестирования для соответствующего класса. Организатор тестирования формирует однородную группу n испытуемых из некоторого класса и предлагает без ограничения общности один тест с определенным числом заданий различной трудности, обеспечивающих объективное тестирование. Если группе предлагаются несколько равноценных вариантов теста, группой в данный момент считается то множество тестируемых, которое решает один и тот же вариант теста, а после оценки результатов тестирования все подготовленности выравниваются на единой шкале. Математическая модель решения задачи Для оценки трудности заданий и подготовленности испытуемых применяется метод максимального правдоподобия (JML) -. Кроме того, в системе для оценки трудности заданий могут применяться методы условного максимального правдоподобия (CML) и маргинального максимального правдоподобия (MML), а для оценки подготовленности испытуемых, помимо метода максимального правдоподобия, могут применяться байесовские оценки. В предложенной системе, помимо дихотомической модели Раша, реализованы следующие расширения этой модели: 2- и 3-параметрические модели (модели Бирнбаума), а также 4-параметрическая модель. Кроме того, реализованы такие политомические расширения модели Раша, как модель RSM и модель PCM. Помимо этого, реализованы линеаризованные политомические модели LLTM, LRSM и LPCM. В качестве критериев проверки гипотезы об адекватности модели Раша - применяют статистику отношения правдоподобия, статистику Хосмера–Лемешоу, коэффициенты детерминации, ROC-анализ. Число заданий в тесте определяет разрешающую способность теста (РСТ), равную разнице подготовленностей двух испытуемых в логитах, у которых разница в количестве правильных ответов равна 1. Разрешающая способность теста, состоящего из заданий, зависит только от числа заданий теста и числа испытуемых и принимает значения из интервала , который и определяет нижний предел дифференцирующей способности теста. Эта величина напрямую не зависит от трудности заданий. К этому пределу и нужно стремиться при проектировании теста. При этом дисперсия ошибки оценивания подготовленности определяется трудностями заданий теста. Определим понятие наилучшего теста как совокупность заданий с такими трудностями при которых тест обладает наивысшей разрешающей способностью в данном классе испытуемых, его среднеквадратичная ошибка для каждого испытуемого максимально приближена к РСТ среди всех возможных тестов с таким же количеством заданий. Каждый испытуемый в группе данного класса должен получить хотя бы одно задание, соответствующее его подготовленности. Чем больше по численности подгруппа испытуемых с близким уровнем подготовленности, тем больше должно быть заданий, соответствующих их подготовленностям. Эти характеристики наилучшего теста показывает его наивысшую дифференцирующую способность при данном количестве заданий и данной группе данного класса испытуемых. Критерием оптимальности распределения заданий в тесте в данной группе по их трудностям естественно принять математическое ожидание квадрата разницы среднеквадратического отклонения ошибки способностей тестируемых от наилучшей точности теста при данном количестве заданий в тесте.  Поэтому система тестирования должна в рекуррентной форме формировать оптимальный набор трудности заданий в текущий момент времени для заданного числа заданий, получая на вход минимально достижимое значение ошибки и вычисленное по итогам прошлого такта значение ошибки. Статистическая обработка результатов тестирования происходит на основе метода максимального правдоподобия. Уравнения правдоподобия имеют вид где  с условиями типа равенства, учитывающими информацию, полученную в предыдущие моменты времени вышеописанные атрибуты ранее вычисленных трудностей. На первом такте работы системы набор трудностей заданий теста формируется на основании теоретических предпосылок о гауссовом распределении подготовленности испытуемых. В дальнейшем в соответствии с рекуррентной процедурой стохастической аппроксимации на каждом такте времени происходит уточнение трудности заданий и параметров нормального распределения подготовленности группы. Функция правдоподобия содержит плотности с теми заданиями, которые встречались на предыдущих тактах времени. В условии типа равенства трудности заданий варьируемы, а подготовленности принимаются константами, равными последним оценкам, полученным на какомлибо раннем такте времени. При этом узловыми могут оказаться задания из любого числа ранее решаемых тестов в любые предыдущие моменты времени. Это обеспечивает единую шкалу трудности заданий и подготовленности тестируемых. Для введения в систему процесса адаптации параметры класса испытуемых на каждом такте времени для соответствующего класса g пересчитываются. На этапе формирования теста в каждый момент времени проверяются следующие условия: 1)предельная дисперсия оценки подготовленности; 2) изменились ли параметры (математическое ожидание и дисперсия) распределения уровня подготовленности тестируемых. При выполнении хотя бы одного из этих условий решается задача оптимизации и тем самым вычисляется оптимальный набор трудностей заданий теста. В каждый такт времени задача оптимизации решается с начальным условием в виде оптимального набора трудностей заданий теста для такта работы. В качестве начального распределения уровня трудности заданий взято равномерное распределение. Кроме того, перед началом работы системы должен быть подготовлен ряд заданий теста и определена их трудность. Трудность этих заданий может быть определена либо посредством апробации их на стратифицированной нормативной выборке тестируемых, либо на основании экспертной оценки. Стоит отметить, что тестируемые возмущаются некоторой случайной ненаблюдаемой помехой она описывает возможное знание испытуемыми конкретных вопросов теста, возможность угадывания и списывания, психологическое волнение, а также другие случайные факторы. Для учета факторов, мешающих получению объективных оценок подготовленности тестируемых, таких как списывание, подсказки и прочее общение с другими участниками тестирования, применяются результаты исследований по квалиметрии групповой деятельности операторов -. Если ввести коэффициент взаимодействия испытуемых Kвз, равный 0 при отсутствии взаимодействия и равный 1 для абсолютного взаимодействия испытуемых, действующих согласованно, как один человек, то при коэффициенте взаимодействия, стремящемся к 1, для соответствующей группы потребуем, чтобы среднеквадратичное отклонение оценок подготовленности тестируемых стремилось к бесконечности. Введенный коэффициент влияет на результаты таким образом: пусть тестируется группа из класса с распределением подготовленности, тогда при Kвз = 0 параметры распределения  остаются прежними. При , когда есть взаимодействие, среднее смещается в сторону лидера группы, на которого все будут ориентироваться, что вносит свой вклад в результаты тестирования, и в таком случае целесообразно описывать группу распределением функции, описывающие влияние взаимодействия на средний уровень подготовленности группы и разброс подготовленностей в группе. При росте Kвз разброс подготовленностей в группе уменьшается. При Kвз=1 взаимодействие участников тестирования абсолютное, то есть все тестируемые дают одинаковые ответы на каждый вопрос теста. Тогда целесообразно не засчитывать такой результат тестирования, так как дисперсия подготовленности стремится к бесконечности. Если на каждом такте коэффициент взаимодействия удовлетворяет условию  и ранг матрицы ответов не равен 1, а критерий минимизации имеет вид  взаимодействие в группе и прочие случайные факторы приводят к тому, что градиент критерия измеряется со случайной помехой  , где – независимые случайные величины с ограниченной дисперсией, то процедура стохастической аппроксимации для поиска оптимального  вектора, где  множители удовлетворяют условиям , сходится при   любой группе почти, наверное, к оптимальному . Кроме того, при  в любой группе вектор асимптотически нормален, где матрица ,  матрица дисперсий шума. Алгоритм адаптивной системы тестирования Для решения поставленной задачи разработано алгоритмическое обеспечение адаптивной системы тестирования, блок-схема алгоритма очередного такта которого представлена на рисунке. В начале работы очередного такта системы организатор тестирования формирует однородную группу испытуемых. После этого система считывает организационные параметры тестирования: требования к заданиям теста, количеству заданий, содержанию заданий, времени выполнения заданий и т.д. С учетом среднеквадратичной ошибки подготовленностей, полученной на предыдущем такте t–1 работы системы, выполняется шаг стохастической аппроксимации и вычисляются оптимальные значения трудностей заданий теста, на основании которых конструируется оптимальный тест для сформированной однородной группы испытуемых. Тест из сформированных оптимальных заданий предъявляется группе испытуемых и в соответствии с установленным регламентом под наблюдением организатора тестирования в диалоговом сеансе происходит фиксация ответов группы испытуемых с записью окончательных ответов на задания теста в таблицу БД. По итогам тестирования определяется правильность ответов тестируемых на предъявленные вопросы и формируется матрица ответов.  Статистическая обработка полученной матрицы ответов происходит в соответствии с описанными выше алгоритмами, вычисляются подготовленности испытуемых и новые значения трудностей заданий теста. Для сформированной группы испытуемых вычисляются параметры адаптации – интегральные характеристики группы, и в случае их отличия от используемых на данном такте происходит их перенастройка для следующего такта работы системы. Кроме того, вычисляются разница между определенной проектировщиком разрешающей способностью теста и вычисленной ошибкой в результате статистической обработки результатов тестирования, а также качество ответов группы испытуемых с оценкой возможного знания испытуемыми конкретных вопросов теста, угадывания, массового списывания. При отсутствии оснований для аннулирования результатов тестирования группы испытуемых осуществляются определение баллов по метрической шкале и пересчет полученных показателей тестирования в логитах путем линейного преобразования в необходимую метрическую шкалу в баллах. Результаты тестирования предъявляются испытуемым и записываются в таблицу БД. В реализованной системе информация о тестах, вопросах, участниках тестирования и их результатах хранится в БД, принципиальная схема которой представлена на рисунке. В таблице dbo.Persons содержатся идентификатор каждого тестируемого и его персональная информация. В таблице dbo.PersonStats хранится информация о том, на каком такте работы системы и в какой группе проходил тестирование испытуемый, а также какую оценку уровня знаний он получил. Таблица dbo.Groups содержит информацию о группах тестируемых и ее свойствах, таких как математическое ожидание уровня знаний и стандартное отклонение. Таблица тактов dbo.Tacts содержит идентификатор очередного такта работы системы и время его начала. При старте очередного такта система формирует для группы тестируемых новый тест и записывает эту информацию в таблицу тестов dbo.Tests. Ответ каждого тестируемого заносится в таблицу dbo.TestResults. Таблица заданий dbo.Questions содержит идентификатор задания, его текст и тип задания, который может быть дихотомическим или политомическим, а также вид задания: задание в отрытой форме, с одним или несколькими правильными ответами, на установление соответствия, на установление правильного порядка. Ответы на все тестовые задания хранятся в таблице ответов dbo.Answers. В каждой строчке хранится либо правильный ответ на вопрос (если запись относится к вопросу в открытой форме), либо вариант ответа. Таблица заданий с вариантами ответов dbo.ClosedQuestions содержит информацию о вариантах ответов на задания и признак правильного или неправильного ответа. Таблица заданий на установление соответствия dbo.MatchingQuestions хранит данные о каждом соответствии для задания. В таблице заданий на установление порядка dbo.OrderingQuestions содержится информация о порядковом номере каждого варианта ответа для задания. Таблица заданий в открытой форме dbo.OpenQuestions хранит данные об ответе на каждую категорию задания, а также балл за эту категорию. Таблица статистики заданий dbo.QuestionStats содержит последнее вычисленное значение трудности категории задания и значение накопленной достаточной статистики, обозначенной в алгоритмах символом c, при последнем вычислении трудности данной категории задания. В заключение отметим, что в данной работе представлено алгоритмическое обеспечение адаптивной системы тестирования знаний, в которой в определенные моменты времени происходят подготовка и корректировка тестов, тестирование и обработка результатов. В начальном состоянии система имеет лишь экспертные оценки трудности заданий и параметров групп тестируемых. После каждого такта работы системы, помимо вычисления уровня подготовленности тестируемых, в зависимости от состояния системы происходят корректировка состава теста с целью минимизации ошибки оценивания результатов тестирования и уточнение трудности заданий. При этом существенную роль в совершенствовании системы тестирования играет обратная связь, на основе которой вырабатывается управляющее воздействие – набор трудности заданий очередного теста. Оценки трудности и подготовленности в предложенной системе базируются на основе сходящейся процедуры стохастической аппроксимации. Указанные свойства адаптивной системы тестирования знаний гарантируют повышение точности тестирования с течением времени и выравнивание оценок тестируемых на единой шкале.
Известно, что бюджетная система Российской Федерации состоит из бюджетов трех уровней:  федеральный бюджет и бюджеты государственных внебюджетных фондов;  бюджеты субъектов Российской Федерации и бюджеты территориальных государственных внебюджетных фондов;  местные бюджеты. Бюджетный процесс охватывает всю структуру бюджетной системы Российской Федерации и представляет собой регламентируемую нормами права деятельность органов государственной власти, органов местного самоуправления и участников бюджетного процесса по составлению и рассмотрению проектов бюджетов, проектов бюджетов государственных внебюджетных фондов, утверждению и исполнению бюджетов и бюджетов государственных внебюджетных фондов, а также по контролю за их исполнением. В основу теоретических представлений о моделировании бюджетного процесса заложена семиотика. Согласно взглядам основателя этого научного направления Чарлза Сандерса Пирса, в своем практическом воплощении наука о знаках и знаковых структурах – семиотика – разделяется на три основных раздела, каждому из которых отвечает некоторый класс методов. Это синтаксис, семантика и прагматика. Обобщая, можно говорить о проверке правил формальной записи бюджетных документов (синтаксический контроль), анализе смысловой нагрузки каждой статьи бюджета (семантический контроль), а также о практическом воплощении бюджетных позиций – об их соответствии некоторым динамическим моделям экономической реальности (прагматический контроль). Обращение к указанным разделам семиотики порождается необходимостью работать с последовательно возрастающей степенью обобщения материала на различных этапах бюджетных процессов. Так, синтаксический контроль осуществляется на уровне структур данных, их размещения в таблицы (базы) данных и организации потоков обмена данными в сетях коммуникаций на уровне стандартных документов. Соответственно, семантический контроль осуществляется на уровне обобщения данных на более высокий агрегатный уровень, уровень информации, когда данные функционально привязываются к логическим структурам по их происхождению и предназначению. Прагматика работает уже с тем уровнем, которому функционально отвечает знание, более высокая по отношению к информации степень обобщения материалов бюджетного процесса. В последнем случае решения принимаются на основе представлений так называемых KBS-систем (технологий), основанных на знаниях (knowledge-based systems), являющихся основой целого семейства интеллектуальных компьютерных приложений . В работе рассматривается одна из моделей процесса социально-экономического развития (СЭР) субъекта управления (СУ), которая может быть использована для описания множества конечных состояний СУ в рамках планирования, исполнения и контроля бюджетного процесса в соответствии с требованиями Бюджетного кодекса России. Базовая терминология Базовыми элементами, определяющими конечные состояния процесса СЭР, являются санкция, операция и транзакция. Исходным постулатом для определения математической модели процесса СЭР являются утверждение взаимного влияния базовых элементов друг на друга и их неразрывная связь, на основе которой образуется пространство Санкции–Операции–Транзакции (СОТ), то есть пространство СОТ. Пространство СОТ является многообразием состояний процесса СЭР, последовательность которых приводит к конечным результатам, определяющим оперативность, результативность, реализуемость и бюджетную эффективность процесса СЭР. Указанные параметры конечных результатов оцениваются на основе значений критериев Efficiency (бюджетная эффективность) и Effectiveness (результативность) при организации и проведении Performance Audit (аудита исполнения/аудита качества деятельности). Критерий Efficiency позволяет оценить оптимальность привлечения различных ресурсов при реализации процессов СЭР и может быть записан формулой где Ef – продуктивность; EC – результативность; Cp – плановые затраты; CR – фактические затраты; Tp – плановое время; TR – фактическое время; MP – плановый результат; MR – фактический результат. Значения критерия Efficiency классифицируются следующим образом:  если , продуктивность выше ожидаемой;  если , продуктивно;  если , непродуктивно. Критерий Effectiveness позволяет оценить степень достижения конечных целей СЭР путем сравнения его фактических и запланированных результатов, включая оценку побочных и предусмотренных последствий, и может быть записан формулой где EC – результативность; TP – плановое время; TR – фактическое время; MP – плановый результат; MR – фактический результат. Значения критерия Effectiveness классифицируются следующим образом:  если , результативность выше ожидаемой;  если , результативность соответствует ожиданиям;  если , результативность ниже ожидаемой. Описание пространства состояний Чтобы применить критерии, необходимо определить пространство СОТ и требования к особенностям взаимосвязи санкции, операции, транзакции в пространстве СОТ. Определим множества: множество допустимых санкций; множество допустимых операций; множество допустимых транзакций. Исходным базовым элементом пространства СОТ является тройка элементов , которая может формироваться по заданным правилам, определяющим отличительные особенности процесса СЭР на рассматриваемом периоде прогнозирования конечных результатов процесса СЭР. Базовые элементы пространства СОТ задаются на множестве. Указанные элементы синхронизированы во времени (имеется единая временная шкала). Временными микроинтервалами взаимосвязь основных элементов определяется мнемосхемой. Можно определить , причем интервал  – время «жизни» тройки . Тройка является кортежем операций над неким ресурсом. Ресурс может иметь интерпретацию: деньги, материалы, технологии, архитектура, стратегия. Перечисленные варианты ресурса должны быть приведены к единой измерительной шкале либо иметь единую измерительную интерпретацию (например деньги). Для применения модели динамический гиперграф (ДГ) необходимо определить понятие родовой структуры , то есть в интерпретации авторов – интервалы изменений ресурсов (в том или ином представлении), а также интервалы . Ребром ДГ в нашем случае является кортеж  в различные интервалы времени. Вершиной ДГ является гиперграф, где ; U – ребра, то есть 4-арные отношения на множествах ; R – отношение порядка на множествe . Основная классификационная схема использования понятий и процессов Каждый из используемых элементов  является описанием некоторого действия с ресурсами различного назначения (материальные, нематериальные, нормативные и прочие). Все виды ресурсов назовем активами, все виды действий – видом работ. Таким образом, имеем множество активов  и множество видов работ , где  – соответственно С;  – соответственно О; ВР3 – соответственно Т. Рассмотрим , который по функциональному назначению имеет два подвида:  – санкция получена;  – санкция отсутствует. Второй элемент  по функциональному назначению имеет подвиды:  – длительность – вид работы, который независимо от объема потребляемого актива  выполняется фиксированное время (интервал времени);  – производительность – вид работы, который зависит от объема потребляемого актива  в единицу времени; – контрольное событие – вид работы, который выполняется в фиксированный момент времени, в течение нулевого интервала времени и не связан с потреблением актива; обременение – вид работы, который выполняется фиксированное время (интервал времени) и связан с потреблением (фиксацией) конечного объема актива. Третий элемент по функциональному назначению имеет подвиды: транзакция проведена; транзакция отклонена по условию выполнения работ. Каждый вид работы (а также подвид) систематизирует и определяет некоторый процесс с активами: определяет процесс получения санкций на использование актива; определяет процесс подготовки использования актива по результатам получения санкции; определяет процесс изменения прав собственности на актив в результате подготовки использования актива. Моделирование элементов процессов и структур СЭР позволяет эффективно решать задачи структурного синтеза и управления и является важнейшим инструментом как на этапах проектирования системы, так и в процессе эксплуатации. Практическое применение этих принципов и требований при создании структурных моделей СЭР позволяет в большей степени отразить специфику гибкого многономенклатурного производства. Рассмотрим процесс разработки математического аппарата, адекватно описывающего структуры СЭР и позволяющего учитывать динамику происходящих в системе процессов. В качестве такого математического аппарата предлагается использовать динамические гиперграфы специального вида (ДГСВ). Определение. ДГСВ называется объект , состоящий из пары множеств, в котором, множество гипергафов (экземпляров), имеющих одно и то же множество вершин X и получаемых в рамках родовой структуры , множество динамических ребер обладающих следующими свойствами:  каждое ребро ДГСВ определяется временным интервалом моделирования:  каждое ребро ДГСВ не может объединять вершины различных экземпляров с одинаковыми индексами:  каждое ребро ДГСВ может включать в себя только по одной вершине из каждого экземпляра в некоторый интервал времени: Родовая структура W0 определяет границы изменения множества экземпляров W. В общем случае W0 включает все возможные ребра на заданном множестве вершин. Рассматривая конкретный вариант календарной структуры, в используются подмножества общего множества ребер, которые выделяются из условий возможности применения операций, СОТ и т.д., в рамках заданных материальных ограничений. Множество ДР ДГВС определяет те вершины множества экземпляров, которые существуют на данном временном интервале (по условию 3). При этом (согласно условию 4) в каждое ребро может входить не более одной вершины из каждого экземпляра множества W. Следует отметить, что все экземпляры множества W могут получаться и изменяться только в рамках родовой структуры ДГСВ. Отличительной особенностью аппарата ДГСВ от ДГ является то, что множество ДР ДГСВ описывает порядок существования вершин различных экземпляров множества в некоторый временной интервал, в то время как множество ребер ДГ определяет порядок существования экземпляров. Назовем указанный временной интервал дискретом существования ДР ДГСВ. Рассмотрим понятия фрагмента экземпляра ДГСВ. Определение. Фрагмент ДГСВ – обозначаемое через множество фрагментов экземпляров рассматриваемых в данный дискрет времени. Другими словами, фрагмент ДГСВ состоит из фрагментов отдельных экземпляров, которые существуют в дискреты времени. Введем понятие зависимых и независимых вершин для данного временного дискрета. Под зависимыми вершинами будем понимать вершины, которые в данный дискрет не удовлетворяют условию (4). Все остальные вершины являются зависимыми. Тогда любое ДР может охватывать только зависимые вершины. При этом время существования ДР, то есть дискрет, определяется минимальным временем существования вершины экземпляра, входящей в это ДР где Tj – время существования ДР, а ti – время существования вершин xi. Рассмотрим представление КС при помощи ДГСВ. Отличительной особенностью модели A является неизменная мощность множества вершин родовой структуры. Множество X определяется объединением множеств Z и E, то есть , где Z и E – множества СОТ и ТЕ (множество технологических операций на множестве СОТ, которые задаются нормативными документами субъекта управления) соответственно. Таким образом, множество X при описании модели КС идентифицирует множества СОТ и ТЕ. Множество ребер родовой структуры определяет границы возможных связей (перемещений) на множестве X. Как будет показано далее, в рамках модели ДГСВ определяет родовую структуру КС. По определению, технологическая структура (ТС) представляет собой набор технологических маршрутов прохождения предметов изготовления в родовой структуре. Технологический маршрут описывается кортежем из элементов множеств Z и E. В рамках модели ДГСВ множество экземпляров W адекватно описывает ТС. Каждый экземпляр отражает технологический маршрут на изготовление предмета dj. При этом должно выполняться следующее условие: то есть все технологические маршруты существуют только в рамках родовой структуры. В каждом j-м экземпляре вершин, связанных ребрами, идентифицируют те СОТ, которые участвуют в изготовлении j-го предмета. В рамках модели ДГСВ календарно-технологическая структура (КТС) определяет возможные порядки процессов календарной синхронизации с точки зрения непересечения сроков занятости элементов в различных экземплярах. В модели A порядок использования вершин различных определяется множеством P. Следовательно, множество ДР отражает КТС, а ДГСВ A – полную систему КС. Для решения задач синтеза и управления КС на основе ДГСВ важным является возможность проведения различных преобразований на множестве экземпляров W. Необходимость преобразований экземпляров возникает при введении в систему возмущающих действий, когда требуется получить новые технологические маршруты (экземпляры) или изменить имеющийся. Другими словами, в рамках родовой структуры необходимо получить новые экземпляры за счет имеющейся в КС гибкости. С этой целью рассмотрим множество операций: перенос ребра, удаление ребра, введение ребра. Покажем их адекватность теоретико-множественными операциями: объединения, вычитания, пересечения. Самой простой является операция удаления ребра, так как она не требует никаких проверочных условий. Действительно, если задано исходное множество ребер  и необходимо удалить ребро , то, представив последнее в виде множества с одним элементом, при помощи теоретико-множественной операции вычитания получим . В результате получим экземпляр. Отметим, что операцию удаления можно производить одновременно для n ребер. Операция переноса из одного экземпляра в другой осуществляется путем простого объединения множеств. Например, требуется перенести ребро из экземпляра в . Для этого представим ребро  как множество  с одним элементом и далее производим операцию, то есть . Таким образом, получим экземпляр , соответствующий экземпляру Wi. Очевидно, что операция переноса справедлива для n ребер. Более сложной операцией является операция введения ребра, так как необходимо каждый раз проверять принадлежность этого ребра. Проверка на принадлежность осуществляется операцией пересечения множеств, а введение – ранее описанной операцией переноса ребра. Покажем это на примере. Пусть требуется ввести ребра между вершинами в экземпляр. Представим ребра родовой структуры в следующем виде. Тогда . Проверим принадлежность родовой структуре вводимых ребер. Следовательно, ребро принадлежит , так как его пересечение с множеством ребер  не пусто, а ребро не принадлежит, поскольку в результате пересечения получается пустое множество. Поэтому не может быть введено в синтезируемый экземпляр. Далее описанной выше операцией переноса ребра мы вводим в исходный экземпляр, в результате чего получаем новый гиперграф. Таким образом, при помощи модели ДГВС возможно адекватное представление структурных составляющих КС, а система теоретико-множественных преобразований является полной относительно получения новых экземпляров  в рамках родовой структуры . В настоящее время коллективом Научно-исследовательского института системного анализа и экспертизы (г. Москва) в рамках исследований и разработки методов проектирования и моделирования процессов СЭР субъекта создается интегрированный программный комплекс на основе моделей ДГСВ в рамках рассмотренных формализмов.
В настоящее время проблема коррозии оборудования становится все более актуальной. Согласно данным, опубликованным в государственных отчетах «О деятельности Федеральной службы по экологическому, технологическому и атомному надзору», ежегодно более 30 % аварий на магистральных трубопроводах, протяженность которых за последние 10–15 лет существенно увеличилась, происходит именно по причине коррозии. Ежегодные потери валовой общественной продукции из-за коррозии составляют приблизительно 4 %. В РФ действует порядка 350 тыс. км трубопроводов, на которых ежегодно происходит большое количество аварий. Результатом этого являются потери углеводородного сырья при добыче и транспортировке до 7 % от добываемого объема, металла – до 20 % годового производства стали, ущерб промышленному производству составляет сотни млрд рублей в год. Аварийность технологических систем в нефтегазовой отрасли из-за коррозии достигает 31 % от общего числа, что связано с большой металлоемкостью оборудования и сооружений и агрессивностью среды . Наибольшие потери от коррозии несут топливно-энергетический комплекс (ТЭК) (порядка 30 %), сельское хозяйство (15 %), химия и нефтехимия (20 %) . Аварийность в химической и смежных отраслях промышленности по причине коррозии оборудования составляет более 30 % от общего числа аварий . В этой связи необходимо иметь соответствующие БД и информационно-моделирующие системы (ИМС), позволяющие оценить и прогнозировать коррозионную стойкость и обеспечить выбор соответствующих материалов, обеспечивающих защиту оборудования от коррозии. В настоящее время разработан ряд информационных систем в области экологической и промышленной безопасности, баз и банков данных со свойствами материалов, различных ингибиторов и химической продукции . Практически все они являются закрытыми или коммерческими. В связи с этим актуальна разработка доступного для широкого круга специалистов информационного и программного обеспечения для решения задач анализа и оценки коррозионной стойкости оборудования опасных производственных объектов. Функциональная структура ИМС Функциональная структура ИМС защиты оборудования от коррозии показана на рисунке 1. Основные функции подсистемы защиты от коррозии:  расчет скорости коррозии оборудования под действием указанных агрессивных сред с использованием и без использования ингибиторов;  определение ингибитора или покрытия для снижения скорости коррозии;  расчет срока службы оборудования с учетом и без учета средств защиты от коррозии;  составление графика профилактических и ремонтных работ технологического оборудования;  выдача рекомендаций по использованию материалов-аналогов при замене оборудования на основе химического состава или эксплуатационных свойств;  выдача рекомендаций – мер по снижению скорости коррозии оборудования. Для реализации ИМС защиты оборудования от коррозии использовалась трехзвенная архитектура проектирования информационных систем:  уровень БД – выполняет задачи хранения, обработки, обеспечения целостности, непротиворечивости информации, контроля прав доступа пользователей и приложений к данным и т.д.; для реализации функций данного уровня используется СУБД Oracle XE (Express Edition) 10g;  уровень бизнес-логики – выполняет задачи исполнения правил и алгоритмов работы информационной системы; для реализации функций данного уровня используют различные серверные среды и языки программирования, платформы разработки серверов приложений; в ИМС в качестве такой платформы используется стек технологий Windows+Apache+PHP (Hypertext Preprocessor), построенный на основе веб-сервера Apache 2.2.15 и языка программирования PHP 5.3, установленных на операционную систему Microsoft Windows Server;  уровень презентации данных – основной его задачей является организация взаимодействия пользователя с системой при помощи текстографических, оконных интерфейсов пользователя; эту роль выполняет стандартный тонкий клиент – интернет-браузер, который должен быть установлен на рабочем месте пользователя системы.  Подсистемы и БД ИМС В структуру ИМС защиты от коррозии входят семь основных подсистем. Подсистема взаимодействия с пользователем. В качестве данной подсистемы выступает стандартный веб-браузер (Google Chrome, Opera, Mozilla Firefox, Safari, Internet Explorer или любой другой), установленный на рабочем месте пользователя. Основное предназначение – обеспечение диалога между пользователем и информационномоделирующей системой. Кроме этого, осуществляется проверка вводимой информации на необходимую полноту и непротиворечивость. Подсистема хранения данных. Представляет собой СУБД Oracle Express Edition (XE) 10g, в которой хранятся  БД типового оборудования опасных производственных объектов (содержит таблицы с характеристиками типов аппаратов, модельных рядов аппаратов, конкретных единиц оборудования (экземпляров) модельных рядов, а также характеристик аппаратов, установленных (эксплуатируемых) на производстве);  БД по веществам и свойствам (содержит данные о физических, пожароопасных, взрывоопасных, токсических свойствах веществ и материалов);  БД по показателям коррозионной защиты (состоит из трех таблиц, в которых содержится подробная информация о коррозионных средах, материалах (металлические и неметаллические), используемых для работы в этих средах, и соответствующих показателях скоростей коррозии);  БД ингибиторов и покрытий (содержат перечень ингибиторов и материалов покрытий и информацию о скоростях коррозии в разных агрессивных средах с использованием ингибиторов и покрытий);  БД материалов. Логическая модель (структура) БД материалов, реализованная в виде диаграммы типа «сущностьсвязь» или ER-диаграммы (Entity-Relationship Diagram – ERD), представлена на рисунке 2 и включает 16 сущностей (таблиц):  таблицу, содержащую систематизированные данные о металлических материалах;  таблицы, содержащие физические, технологические и эксплуатационные свойства материалов и их химический состав (испытание на усталость, характеристики при испытании на длительную прочность, испытание на изгиб в холодном состоянии, теплостойкость, жаростойкость, эксплуатационные свойства, температурные свойства, ползучесть при испытании на длительную прочность и др.);  таблицы со сведениями об источниках, откуда берутся данные о материалах: справочники, литературные источники и нормативно-технические документы (НТД);  таблицы с дополнительными сведениями (страна, производители). Для создания БД использовалась реляционная модель БД. Между таблицами реализуется связь «один-ко-многим», которая означает, что один экземпляр (запись) первой сущности связан с несколькими экземплярами второй. Например, одной записи в таблице «Материал» могут соответствовать несколько записей в таблицах свойств. Описанная БД интегрирована с БД по показателям надежности типового оборудования химически опасных и других опасных производственных объектов, являющихся источниками химической и токсической опасности, и БД по химической, токсической и коррозионной стойкости типового оборудования химически опасных и других опасных производственных объектов, являющихся источниками химической и токсической опасности, разработанных ранее в рамках учебно-методического комплекса по проблемам химической и биологической безопасности кафедрой компьютерно-интегрированных систем в химической технологии РХТУ им. Д.И. Менделеева . Подсистема подбора материалов оборудования вспомогательных конструкций и изделий предназначена для поиска материалов в БД по составу стали или сплава или указанным свойствам (физико-химическим, эксплуатационным и др.). Кроме этого, возможен подбор материала, отвечающего всем введенным параметрам и при этом наиболее экономически целесообразного в использовании. Расчетная подсистема включает несколько блоков. Блок расчета скорости коррозии оборудования, предназначенный для определения периода времени, в течение которого стенки оборудования будут разрушены по причине коррозии. При этом возможно проведение расчетов при наличии или отсутствии ингибиторов или защитных покрытий. Блок технико-экономических расчетов позволяет провести оценочный расчет рентабельности капитальных вложений на модернизацию, реструктуризацию оборудования и установок, цехов или предприятия в целом с использованием выбранных материалов или средств и методов коррозионной защиты. Блок расчета срока службы оборудования предназначен для определения среднего срока службы оборудования для различных условий эксплуатации. Подсистема защиты от коррозии предназначена для определения мер по повышению коррозионной стойкости оборудования. Данная подсистема состоит из двух блоков: блока определения оптимального материала, предназначенного для идентификации более коррозионностойкого материала оборудования в указанных пользователем условиях эксплуатации (температурный режим, агрессивная среда), и блока определения оптимального ингибитора/покрытия, используемого для выбора ингибитора агрессивной среды или покрытия материала для снижения скорости коррозии. Подсистема выдачи рекомендаций состоит из двух блоков: базы знаний по условиям и рекомендациям и блока генерации рекомендаций по мерам повышения коррозионной стойкости оборудования и отказоустойчивости оборудования и установок в различных агрессивных средах. Рекомендации генерируются на основе продукционных правил. В качестве критериев подбора материалов и покрытий могут выступать срок службы оборудования, минимальная стоимость оборудования, изготовленного из выбранных марок стали, с учетом выбранных ингибиторов и покрытий и без их учета и другие. На рисунке 3 представлен фрагмент главной страницы ИМС (полноценный вариант), а на рисунке 4 – основные интерфейсы подсистемы защиты от коррозии, в частности, процесс добавления нового материала в БД. Для добавления необходимо нажать «Добавить новую марку материала» и заполнить вкладки карточки добавления. Обязательным является заполнение вкладки «НТД», в которой дается краткое описание нормативно-технического документа, вкладки «Материал», в которой описываются характеристика и назначение материала, и вкладки «Химический состав». Также представлен модуль подбора материалов по химическому составу. Для подбора материалов в ниспадающих списках нужно выбрать химический элемент и ввести его содержание в поля ввода, в таблицу ниже будет выведен список материалов. Подытоживая, отметим, что разработанная ИМС применима для широкого круга специалистов на всех стадиях жизненного цикла химического предприятия: при проектировании нового объекта или реконструкции, ремонте существующего; при модернизации химических, нефтехимических, нефтеперерабатывающих и других предприятий; при обучении проектировщиков и специалистов в области материаловедения и защиты от коррозии, техносферной безопасности.
В настоящее время все более популярными становятся автономные системы, предназначенные для решения таких задач, как патрулирование местности, оценка экологического состояния территории, исследование поверхности космических объектов и других. Для выполнения этих задач используются системы управления, способные к ориентированию на местности, следованию заданному маршруту, уклонению от препятствий. При этом могут возникнуть ситуации, когда невозможно использование активного приводного оборудования – радиомаяков, световых маяков и т.д. В таком случае основной упор делается на видеоданные и информацию с бортовых датчиков ближнего действия. Для проектирования мобильной платформы, которая могла бы функционировать в подобных условиях, и разработки сопутствующего ПО необходимы знания в областях обработки изображений, нейронных сетей и нечеткой логики. В связи с этим возрастает потребность в специалистах, способных решать задачи по проектированию подобных систем. Зарубежные университеты предлагают курсы по подготовке специалистов, способных пилотировать и разрабатывать беспилотные летательные аппараты , а также наземные автономные системы . Важную роль в системах управления аппаратами такого типа имеют также системы нечеткой логики . При этом следует отметить, что при изучении предметов в этой области знаний возникает довольно специфическая проблема: для выполнения полноценных практических или лабораторных работ студент должен владеть не только теорией, но и навыками программирования. Однако для различных специальностей и направлений подготовки уровень знаний и навыков в этой области может существенно отличаться, что затрудняет полноценное выполнение работ по этим дисциплинам. Также студенты должны понимать, какое влияние один из компонентов будет оказывать на другой в реальных условиях функционирования разрабатываемой мобильной платформы. Чтобы помочь студентам понять различные аспекты обработки и анализа данных с мобильных платформ, была разработана учебная распределенная система управления мобильной колесной платформой на основе видео- и сенсорной информации. Данная система позволяет анализировать эффективность различных подходов обработки данных и методов их анализа в управлении мобильной платформой. В качестве задачи, поставленной перед автономным аппаратом, было выбрано движение к заданной точке. Точка назначения обозначается графическим маркером, в роли которого может выступать объект специфической формы либо специфического цвета. При движении к цели автономная платформа в своей системе управления использует способности к ориентированию на местности, следованию заданному маршруту и уклонению от препятствий. Описание мобильной платформы Данная разработка является дальнейшим развитием идей, заложенных в полуавтономную колесную платформу, ранее использовавшуюся для решения сходных задач . Аппаратная платформа представляет собой полиуретановое шасси в виде пластины размером мм. Выбор материала обусловлен высокими прочностными и массовыми характеристиками, что позволяет увеличить полезную нагрузку системы. В качестве движетеля используются четыре колеса диаметром 115 мм, независимо приводимые в движение электрическими моторредукторами номинальным напряжением 12 вольт, номинальной частотой вращения и крутящим моментом . На платформе размещено оборудование, предназначенное для ориентирования на местности, и дополнительное оборудование для проведения исследований на местности, в частности, отбора проб грунта. Для определения направления на точку назначения на платформе размещена IP-камера. Управление платформой осуществляет микрокомпьютер Raspberry Pi B+  в связке с микроконтроллером Arduino Duemilanove . Для управления работой двигателей предназначен силовой модуль расширения. Связь Raspberry Pi и Arduino осуществляется через интерфейс USB. На модуле расширения для дополнительного прототипирования установлен модуль датчиков, оборудованный магнетометром и акселерометром. Кроме того, к этому модулю подключены ультразвуковые датчики расстояния, оценивающие расстояние до препятствий перед платформой слева и справа по направлению движения. В центре передней части платформы расположен инфракрасный датчик препятствий, выдающий сигнал в случае, если препятствие находится прямо перед платформой на расстоянии 800 мм и менее. Операционной системой бортового компьютера является Raspbian (Debian Wheezy) . Микроконтроллер управляется специализированным ПО, принимающим информацию от всех датчиков платформы и передающим ее на бортовой компьютер. Кроме того, ПО микроконтроллера принимает команды от компьютера и управляет работой ведущих двигателей в соответствии с этими командами, за исключением случая, когда присутствует сигнал от датчика препятствий по центру платформы и движение платформы вперед блокируется. Питание оборудования платформы обеспечивается свинцово-кислотным аккумулятором напряжением 12 вольт, номинальной емкостью. К разъему USB бортового компьютера подключен USB-WiFi адаптер. В процессе работы бортовой компьютер создает программную точку доступа, что позволяет получать изображение с бортовой камеры и управлять ею, а также получать доступ к телеметрии и управлять движением платформы непосредственно, путем посылки команд Arduino с отдельного компьютера или смартфона, оснащенного модулем WiFi. Основной частью системы является программный комплекс, включающий в себя сервер обработки изображения, поступающего от набортной IP-камеры, нейронной сети, определяющей направление на целевую точку, и системы нечеткой логики, оценивающей информацию от сенсоров и нейронной сети и управляющей перемещениями модуля. Комплекс запускается на бортовом компьютере. Одной из целей при разработке системы было обеспечение ее высокой гибкости. Так, каждый из программных продуктов может работать независимо, а непосредственное взаимодействие компонентов системы осуществляется с помощью сетевых протоколов, в частности, через сетевые сокеты и с использованием Windows Communication Foundation-технологий. Это позволяет легко заменить любой из компонентов системы, например, для определения направления движения платформы может быть применен программный модуль, использующий данные от магнетометра (компаса). Типичное функционирование распределенной системы управления платформой можно представить в виде следующих шагов. Шаг 1. Получение изображения с IP-камеры и локализация в кадре объекта интереса. Шаг 2. Выделение на изображении маркера, по направлению к которому должна двигаться платформа при помощи нейронной сети на основе цветовых или структурных характеристик объекта. Шаг 3. Расчет параметров движения на основе выходов нейронной сети и показаний датчиков с использованием нечеткой логики. Шаг 4. Формирование команд управления мобильной платформой. Обработка графической информации и локализация в кадре объекта интереса За подготовку изображения, поступающего с IP-камеры, к анализу нейронной сетью отвечает сервер обработки изображения, реализованный в среде RAD Studio на языке Delphi. В нем имеется возможность задания последовательности действий, необходимых для обработки изображения, и локализации объекта интереса. Поскольку сервер обработки изображения является частью учебной системы, в нем предусмотрен просмотр промежуточных этапов обработки изображения, а также возможность изменения параметров обработки изображения. Такая возможность позволяет студентам выбрать наиболее оптимальную конфигурацию цепочки методов обработки для локализации объекта интереса. В задачу студентов при работе с сервером обработки изображения входит настройка цепочки методов обработки с целью выделения объекта, к которому необходимо выполнять движение. В связи с тем, что получаемое с камеры изображение не является идеальным (может содержать разнообразные шумы, а также иметь нарушенный цветовой баланс), в сервере обработки изображения реализованы основные фильтры, позволяющие ознакомиться с методами шумоподавления и цветовой коррекции . Так, для изучения студентам доступны механизмы изменения яркости, контрастности и гаммы с возможностью применения их к выбранным компонентам цветовой модели, а также основные методы цветовой коррекции. Для шумоподавления предложены фильтр Гаусса, медианный фильтр и группа линейных фильтров с возможностью задания произвольного ядра. При добавлении фильтров в цепочку обработки в программе предусмотрена возможность настройки параметров, включая выбор цветовой модели, в которой будет осуществляться обработка (RGB, HSV, YUV). Таким образом, студенту будет проще понять различия между цветовыми моделями и их влияние на результат обработки. Для непосредственной локализации объекта предложены набор методов бинаризации и пороговая сегментация  в цветовых моделях RGB, HSV, YUV. При реализации данных методов предусмотрена возможность настройки отображаемого результата. Так, например, результат сегментации можно задать следующим образом: фоновую часть изображения (значения, не соответствующие условию определения объекта) можно закрасить указанным цветом, в то время как сам объект будет иметь оригинальный вид. После получения области с объектом интереса возможно применение дополнительных методов обработки, нацеленных на подготовку данных для передачи нейронной сети, в частности, изменение масштаба. После подготовки изображения оно передается в модуль нейронной сети. Поиск объекта интереса Для выделения в кадре объекта интереса была использована полносвязная трехслойная нейронная сеть типа персептрон. Размер входного слоя сети был выбран равным  нейронам. Это соответствует размеру изображения  пикселей, причем цвет каждого пикселя кодируется тремя 1-байтными значениями (RGB-изображение). Задачей студентов при выполнении практических работ являлось определение числа нейронов во втором слое сети и числа нейронов в выходном слое в зависимости от входных данных следующего модуля (системы нечеткой логики). Поскольку бортовой компьютер платформы не обладает достаточной вычислительной мощностью, обучение нейронной сети производилось на отдельно установленном персональном компьютере с дальнейшей передачей файла уже обученной нейронной сети в бортовой компьютер платформы. Передача осуществлялась через локальную сеть, а точнее, через точку доступа, созданную бортовым компьютером платформы. Обучение нейронной сети производилось методом роя частиц. Для обеспечения переносимости кода между разными платформами (поскольку бортовой компьютер использует ветку Linux, а персональные компьютеры в учебной аудитории оснащены Windows 7) ПО нейронной сети было разработано с использованием платформы Microsoft .Net (язык разработки C#). В среде Linux приложение запускалось с использованием фреймворка Mono . Разработка структуры сети (выбор количества слоев и нейронов в слое) выполнялась на персональном компьютере аудитории в графической оболочке, после чего производилось обучение нейронной сети путем предъявления ей обучающего множества. Это множество формировалось из изображений, полученных с IP-камеры платформы, и желательных значений выходов нейронной сети, заданных пользователем (студентом). Предполагалось, что студенты используют от 2 до 3 выходов нейронной сети, что будет соответствовать ситуациям «маркер цели присутствует в кадре слева»–«маркер цели присутствует в кадре справа» или «маркер цели присутствует в кадре слева»–«маркер цели присутствует в кадре по центру»–«маркер цели присутствует в кадре справа». Значения выходов нейронной сети передавались далее в модуль нечеткой логики при помощи сетевого интерфейса между соответствующими приложениями. Формирование команд управления Для определения направления, в котором должна двигаться мобильная платформа в данный момент времени, использовалась система нечеткой логики. Данный модуль был реализован также на платформе Microsoft .Net на языке C#. Фактически данный модуль состоял из двух независимых приложений, использующих общую библиотеку. Приложение для персонального компьютера использовалось для разработки правил системы нечеткой логики и оценивания их правильности путем подачи значений, заданных пользователем, на вход разработанной системы правил. Разработанная система правил передавалась в бортовой компьютер посредством сети Wi-Fi, созданной точкой доступа бортового компьютера. Приложение для бортового компьютера получало данные из двух источников – от нейронной сети и телеметрию от микроконтроллера Arduino, в частности, дистанцию до препятствий, информацию об ускорениях платформы от акселерометра, а также данные о векторе магнитного поля, в котором находится платформа (на основании которых можно было определить направление движения платформы относительно магнитного поля Земли). На основании сформированного пользователем набора правил система должна была выдавать значение – угол, на который необходимо изменить траекторию движения платформы. Полученное значение посредством сетевого интерфейса передавалось в следующий компонент системы – модуль управления, задачами которого являлись обработка данных, полученных от модуля нечеткой логики, и формирование управляющих команд для микроконтроллера Arduino. При движении платформы использовался следующий алгоритм. 1. Управляющий модуль посылает запрос на получение данных об угле поворота платформы, а также телеметрическую информацию от датчиков модулю нечеткой логики. 2. Модуль нечеткой логики посылает запрос на получение данных нейронной сети. 3. Модуль нейронной сети запрашивает изображение с IP-камеры, преобразует его в вид, пригодный для обработки нейронной сетью, и вычисляет ее отклик. 4. Полученный отклик передается модулю нечеткой логики. 5. Модуль нечеткой логики на основании данных, полученных от нейронной сети, а также от датчиков, определяет угол, на который необходимо развернуть платформу, и передает его управляющему модулю. 6. На основе данных, полученных от модуля нечеткой логики, управляющий модуль формирует команды для микроконтроллера Arduino, которые и посылает ему. Движение осуществлялось по следующему алгоритму: вначале определялся угол, на который необходимо изменить направление движения платформы по описанному выше алгоритму, затем – поворот платформы, после чего платформа смещалась вперед на 150 мм (при условии отсутствия препятствий) и цикл повторялся. На основании изложенного сделаем следующие выводы. Предложенная структура ПО обеспечивает высокую гибкость системы. В частности, каждый из программных продуктов способен работать независимо, с использованием универсальных сетевых протоколов. Это позволяет как создавать распределенную систему (например, для повышения общего быстродействия системы), где каждый программный продукт работает на специально выделенном компьютере, так и легко заменять любой из ее компонентов. Например, при решении ряда задач нейронная сеть, визуально определяющая необходимое направление движения, может быть заменена на модуль, определяющий направление движения с использованием GPS-технологий и сигналов от магнетометра (компаса). Также система управления на основе нечеткой логики может быть заменена на любую другую систему, например, на основе конечных автоматов. Описываемая система была реализована в два этапа с развитием аппаратной и программной баз в период с февраля 2013 г. по август 2014 г. и использовалась при проведении в это время международных летних технических школ. Система была применена для обучения по дисциплинам «Основы обработки изображений», «Нейронные сети» и «Нечеткая логика». Использование показало высокую эффективность предложенной системы и заинтересованность студентов в процессе обучения. Предложенная система может найти применение как для проведения исследований и опытноконструкторских работ в области обработки и анализа изображений, нейронных сетей, нечеткой логики и систем автоматического управления, так и для обучения студентов, бакалавров и магистров соответствующих направлений.
В настоящее время в лечебно-профилактических учреждениях активно внедряются медицинские информационные системы (МИС), и количество таких систем неуклонно растет . Информация, обрабатываемая в МИС, включает в себя персональные данные пациентов, представляющие собой медицинскую тайну. Следовательно, доступ к такой информации должен быть ограниченным и строго контролируемым . Наиболее надежным с точки зрения безопасности является подход, подразумевающий проверку прав доступа пользователей на уровне БД. Во многих СУБД для этого применяются встроенные средства контроля доступа (СКД), в основе которых лежит комбинация дискреционной и ролевой моделей безопасности. В данном случае в качестве защищаемых объектов выступают объекты БД (таблицы, представления и пр.), в качестве субъектов безопасности – пользователи или группы пользователей МИС, а для каждой пары «субъект безопасности–защищаемый объект» явно задается список разрешенных операций (вставка, выборка, редактирование, удаление) . Однако применение такого подхода к защите данных в МИС в чистом виде является недостаточным и неудобным. Прежде всего это связано со специфическими особенностями МИС, к которым можно отнести зависимость прав доступа и большое количество защищаемых объектов. Права доступа зависят от следующих факторов:  время доступа к данным; по прошествии определенного времени доступ на изменение ранее сделанной записи должен быть закрыт;  текущие взаимоотношения врач–пациент– лечащий врач; на время лечения пациента врач должен получить доступ к его медицинским данным в полном или ограниченном объеме;  статус пациента; доступ к информации ряда пациентов должен быть ограничен независимо от других факторов;  место пребывания пациента; некоторые сотрудники подразделения, в которое переводится пациент, должны получать доступ к медицинским данным пациента в полном или ограниченном объеме;  степень конфиденциальности информации; доступ к некоторым медицинским данным пациента должен быть открыт только узкому кругу лиц независимо от других условий. Большое количество защищаемых объектов – доступ в МИС должен ограничиваться не только на уровне таблиц, но и на уровне записей . Также примем во внимание требование предоставлять врачу доступ только к строго ограниченному объему информации о пациенте, которая ему нужна в данный момент времени для выполнения своих должностных обязанностей. Таким образом, для обеспечения этого требования с учетом особенностей, указанных выше, необходимо практически постоянно переопределять права доступа пользователей МИС к данным пациента . Учитывая огромное количество защищаемых объектов в МИС, можно сделать вывод, что обеспечить такой режим работы штатными средствами весьма затруднительно. В любом случае объем работы администратора безопасности существенно увеличится, что неизбежно приведет к ошибкам и несвоевременному переназначению прав доступа. Таким образом, актуальной является задача модификации стандартного механизма СКД в МИС с целью избавления администратора безопасности от большого объема рутинной работы. Для достижения поставленной цели прежде всего необходимо разработать модель системы безопасности обобщенной МИС. Формальное описание новой модели С учетом требований к модели разграничения доступа в МИС была разработана следующая модель. Основные элементы: S – множество субъектов; G − множество групп субъектов; O – множество объектов (права доступа на некоторые объекты могут быть заданы явно, для остальных объектов права определяются динамически); ACL – множество списков контроля доступа (для явного задания прав); список контроля доступа; R − множество прав доступа; решетка уровней конфиденциальности; метка времени, представляющая собой объект, время его создания, предельное время доступа к нему; N – множество меток времени (определяют предельное время изменения объекта); функция, возвращающая значение времени, по истечении которого доступ на изменение объекта o прекращается;функция, определяющая для каждого субъекта права доступа на определенный объект в зависимости от взаимоотношений между ними; множество привилегированных групп, члены которых имеют полный доступ ко всем объектам; функция, определяющая множество прав r группы на объект; множество групп, к которым принадлежит субъект; функция, определяющая множество групп, к которым принадлежит субъект s; функция, определяющая доступность права r субъекта s на объект o; состояние системы; Q – множество состояний системы. Операторами, используемыми в данной модели, являются следующие.  Создать объект с уровнем конфиденциальности , меткой времени . Условие выполнения. Новое состояние системы.  Создать группу с уровнем конфиденциальности, множеством прав доступа на объекты. Условие выполнения. Новое состояние системы.  Добавить право доступа группы на объект путем изменения/добавления списка контроля доступа acl. Условие выполнения. Новое состояние системы.  Удалить право доступа группы на объект путем изменения/удаления списка контроля доступа . Условие выполнения. Новое состояние системы.  Создать субъект, принадлежащий множеству групп. Условие выполнения. Новое состояние системы.  Включить субъект в множество групп. Условие выполнения. Новое состояние системы.  Исключить субъект из множества групп . Условие выполнения. Новое состояние системы.  Включить группу в множество привилегированных. Условие выполнения. Новое состояние системы.  Исключить группу из множества привилегированных. Условие выполнения. Новое состояние системы.  Уничтожить объект. Условие выполнения. Новое состояние системы.  Уничтожить группу. Условие выполнения. Новое состояние системы.  Уничтожить субъект. Условие выполнения. Новое состояние системы.  Определить доступность права субъекта  с уровнем доступа на объект с уровнем конфиденциальности. При использовании данной модели процедура определения доступности объекта выглядит следующим образом. Каждый субъект (пользователь МИС) входит в определенные группы. Группы могут быть привилегированными и непривилегированными. Каждая группа обладает определенным уровнем конфиденциальности. Права доступа субъектов определяются как совокупность прав, явно указанных ему, и прав, указанных для групп. При попытке субъекта совершить определенную операцию над объектом происходит проверка доступности данной операции. Если пользователь входит в одну из привилегированных групп, он имеет полный доступ к любому объекту. Иначе происходит проверка меток времени. Если текущее время превышает предельное время доступа к объекту, субъект не имеет права изменять объект. Следующим шагом является проверка явно указанных прав и меток конфиденциальности. Если одна из групп, в которые входит пользователь, обладает правами на данную операцию и уровень доступа субъекта больше либо равен уровню конфиденциальности объекта, доступ гарантируется. Вторым условием гарантии доступа является наличие возможности осуществления данной операции исходя из взаимоотношений между объектом и субъектом непосредственно (при этом не учитываются уровни конфиденциальности). Таким образом, в системе заведомо заводятся необходимые группы со всеми атрибутами и списком прав доступа. Наличие меток конфиденциальности обусловливается обширным списком условий, по которым доступ должен или не должен предоставляться, а также большим количеством объектов. Разделение пользователей на группы необходимо для того, чтобы разграничивать права в зависимости от должности пользователя и места пребывания пациента. Для реализации представленной модели авторы предлагают использовать механизм триггеров, в которых должна быть реализована логика расширенной проверки прав пользователя на выполнение операции в соответствии с разработанной моделью. Если пользователь имеет право выполнить инициированную им операцию, которая вызвала срабатывание триггера, операция беспрепятственно завершается, в противном случае операция отменяется (см. рисунок). Для автоматизации процедуры создания и редактирования триггеров авторы разработали программу «Консоль администратора безопасности». Для удобства администрирования данная программа позволяет выполнять стандартные операции по управлению пользователями БД, а также дает возможность определять дополнительные параметры безопасности, предусмотренные предлагаемой моделью. При разработке программы ис- пользовались язык C# и технология .Net. На взгляд авторов, применение предложенной модели позволит существенно сократить объем работы администратора безопасности и, как следствие, снизить вероятность возникновения ошибок настройки прав доступа.
С развитием информационных технологий большое внимание уделяется автоматизированным способам анализа и обработки информации. Создание программного комплекса для автоматизированного учета и прогнозирования состояний больных описывалось в . Одним из важных модулей разработанного программного комплекса является модуль анализа и обработки данных о состоянии здоровья пациента на основе результатов анализов. В данной статье представлены два связанных способа обработки данных: многофакторный анализ данных с использованием метода главных компонент (МГК) . С помощью этих математических методов обрабатываются данные анализов пациента и на их основе прогнозируется правильность лечения с использованием выбранного целевого параметра во времени. На основе значений целевого параметра лечащий врач принимает решения о дальнейших назначениях и путях лечения пациента. В ходе использования и развития программного комплекса выявлено, что входящими данными для анализа и обработки являются 16 параметров (данные анализов пациентов). В данной статье входящие параметры обозначим. Результат обработки информации – предсказанное значение целевого параметра, изменяемое во времени. Задачей модуля программного комплекса является прогнозирование изменения значений целевого параметра во времени исходя из значений входящих параметров X1–X16. Методы анализа и обработки данных Многофакторный анализ данных с использованием МГК. МГК дает возможность от непосредственно измеряемых факторов перейти к их некоррелированным линейным комбинациям, которые называют принципиальными компонентами и дисперсии которых убывают. Коэффициентами линейных комбинаций, которые называют нагрузками i-й переменной в j-й компоненте, являются элементы собственных векторов матрицы ковариаций. Дисперсии компонент будут равны собственным числам матрицы ковариаций . Геометрически нахождение главных компонент сводится к переходу к новой ортогональной системе координат. Первую координатную ось определяют так, чтобы соответствующая ей линейная комбинация извлекла возможно большую дисперсию. Вторую ось строят ортогонально первой таким образом, чтобы извлечь наибольшую часть от оставшейся дисперсии. Все оставшиеся компоненты определяют аналогично. Таким образом, все компоненты ортогональны друг другу. От новых координат всегда можно перейти к начальным, где  главная компонента;  масса j-й компоненты в i-й переменной. Доля дисперсии, выраженная в процентах, объясняемая j-й компонентой, определяется следующим образом, где собственные значения дисперсионно-ковариационной матрицы. В ряде задач МГК дает возможность значительно снизить размерность задачи за счет того, что линейные комбинации (компоненты), имеющие маленькие дисперсии, отбрасываются, а анализируются лишь линейные комбинации с большими дисперсиями , обычно не менее 80 % от общей дисперсии. Данные, полученные в результате экспериментальных исследований, были обработаны с использованием МГК . На рисунке 1 представлена диаграмма, отражающая статистические параметры обрабатываемых данных. Из диаграммы видно, что данные сильно различаются по шкалам и требуется их дополнительная обработка. Для оценки того, сколько компонент необходимо для описания данных с заданной точностью (не менее 90 % дисперсии), а сколько можно отбросить и не учитывать в дальнейшем, была построена диаграмма Парето, приведенная на рисунке 3. Из диаграммы видно, что для описания 90 % дисперсии достаточно учесть первые 10 компонент . Рассмотрим их подробнее. Перед применением МГК данные предварительно центрировались и шкалировались относительно стандартного отклонения. Нахождение компонент проводили с использованием самостоятельно разработанного программного пакета. Статистические параметры данных после их обработки показаны на рисунке 2.  Для визуального представления расположения данных в проекционном пространстве были построены графики счетов для первых десяти компонент (примеры для первых четырех компонент приведены на рисунке 4). Графики счетов, а также значения коэффициентов перехода к системе координат на основе главных компонент позволяют наглядно представить, как распределены данные, что особенно важно при формировании обучающей и тестовой выборок с помощью различных математических моделей. На графике счетов для первых двух компонент (компоненты 1 и 2 на рис. 4) отсутствует явное разделение данных на группы. На графике счетов для вторых двух компонент (компоненты 3 и 4, рис. 4) данные образуют две большие группы, причем данные в них распределены практически равномерно. Из приведенного анализа данных следует отметить, что, формируя обучающие и тестовые выборки, при разработке математического описания необходимо учитывать не только факторы, которые варьировались в ходе проведения эксперимента, но и ряд характеристик полученных образцов. Обучающая и тестовая выборки должны содержать данные, отличающиеся по этим показателям. Многофакторный анализ данных позволил заключить, что наибольшее влияние на изменение свойств исследуемого объекта оказывают следующие условия (приводятся в порядке убывания значимости). Наглядное представление о связи переменных между собой и их взаимном влиянии дают графики нагрузок – векторные графики в виде двухмерных или трехмерных проекций, приведенные на рисунках 5 и 6. Данные графики читаются следующим образом: чем ближе векторы расположены относительно друг друга, тем большая положительная корреляция имеется между данными; если векторы направлены в диаметрально противоположные стороны, между данными существует сильная обратная корреляция. Из анализа графиков нагрузок можно заметить следующее. На рисунке показатели морфофункционального статуса эритроцитов попарно коррелированы между собой (пары) и имеют обратную зависимость: увеличение значений показателей и будет сопровождаться снижением значений показателей. Причем более низкие будут способствовать увеличению показателей, входящих в первую пару. На рисунке между собой положительно коррелированы такие показатели. Между этими двумя показателями и имеется сильная обратная корреляция. На рисунке более высокий, более высокий  и больший  приводят к увеличению  и к большему . На рисунке наблюдается корреляция между  и : при меньшем  меньший . На основе многофакторного анализа было принято, что в качестве входных факторов при построении нейросетевой модели должны быть взяты , , ,  в связи с отсутствием явной корреляции между ними. Остальные параметры сильно коррелированы между собой и зачастую могут быть выражены функционально относительно друг друга. Таким образом, использованный метод многофакторного анализа (МГК) позволил сократить размерность задачи, выявил требования к формированию обучающей и тестовой выборок для построения математической модели на основе нейронной сети, а также позволил определить ключевые факторы, которые должны быть использованы в качестве входных параметров в нейросетевой модели. Многообразие существующих архитектур нейронных сетей позволяет использовать методы нейроинформатики для решения практически любых классов задач. Чаще всего среди этих задач фигурируют аппроксимация данных, прогнозирование временных рядов, математическое моделирование свойств объектов, распознавание образов, классификация, кластеризация данных, управление. Искусственные нейронные сети – достаточно сложный математический аппарат. Для их использования зачастую требуются большие объемы исходной информации и значительные вычислительные ресурсы. Поэтому для решения достаточно простых задач целесообразно использовать другие известные и широко применяемые методы. Определим основные понятия нейронных сетей. Искусственный нейрон – это элементарная структурная единица искусственной нейронной сети, выполняющая функции по обработке входных сигналов, поступающих с других нейронов, и представлению результата в форме выходного значения. Для решения задачи прогнозирования изменения значений целевого параметра во времени были спроектированы и протестированы нейронные сети. Структура нейронной сети, наиболее подходящей для решения задачи , представлена на рисунке 7. Предложенная нейронная сеть была обучена на обучающих выборках (из 200 экспериментов) и проверена с помощью тестовых выборок (из 50 экспериментов). Выходным показателем для нейронной сети был выбранный целевой параметр. На рисунке 8 показано сравнение расчетных и экспериментальных данных во времени. Использование нейронных сетей. Методы искусственного интеллекта – это современное направление развития методов математического моделирования свойств объектов, динамических процессов и поведения систем. Одним из широко применяемых методов являются нейронные сети . Ошибка предсказания значения целевого параметра не превышала 8 %. Это дает возможность использовать обученную нейронную сеть для предсказания значений целевого параметра во времени при различных значениях показателей. Расчет по нейронным сетям проводился с помощью созданного ПО, скриншоты которого представлены на рисунке 9. ПО было реализовано на языке программирования C#. Для его разработки и отладки использовалась среда разработки Visual Studio. Разработанное ПО может быть исполнено на обычном рабочем компьютере и позволяет обработать массив данных до 500 экспериментов за 10 минут. В заключение отметим, что в статье была проиллюстрирована возможность применения двух современных математических подходов к анализу и обработке данных: многофакторного анализа данных с использованием МГК и нейронных сетей. Данные методы хорошо показали себя при обработке большого массива разнородных данных и выявлении корреляций между данными. Для применения этих методов было разработано ПО, которое позволило обработать исходные данные (анализы пациентов) и на их основе предсказать значение целевого показателя. 184
Традиционно автоматизированная интерпретация результатов исследования органов дыхания опирается на методы, определяющие структурные изменения в объекте (рентгенографию, томографию). Они позволяют обоснованно формулировать диагностическое заключение, но являются затратными (томография) и требуют повышенных мер безопасности при эксплуатации аппаратуры (рентгенология). Учитывая, что заболевания органов дыхания могут развиваться довольно быстро и носят инфекционный характер, особое значение приобретают инструментальные средства для проведения массовых обследований. Очевидно, что улучшение их технических характеристик будет способствовать повышению точности постановки диагноза на ранних стадиях заболеваний. Наиболее распространенными среди скрининговых методик являются методики аускультации. В последние годы в этой области наблюдается значительный прогресс, обусловленный появлением электронных стетоскопов. Эти устройства не только позволяют повысить точность оценки акустических признаков, но и обеспечивают регистрацию дыхательных шумов (ДШ), что способствует переходу к методикам доказательной медицины. Электронные стетоскопы дают возможность применять компьютерные технологии и для решения задач, связанных с автоматическим анализом слабых акустических сигналов, регистрируемых на корпусе человека. Программные средства, применяемые в электронных стетоскопах На данный момент устройства регистрации звуков дыхания технически достаточно совершенны. Среди наиболее значимых серийных линеек электронных стетоскопов можно выделить 3M Littmann, Cardionics e-scope, Contec cms-ve, Adc adscope, Welch Allyn elite, Think Labs . Они обладают близкими функциональными характеристиками: имеют интерфейсы для передачи данных на другой регистратор или на устройство обработки (ПК, планшет и т.д.) и память для сохранения нескольких записей. Главные различия моделей связаны с характеристиками первичных преобразователей акустического сигнала в электрический. Звуковые сенсоры (датчики), применяемые в стетоскопах различных производителей, могут быть разных типов (микрофонного, конденсаторного, пьезо-электрического), что приводит к значительным отличиям в их частотных характеристиках . Практически все представленные на рынке электронные стетоскопы поставляются с собственным ПО, которое определяет функции обработки и интерпретации зарегистрированных акустических сигналов. 1. Программа 3M Littmann StetshAsist поставляется в комплекте с цифровыми стетоскопами Littmann. Основное ее назначение – визуализация результатов исследований, захват и сохранениезвуковых записей со стетоскопа, преобразование их в различные аудиоформаты (wav, mpeg layer 3). Программа выполняет расчет и визуализацию спектрограмм легочных и сердечных шумов, обеспечивает возможность сохранения результатов в специальном формате, включающем, помимо самих звуковых записей, метаинформацию об условиях проведения исследования, локализации точек регистрации и т.п. 2. Программа 3M Littmann Scope-to-Scope TeleAuscultation Software ориентирована на задачи телемедицины, позволяет передавать от стетоскопа цифровой зашифрованный сигнал через сети передачи данных к другому стетоскопу либо к компьютеру. 3. Программа Welch Allyn Elite Analyzer по своему функционалу аналогична 3M Littmann StetshAsist. Ее отличительная особенность – возможность отображения комбинации фонокардиограммы с синхронизированным сигналом электрокардиографа. 4. Программа Think Labs Phonocardiography Software основана на свободно распространяемом ПО Audacity. Программное средство обладает наиболее богатым набором возможностей среди прочего рассмотренного ПО и обеспечивает следующее: запись и воспроизведение звуков, импорт звуковых записей из различных форматов; визуализацию записей в виде спектрограммы или амплитудных изменений звука, возможность усиления отдельных участков записей, контроль скорости воспроизведения звука, возможность применения различных фильтров; возможность группировки записей пациентов; возможность добавления метаинформации с данными о заболевании к звуковым файлам и анализа частотного состава записей при помощи быстрого преобразования Фурье. 5. Программа Viscope Visual Stethoscope Software является автономной и не связана с какимлибо производителем стетоскопов. Ее основные возможности – масштабирование графика амплитудных изменений звука, отображение записей нескольких пациентов для визуального сравнения и оценки длительности отдельных участков записей. Проведенный анализ технических и программных характеристик современных электронных стетоскопов показывает, что эти устройства позволяют с высокой точностью регистрировать, усиливать, оцифровывать, фильтровать и записывать результаты аускультативных исследований органов дыхания человека. Однако интерпретация результатов исследования, как и в случае с обычными стетоскопами, во многом зависит от врача, его опыта и ряда других субъективных факторов. ПО электронных стетоскопов не включает функционал автоматической классификации регистрируемых звуков, следовательно, не может осуществлять автоматическую генерацию расширенной интерпретации результатов анализа ДШ. Предпосылки к автоматической интерпретации записей ДШ Проведенный анализ особенностей ПО регистраторов звуков дыхания выявил необходимость создания специализированного ПО, которое будет интегрироваться с ПО электронных стетоскопов и обеспечит применение современных методов анализа и классификации слабых акустических сигналов. Значимые результаты по исследованиям автоматической классификации ДШ можно найти в работах . Большинство из них акцентируют внимание на способах выделения разделительных классификационных признаков, а также на методиках регистрации звуков дыхания. Непосредственно сама процедура классификации, как правило, отодвигается на второй план, зачастую исследователи ограничиваются применением классических алгоритмов. Построение классифицирующих систем входит в перечень задач, решаемых совокупностью методов автоматического анализа, за которыми закреплено собирательное название Data mining, в отечественных источниках также применяется название «Интеллектуальный анализ данных», ИАД . Одной из главных особенностей этого направления является большая зависимость результатов классификаций от качества обучающих выборок, то есть от набора данных, на основе анализа которого формируются решающие правила, деревья вывода и другие результирующие формы. Особенности выборок ДШ исследованы в работах . Анализ результатов показывает существенную зависимость данных от некоторых характеристик стетоскопов. Очевидно, при построении автоматического интерпретатора записей ДШ необходимо учитывать такие технические характеристики регистраторов сигналов, как амплитудночастотную характеристику, частоту дискретизации, уровень квантования сигнала, регистрируемого стетоскопом. Различия в этих признаках приводят к существенным вариациям сигналов, зарегистрированных при одной и той же патологии, что создает большие сложности для формирования обучающих выборок и построения универсального классификатора ДШ. Выявленное техническое противоречие можно преодолеть с помощью специальных средств адаптации программы интерпретатора к модели электронного стетоскопа. Архитектура системы автоматической интерпретации записей ДШ с адаптацией к средствам регистрации сигналов Рассмотрим архитектуру программной системы, решающей задачи анализа акустических шумов с адаптацией правил интерпретации к средствам регистрации сигналов. Система предназначена для расширения функциональных возможностей программ, поставляемых в комплекте с электронными стетоскопами. В ее структуре можно выделить четыре обязательных компонента:  модуль сопряжения;  подсистема подготовки моделей объектов классификации;   подсистема настройки (адаптации) к модели регистратора сигналов;  классификатор сигналов. Один из вариантов такой системы, интегрирующей аппаратно-программный комплекс электронного стетоскопа Littmann 4100 (1) и средства интерпретатора (2), показан на рисунке 1.  Электронный стетоскоп (Littmann 4100)  ПО регистрации результатов исследования (Littmann StethAssist) Аудиофайлы Модуль сопряжения Загрузка сырых данных  Подсистема подготовки данных Модуль функциональных преобразований Выделение дыхательных циклов Выделение СПМ Понижение размерности  Фильтрация  Ядро классификатора Редактор выборки  Хранилище описаний объектов Подсистема настройки на стетоскоп Редактор ЛП  Модуль фаззификации 1  ЛП  Правила  Модуль фаззификации 2  Модуль автоматической генерации продукционных правил  Модуль логического вывода  Модуль представления результатов Модуль сопряжения выполняет роль интерфейса, обеспечивая загрузку файлов с записями ДШ, созданных с помощью электронного стетоскопа. Учитывая, что программы этих устройств обеспечивают регистрацию результатов исследования не только в собственном формате, но и в форматах, доступных для воспроизведения стандартными средствами (форматы wav, mp3, txt), набор конверторов получается довольно ограниченный. Подсистема подготовки моделей объектов классификации предназначена для формирования различных описаний ДШ в пространстве дискретных признаков. Она решает задачи первичной обработки данных: фильтрации, нормализации, расчета спектральной плотности мощности (СПМ) и аттрактора, а также формирование на основе аттрактора вектора вторичных признаков. Подсистема настройки (адаптации) к модели регистратора сигналов позволяет производить адаптацию интерпретатора под конкретные модели стетоскопов. Ее основное назначение – генерация продукционных правил, которые используются базовым ядром интерпретатора для решения задачи классификации образца ДШ, поступившего от электронного стетоскопа. Все правила имеют унифицированную структуру, в качестве консеквента в ней используется наименование класса, который может совпадать с видом патологии, а для формирования антецедента используется модель класса спектров ДШ. Особенность этих моделей заключается в использовании нечетких переменных для представления спектров ДШ. Экспериментально доказано, что под каждую модель стетоскопа необходима генерация собственных вариантов классов спектров ДШ . Для решения этих задач в состав подсистемы настройки включены редакторы лингвистических переменных (ЛП) и функций принадлежности (ФП), модуль фаззификации и модуль автоматической генерации продукционных правил. Учитывая, что разные модели стетоскопов могут обеспечивать разные уровни громкости записей ДШ, количество лингвистических переменных, необходимых для фаззификации ДШ, а также границы нечетких множеств для каждого терма могут редактироваться с учетом размаха выборки моделей ДШ, построенных с использованием СПМ. Переход от рассчитанного спектра мощности ДШ к модели, представленной совокупностью нечетких признаков, описывающих ординаты СПМ, осуществляется с помощью модуля фаззификации 1. В состав этой подсистемы включен также специальный редактор функциональных преобразований, с помощью которого можно создавать модели ДШ на основе линейных и нелинейных преобразований, включающих построение аттракторов, вычисление длительности дыхательных циклов и т.п. Основным компонентом подсистемы настройки является модуль автоматической генерации продукционных правил. Его назначение – генерация нейроподобной иерархической структуры (НИС), задающей обобщенные представления обо всех объектах анализируемой обучающей выборки. НИС отражает характерные закономерности классов, представленных в выборке . С помощью каждой модели стетоскопа создается своя обучающая выборка ДШ и генерируется отдельный вариант НИС, которая затем конвертируется в соответствующий набор классификационных правил. Классификатор сигналов (ядро интерпретатора) составляют модули, инвариантные к виду стетоскопов: редактор обучающей выборки, модуль фаззификации 2, модуль логического вывода и модуль представления результатов. С помощью редактора обучающей выборки можно просматривать и редактировать загруженную выборку объектов – моделей ДШ. Модуль фаззификации 2 по своему предназначению аналогичен модулю фаззификации 1. Модуль нечеткого логического вывода решает задачу классификации с помощью построенных продукционных правил и вычисляет значение функции соответствия объекта описанию класса. Модуль представления результатов отображает информацию о функционировании интерпретатора: статистику по результатам классификации наборов данных, детализированные результаты системы нечеткого логического вывода, структуру построенной НИС, структуру сгенерированных продукционных правил. Информация отображается в табличном, текстовом либо графическом виде. Методика настройки интерпретатора на модель стетоскопа Настройка программы интерпретатора на модель стетоскопа заключается в формировании правил классификации на основе обучающей выборки, созданной с помощью указанного регистратора. Учитывая, что средства генерации правил инвариантны к модели стетоскопа, основные этапы методики связаны с формированием обучающей выборки (рис. 4). Данные аускультативного исследования могут быть получены различными способами, поэтому в программе предусмотрена возможность указания параметров источника данных и способа их получения. Для этого (этап 1) указывается тип специфичного модуля-приемника входных данных (wav, mp3, txt). Для описания ДШ можно использовать различные системы признаков. Средства подготовки данных (этап 2.1) предусматривают расчет частотных признаков (спектров мощности, автокорреляционной функции, кепстральных коэффициентов), временных признаков (длительность фаз дыхательного цикла). Для полученных таким образом групп признаков определяются тип признаков (однородные/неоднородные) и наличие взаимосвязи (коррелированные/некоррелированные) (этап 2.2). Если признаки являются однородными и взаимосвязанными, допускается выполнение процедуры укрупнения признаков, что позволяет сократить размерность описания объектов, а также может привести к улучшению качества классификации . После представления исходных признаков в наиболее подходящем для анализа виде следует преобразовать их к нечеткому представлению. При этом преобразование к нечеткому виду должно быть задано исходя от решаемой задачи и определяться структурой ЛП признака (этап 3). После выполнения всех перечисленных этапов следует запустить процедуру построения ядра НИС с последующим выделением продукционных правил (этап 4). Эксперт, анализируя полученные на предыдущем этапе продукционные правила, может (этап 5) корректировать структуры ЛП признаков для увеличения точности классификации (этап 5.1). После выполнения всех этапов настройки возможны загрузка рабочих выборок объектов, расчет и визуализация результатов их интерпретации (этап 6). Результаты испытаний системы автоматической интерпретации записей ДШ с адаптацией к средствам регистрации сигналов Испытания системы были произведены с использованием трех разных устройств регистрации ДШ: 3M Littmann 4100, авторского устройства  и устройства цифровой электронной аускультации КoРА-03М1 . Регистрация ДШ осуществлялась в трех точках корпуса пациента. В качестве испытуемых выступали как здоровые люди, так и пациенты с патологиями органов дыхания. С помощью субъективной классификации экспертом (врачом высокой квалификации с хорошим состоянием органов слуха) из всех зарегистрированных записей сформированы два множества образцов ДШ (норма, патология). Электронные стетоскопы, участвовавшие в испытаниях, имеют существенные различия в своих характеристиках. С помощью устройства 3M Littmann 4100 регистрировались образцы шума с частотным диапазоном 0–4 кГц, при частоте дискретизации 8 кГц и разрешении 16 бит. Записи аворского устройства имеют частотный диапазон 0–5кГц, частоту дискретизации 11025 Гц, разрешение 16 бит. Коллекция записей ДШ, сделанная с помощью устройства КoРА-03М1, иллюстрирует легочные шумы в частотном диапазоне 0–3 кГц при частоте дискретизации 4 кГц и разрешении 16 бит. Состав обучающей выборки (ОВ) и тестовой выборки (ТВ) приведен в таблице 1. Классификация записей проводилась по ОВ и ТВ, которые не пересекались. Анализ результатов работы программы выявил существенные различия в структурах НИС-классификаторов , сформированных для разных средств регистрации шумов (табл. 2). Однако варианты сгенерированных правил показали близкие по точности результаты работы интерпретаторов ДШ для каждой модели электронного стетоскопа (табл. 3). В заключение отметим, что предложенная методика и программные средства позволяют существенно расширить возможности ПО, поставляемого в комплекте с электронными стетоскопами. Дальнейшее развитие этого направления исследований позволит перейти к классификации ДШ по видам патологий и будет способствовать повышению качества аускультативного исследования органов дыхания.
При проектировании программных продуктов необходимо учитывать требования, пожелания, а также знания и возможности потенциального контингента пользователей, которые отражаются в техническом задании на ПО. Для получения единого представления о создаваемом ПО предусматривается активное взаимодействие разработчика с заказчиком. Заказчики разрабатывают концепцию (часто подсознательную и неполную) того, как их приложение будет работать. Разработчикам же необходимо учитывать аппаратные ресурсы, базовое ПО, операционную систему и другое. Кроме того, требуется знать возможности целевой аудитории пользователей, варианты использования ПО или сценарии работы, которые формируются из пользовательских историй. Одним из способов взаимодействия с заказчиком и получения единого видения ПО является разработка прототипа интерфейса . Прототип пользовательского интерфейса представляет собой макет (черновую, пробную версию) интерфейса, разрабатываемый с целью проверки пригодности предлагаемых для применения концепций, технологических решений, а также для представления программы заказчику на ранних стадиях процесса разработки.  Постановка задачи Задача заключается в разработке программного средства с функцией поддержки принятия решения по подборке альтернативного варианта шаблона интерфейса на основании экспертной оценки и теории нечетких множеств. Для автоматизации принятия решения необходимо разработать шаблоны пользовательского интерфейса с учетом целевой аудитории . Под шаблоном пользовательского интерфейса будем понимать сочетание одного или нескольких элементов, широко используемых для предоставления навигации, команд и содержимого, которые используются большинством приложений и служат основой для разработки собственного пользовательского интерфейса. Характеристиками контингента пользователей являются их уровень знаний, физические и психологические качества. Каждая из характеристик оценивается по трехбалльной шкале: высокий, низкий и средний уровень развития. С учетом разных градаций выделенных характеристик пользователи разделены на пять групп  Декомпозиция функций программной системы представлена в нотации IDEF0 (Integrated DEFinition) на рисунке 1.  Входными данными программной системы являются характеристики групп пользователей, требования заказчика, лицензия разработчика, лицензия заказчика, шаблоны интерфейсов. В качестве управляющего воздействия выступают Федеральный закон «О персональных данных», ГОСТ 19.201-78, экспертные оценки специалистов в данной предметной области, нечеткие отношения. На выходе программной системы формируется фрагмент технического задания на разработку ПО с прототипом интерфейса. Реализация поставленной задачи Входные данные для задачи проектирования интерфейса программных средств характеризуются той или иной степенью неопределенности, обусловленной неполнотой, внутренней противоречивостью, неоднозначностью, и представляют собой приближенные количественные или качественные оценки параметров процессов проектирования и управления проектированием . Таким образом, так как исходные данные задачи трудно формализуемы, целесообразно применить один из методов искусственного интеллекта, основанный на нечеткой логике. Нечеткие алгоритмы, оперирующие лингвистическими переменными, значения которых задаются нечеткими множествами, удобны для описания слабо формализуемых процессов. Такие алгоритмы интуитивно более понятны. Их автоматизация позволяет повысить объективность и оперативность решений, принимаемых разработчиком ПО . Методология нечеткого логического вывода достаточно успешно применяется при построении систем управления объектами , в частности, при разработке компонентов ПО  и минимизации рисков программных проектов . Этап 1. Формирование и оценка компетентности группы экспертов. Экспертная оценка – процедура получения оценки проблемы на основе мнения специалистов с целью принятия решения. Экспертное оценивание предполагает создание некоего разума, обладающего большими способностями по сравнению с отдельным человеком. Основной сложностью этого метода является подбор экспертов, которые должны иметь опыт в соответствующих решаемым задачам областях. При подборе экспертов следует учитывать личную заинтересованность, которая может стать существенным препятствием для получения объективного суждения. При формировании группы экспертов на стадии выявления знаний учитывались следующие характеристики:  компетентность (степень квалификации эксперта в данной области знаний);  креативность (способность решать творческие задачи);  отношение к экспертизе (негативное или пассивное отношение, занятость, существенно влияющие на качество работы эксперта в группе);  конформизм (подверженность влиянию авторитетов, при котором их мнение может подавлять лиц, обладающих более высокой компетентностью);  коллективизм и самокритичность. Авторами реализован один из возможных путей количественного описания характеристик эксперта, основанный на вычислении относительных коэффициентов компетентности по результатам высказываний специалистов о составе экспертной группы. Суть методики сводится к тому, что ряду специалистов предлагается высказать мнение о списочном составе экспертной группы. Если в этом списке появляются лица, не вошедшие в исходный список, им тоже предлагается назвать специалистов для участия в экспертизе. После нескольких этапов будет получен достаточно полный список кандидатов в группу . По результатам опроса составляется матрица, по строкам и столбцам которой записываются фамилии экспертов, а элементами таблицы являются переменные:  если j -й эксперт назвал i-го;  если j -й эксперт не назвал i-го. При этом эксперт может включать или не включать себя в экспертную группу  По данной таблице можно вычислить относительные коэффициенты компетентности, используя алгоритм решения задач о лидере. Относительные коэффициенты компетентности h-порядка для каждого эксперта имеют следующий вид: где m – число экспертов в списке (размерность матрицы ║xij║); h – номер порядка коэффициента компетентности. Коэффициенты компетентности нормированы так, что их сумма равна единице: По формуле можно вычислить значение компетентности для различных порядков, начиная с первого. При выражение будет иметь Таким образом, коэффициент компетентности первого порядка – это относительное число экспертов, высказавшихся за включение i-го эксперта в группу. Относительный коэффициент компетентности второго порядка получаем из для при условии, что  определены по: Коэффициенты второго порядка представляют собой относительное количество голосов взвешенных коэффициентов компетентности первого порядка. Последовательно вычисляя относительные коэффициенты компетентности более высокого порядка, можно убедиться, что процесс быстро сходится после 3-4 вычислений, то есть относительные коэффициенты быстро стабилизируются . В общем случае коэффициенты относительной компетентности определяются как  Этап 2. Групповая экспертная оценка объектов при непосредственном оценивании. Пусть m экспертов провели оценку n объектов по  показателям. Результаты оценивания представлены величинами , где  – номер объекта;  – номер эксперта;  – номер показателя. Величины , полученные методом непосредственного оценивания, представляют собой числа из некоторого отрезка числовой оси или баллы. В качестве групповой оценки для каждого из объектов можно принять среднее взвешенное значение его оценки:   где  – коэффициенты весов показателей сравнения объектов;  – коэффициенты компетентности экспертов. Величины  и  являются нормированными Коэффициенты  могут быть определены экспертным путем как средний коэффициент веса При алгоритм вычисления групповых оценок и коэффициентов компетентности экспертов имеет вид: а) начальные условия при то есть начальное значение ко эффициентов компетентности для всех экспертов принимается одинаковым; б) рекуррентные соотношения для  представлены в  Этап 3. Построение нечеткой модели на бинарных нечетких отношениях. Пусть имеются два множества: совокупность групп пользователей и совокупность шаблонов интерфейсов, которые нужно максимально эффективно подобрать для пользователей с заданными характеристиками. Таким образом, входными данными являются указанные множества, а выходными – степени соответствия шаблонов интерфейсов пользователям. В общем случае нечетким отношением или, точнее, нечетким -арным отношением, заданным на множествах (универсумах)  называется некоторое фиксированное нечеткое подмножество декартова произведения этих универсумов. Другими словами, если обозначить произвольное нечеткое отношение через , то по определению   функция принадлежности данного нечеткого отношения, которая определяется как отображение Здесь через обозначен кортеж из  элементов, каждый из которых выбирается из своего универсума: Бинарное нечеткое отношение задается на базисных множествах  и определяется как нечеткое отношение  Здесь функция принадлежности бинарного нечеткого отношения, которая определяется как отображение, а через обозначен кортеж из двух элементов, при этом  Кроме того, для построения решения задачи необходимы композиции нечетких бинарных отношений. Пусть  и  – конечные или бесконечные бинарные нечеткие отношения. Причем нечеткое отношение задано на декартовом произведении универсумов, а нечеткое отношение – на декартовом произведении универсумов. Нечеткое бинарное отношение, заданное на декартовом произведении и обозначаемое через, называется композицией бинарных нечетких отношений  и , а его функция принадлежности определяется следующим выражением: Определенную таким образом композицию бинарных нечетких отношений называют композицией или максиминной сверткой нечетких отношений. Нечеткое бинарное отношение, заданное на декартовом произведении и обозначаемое через, называется композицией бинарных нечетких отношений  и , если его функция принадлежности определяется следующим выражением:  В частности, если в этом выражении вместо операции использовать операцию алгебраического умножения, получим определение композиции. Нечеткое бинарное отношение, заданное на декартовом произведении и обозначаемое через , называется композицией бинарных нечетких отношений  и , если его функция принадлежности определяется следующим выражением:   С учетом введенных понятий построим нечеткую модель, основанную на двух бинарных нечетких отношениях  и . Первое из этих нечетких отношений строится на двух базисных множествах  и , а второе – на двух базисных множествах  и . Здесь описывает множество интерфейсов множество характеристик пользователей, множество групп пользователей. В данном контексте нечеткое отношение  содержательно описывает характеристики интерфейсов, а характеристики пользователей. Элементы универсумов имеют следующий содержательный смысл:  – шаблон интерфейса  – шаблон интерфейса  – шаблон интерфейса  – шаблон интерфейса  – шаблон интерфейса   – компьютерная грамотность, – системный опыт, – опыт работы с подобными программами, – машинопись, – мышление, – память, – моторика, – дальтонизм, – концентрация внимания,  – эмоциональная устойчивость;  – новичок, – обычный пользователь, – уверенный пользователь, – квалифицированный, – программист, хакер, администратор. Для определения соответствия интерфейса группе пользователей воспользовались композициями исходных нечетких отношений. Так, композиции дают информацию о степени соответствия шаблона интерфейса группе пользователей, а композиция позволяет определить шаблон, который не подходит для данной группы пользователей . Таким образом, применяются три модели: Способы для определения результата композиции нечетких отношений могут быть следующими.  Результаты применения разработанной методики Рассмотренная методика реализована в программном средстве «Автоматизированная информационная система составления технического задания с экспертной оценкой принятия решения», предназначенном для поддержки руководителя программного проекта при разработке технического задания на ПО с подбором альтернативного варианта интерфейса . Главное окно программного средства показано на рисунке 2.  Одной из функций разработанного программного средства является тестирование пользователя, реализованное в форме «Оценка пользователем», в которой можно выбрать характеристику и ее градацию из ниспадающих меню  Оценка пользователя экспертами (выбор его характеристик и их градаций) основывается на результатах тестирования пользователя и проводится  в форме «Оценка пользователя экспертами»  Результаты вычислений выводятся в окне «Анализ»  По нажатии кнопки «Сохранить отчет» генерируется документ, в котором заполнены следующие разделы: наименование ПО, материально ответственное лицо (руководитель проекта), наименование заказчика, характеристики пользователя, рекомендуемый шаблон интерфейса, назначение и область применения ПО, требования к функциональным характеристикам ПО, требования к составу и параметрам технических средств, требования к информационной и программной совместимости, состав программной документации, стадии, этапы разработки и сроки сдачи ПО. Во вкладке «Справочники» обеспечивается работа с информацией: должности, тип юридического лица, тип предметной области, тип характеристик пользователя, тип шаблона, физическое лицо, группа пользователей, единицы измерения. Эту информацию можно корректировать, добавлять и удалять. Заключение Разработанное программное средство ведет базу знаний о различных категориях пользователей с учетом их компьютерной грамотности, моторики, памяти, мышления, дальтонизма, концентрации внимания и др. На основе этих характеристик разрабатываются различные варианты пользовательского интерфейса, которые согласуются с заказчиком и реализуются в конечном ПО. Основными результатами работы является программное и математическое обеспечение для решения задач интеллектуального проектирования пользовательского интерфейса ПО с учетом индивидуальных характеристик пользователей. Решены следующие научно-практические задачи:  выявлены основные параметры, по которым можно классифицировать пользователей при разработке интерфейсов ПО;  разработана методика проектирования адаптированных пользовательких интерфейсов с элементами искусственного интеллекта;  разработана нечеткая модель на бинарных нечетких отношениях для подбора альтернативного шаблона интерфейса ПО. Предложенная методика позволит автоматизировать процесс проектирования пользовательского интерфейса, конкретизировать техническое задание на ПО, что в конечном итоге повысит объективность и оперативность решений, принимаемых разработчиками.
В последнее время среди специалистов по анализу данных и машинному обучению все более популярным становится ПО для организации исследований. Прежде всего это связано с большим количеством этапов обработки данных и спецификой их выполнения. Можно выделить такую библиотеку, как Sacred , которая позволяет организовать эксперименты без привязки к конкретным моделям, данные параметров моделей и результаты можно сохранить в БД. В библиотеке Hyperopt  акцент делается на оптимизации параметров моделей. FGLab  позволяет аналитику запускать свои модели на распределенной системе с возможностью сохранять результаты экспериментов и их параметры в БД. Для сложных вычислительных задач с применением Hadoop, которые могут длиться дни или недели, подойдет Luigi . Данный пакет позволяет организовать управление многочисленными вычислительными задачами в одном месте. Последние две системы имеют интерфейс для визуализации результатов и информации по задачам. Заключительным этапом в решении задачи машинного обучения является построение ансамбля моделей, поскольку в некоторых случаях оптимальное решение может быть получено с применением ансамбля нескольких различных моделей. Большое количество источников показывают практическую значимость применения ансамбля в решении прикладных задач . Очень часто в таких ансамблях используют нейросетевые модели. Примечательно, что построение ансамбля только из нейросетевых моделей в некоторых задачах дает преимущество . В связи с этим возникает проблема хранения данных на этапах моделирования, в том числе данных самих моделей и построенных с их помощью ансамблей. Проведенный обзор систем организации экспериментов показал, что существующие системы не решают такую проблему в явном виде. Цель данной работы – проектирование и разработка системы хранения ансамблей нейросетевых моделей, обеспечивающей структурированное хранение данных на различных этапах решения задач прогнозирования временных рядов. Разработка хранилища позволит не только организовать процесс анализа данных, но и повысить качество результирующих моделей за счет автоматизации процесса формирования ансамблей. Работу можно разделить на следующие основные части:  разработка модели данных и архитектуры системы хранения;  разработка пользовательского интерфейса;  тестирование системы на реальных данных. Для задач прогнозирования временных рядов принято использовать два типа ИНС: рекуррентные сети (RNN) . Задержку по времени также можно применять и для рекуррентных сетей . В работе при решении задачи прогнозирования временного ряда будет использована LSTM (long short-term memory – долгая краткосрочная память). Рекуррентные нейронные сети, основанные на этом подходе, получили большое распространение при решении задач распознавания рукописного текста, моделирования языка, машинного перевода, обработки аудио- и видеоизображений, анализа тональности и классификации текстов, прогнозирования временных рядов. При решении сложных задач классификации, регрессии, а также прогнозирования временных рядов часто оказывается, что ни один из алгоритмов не обеспечивает желаемого качества восстановления зависимости. В таких случаях имеет смысл строить композиции алгоритмов (ансамбли), в которых ошибки отдельных алгоритмов взаимно компенсируются. Для задачи прогнозирования временных рядов подойдут такие подходы, как голосование и cтекинг (stacking). Они подразумевают формирование ансамбля из моделей, полученных на одинаковых данных, что подходит для временных рядов, в отличие от бустинга (boosting) и бэггинга (bagging), где для базовых алгоритмов используются разные данные. Наиболее известные корректирующие операции голосования:  простое: взвешенное:  смесь экспертов: Простое голосование – это лишь частный случай взвешенного голосования, а взвешенное является частным случаем смеси экспертов. Основная идея стекинга и его разновидности блендинга заключается в использовании базовых алгоритмов для получения предсказаний (метапризнаков) и использовании их как признаков для некоторого обобщающего алгоритма (метаалгоритма). Иными словами, основной идеей стекинга является преобразование исходного пространства признаков задачи в новое пространство, точками которого являются предсказания базовых алгоритмов . Разработка модели данных и архитектуры системы хранения Для реализации поставленных задач необходим следующий набор программных средств: реляционная БД для хранения данных об объектах и связей между этими объектами;  нереляционная БД для хранения временных рядов;  язык программирования для реализации логики системы хранения;  сопутствующие программные пакеты, в том числе реализующие LSTM. В качестве реляционной СУБД была использована MySQL . Для хранения временных рядов современное решение – InfluxDB . Для программирования логики хранилища нейросетевых моделей выбраны Python версии 2.7.11 и следующие свободно распространяемые пакеты:  numpy (для работы с массивами );  sklearn (библиотека для анализа данных );  cherrypy (библиотека, позволяющая реализовать веб-сервер ). Среди многочисленных программных реализаций архитектур нейронных сетей, в частности LSTM, выделим Theano , а также созданную на ее основе библиотеку Keras . Библиотека Keras позволяет использовать как Theano, так и TensorFlow  в качестве основы вычислений. Keras упрощает процесс создания нейронных сетей, предоставляя для этого специальный конструктор. В основе любого кода с использованием Keras лежит объект model, который описывает то, в каком порядке и какие именно слои содержит ваша нейронная сеть. Для построения структуры реляционной БД рассмотрим необходимые сущности и их структуру . Проект (project) объединяет ряд исследований над набором данных. В рамках проекта рассматриваются данные из определенного источника (временной ряд, который хранится в InfluxDB). Все действия по преобразованию данных, построению моделей или ансамблей производятся в рамках проекта. Источник данных (data_source) представляет собой описание данных в источнике. В рамках хранилища рассматривается основная задача – прогнозирование временных рядов. Соответственно, информация об источнике данных включает такую информацию, как начало периода, конец периода, интервал измерений и другие. В связи с тем, что источник данных не фиксирован, то есть данные в нем могут изменяться, дополняться, удаляться, необходимо фиксировать состояние источника данных на момент начала какого-либо исследования или ряда исследований. Снимок данных (data_snapshot) отражает состояние источника данных на момент времени. Однако сами данные в исходном виде, как правило, не пригодны для построения качественных моделей, поэтому необходимо выполнить ряд преобразований. Преобразование данных (data_preparation) показывает способ преобразования данных (снимка данных), а также сохраняет преобразованные данные для дальнейшего применения. Преобразованные данные по-прежнему являются временным рядом, но для использования в различных нейросетях должны быть созданы конечные наборы данных в виде матриц X и Y, объясняющие признаки и целевые значения. Набор данных (data_set) – это конечная выборка данных, отвечающая требованиям той или иной модели. Например, одна модель может использовать для прогнозирования окно в значений, а целевое (прогнозируемое) значение будет отступать на пункта от окна. В этом случае размерность матрицы, а формируется по определенному правилу. При других параметрах выборка будет сформирована иначе, что и объясняет необходимость введения рассматриваемой сущности. Снимок данных, преобразование данных и набор данных – это отдельные наборы данных, пошагово полученные из предыдущего источника. Эти данные уже необязательно являются временными рядами с точки зрения способа их хранения. В связи с этим необходимо организовать хранение этих данных в унифицированном виде. Наиболее подходящим форматом хранения является CSV (Comma-Separated Values – разделяемые запятыми значения) – текстовый формат, предназначенный для представления табличных данных. Определим также сущность CSV-данных (data_csv) – это зависимая сущность, которая представляет собой только сами данные под уникальным идентификатором. Другим не менее важным набором сущностей является набор, связанный с нейросетевыми моделями. Исследование (research) – это группа моделей, полученных по определенным правилам. Такие правила устанавливают порядок преобразования данных, способ построения моделей и их настройки и т.д. В рамках исследования рассматриваются данные, преобразованные определенным образом, поэтому все модели, построенные в ходе исследования, с точки зрения представления работают с одними и теми же данными. Модель (model) – это представление математической модели. Модель может быть любого типа: как нейросетевой, так и любой другой (случайный лес, логистическая регрессия и др.). Для получения модели данные должны быть подготовлены определенным образом, как говорилось ранее, и сохранены как набор данных. Такой набор и используется далее для обучения и тестирования модели. В ходе описания настроек или построения модели могут возникать некоторые данные, так или иначе описывающие модель. Они называются метаданными и требуют вынесения в определенную сущность (для реляционной БД). Метаданные модели (model_meta) – это простое представление данных о модели в виде «ключ-значение». Такие данные могут содержать настройки модели и/или данные о процессе обучения (ошибка, доля правильных ответов, время обучения, алгоритм обучения и другие). Последним набором сущностей являются сущности, связанные с ансамблями (комитетами). Ансамбль (ensemble) – это сущность, создаваемая в рамках одного проекта. Такое ограничение вводится для ограничения данных: все модели должны работать на данных одного и того же рода. Здесь описываются такие данные об ансамбле, как метод построения ансамбля, тип метамодели (метаклассификатора), математическая модель ансамбля и т.д. Ансамбль составляется из моделей. При этом каждая модель может содержать ряд определенных параметров с точки зрения ансамбля, например, вес эксперта для линейной регрессии. Элемент ансамбля (ensemble_item) – параметризованная модель, используемая в построении ансамбля. Физическая модель данных в MySQL представлена на рисунке 1. Для связи реляционной БД MySQL и нереляционной InfluxDB необходимо ввести ряд спецификаций:  измерение (measurement), содержащее экспортируемую информацию, должно иметь имя data_source;  измерение обязательно должно включать тэг (tag) mysql_id, содержащий идентификатор исходных данных, куда будет произведена привязка;  тип данных значения (value) должен быть float-числом с плавающей точкой. Измерение создается автоматически при добавлении новых данных. Рассмотрим пример добавления данных в необходимое измерение по установленным правилам: data_source,mysql_id=234 value= =. Такой запрос добавит в БД InfluxDB запись в необходимое измерение для источника данных с идентификатором 234. Запись будет содержать значение и привязку ко времени со значением  (timestamp) – . Одна из основных задач хранилища – предоставление функционального программного интерфейса для взаимодействия с данными, хранящимися в БД. Поэтому, помимо средств хранения данных (MySQL, InfluxDB), хранилище ансамблей нейросетевых моделей включает внутреннюю логику, определяющую правила функционирования системы. Рассмотрим каждый пакет из представленных на рисунке 2:  enstorage (полная библиотека хранилища, включающая в себя основные модели объектов, отражающие сущности БД (ORM, Object-Relational Mapping))  adapter (библиотека, реализующая методы преобразования данных для дальнейшего использования в моделях);  enmyadmin (содержит основной функционал встроенной системы администрирования хранилища).  Отдельным классом, который обязательно должен быть использован перед работой с хранилищем ансамблей нейросетевых моделей, является Connector (рис. 3). Он осуществляет подключение к необходимым БД (MySQL и InfluxDB). Все остальные классы являются компонентами ORM и реализуют следующие стандартные public (доступные извне) функции:  delete – удаление связанного с БД объекта;  save – сохранение (создание или обновление) объекта БД;  get – статичный метод, возвращающий объект класса, к которому он относится, по указанному идентификатору записи (id);  get_list – статичный метод, возвращающий список объектов класса, в котором вызван, по указанным идентификаторам (ids); если идентификаторы не указаны, возвращается полный список всех объектов. С течением времени исходные данные в InfluxDB могут обновляться и пополняться. Для фиксации определенного набора данных необходимо создать снимок (DataSnapshot). Снимок создается при помощи метода DataSource.create_snapshot(), который загружает текущее состояние источника на указанный временной период (DataSource: time_from, time_to). Загрузка данных выполняется с применением агрегирующей функции, группирующей данные по временному интервалу, – DataSource.time_interval. Сохраненный снимок выступает в роли самостоятельных данных, которые могут быть использованы для дальнейших исследований. Для подготовки данных создается объект DataPreparation, который обеспечивает хранение подготовленных данных, а также преобразование данных DataSnapshot. Создание преобразованных данных выполняется при помощи метода DataSnapshot.create_preparation(clean_method, transform_method, train_part, valid_part, test_part), где параметрами являются (по порядку) метод заполнения пустых значений, метод преобразования значений, доля обучающей выборки, доля валидационной выборки, доля тестовой выборки. Подготовка данных в ручном режиме осуществляется методом DataPreparation.prepare(). После преобразования данных на их основе может быть создано исследование – Research. При создании модели Research не определяет, с каким набором работает модель, это задача сервиса, использующего хранилище.  Сервис должен произвести следующие действия:  создать модель, привязанную к исследованию;  исходя из типа модели преобразовать нужным адаптером (adapter) данные – DataPreparation.create_set(adapter);  сообщить модели о созданном наборе данных посредством Model.data_set(created_data_set). При этом в автоматическом режиме адаптером будут обработаны преобразованные данные DataPreparation. Данный этап является завершающим для серии преобразования исходных данных, полученных из DataSource. Еще одним важным потоком данных является информация, поступающая в ходе построения моделей и ансамблей. Фиксация таких данных (метаданных) для модели осуществляется методами Model.meta(key, value). Метаданными могут быть абсолютно любые данные, описывающие модель. При построении ансамбля дополнительные параметры фиксируются в свободной форме в EnsembleItem.properties, однако рекомендуется использовать формат JSON. Особенностью хранилища является то, что хранение объектов конечных реализаций моделей осуществляется благодаря специальному формату. Данные объектов сериализуются и десериализуются при помощи библиотеки pickle. Такой подход обеспечивает возможность сохранения и восстановления объектов целиком, тем самым обеспечивая высокий уровень интеграции пакета enstorage с другими библиотеками. Также, благодаря используемому формату, хранилище может принять не только нейросетевые модели, но и любые другие. Однако из-за использования библиотеки pickle существует ограничение на использование этих данных в языках, отличных от Python, так как данные совместимы только с ним.  Тестирование системы на реальных данных  Разработка пользовательского интерфейса  Для тестирования работоспособности хранилища в реальных задачах необходимо реализовать систему построения ансамблей, а также обучения моделей. Конструктор ансамблей – это отдельный функционал, который может быть вынесен в специальный пакет endirector. Данный пакет включает методы построения ансамбля и использует объекты хранилища из пакета enstorage. Ансамбль формируется в автоматическом режиме на основе настроек в Ensemble. Далее приведен пример построения ансамбля из набора моделей, а также применения Conductor для формирования метамодели:  Для упрощения администрирования хранилища ансамблей нейросетевых решений предусмотрен пакет enmyadmin, входящий в enstorage. Данный пакет представляет собой веб-сервер с основными методами администрирования. Основным шаблоном проектирования веб-приложения является model-view-controller (MVC, «модель-представление-контроллер»). В роли клиентского приложения выступает HTML5-JS-приложение, разработанное с использованием AngularJS . Взаимодействие клиентского и серверного приложений осуществляется по технологии REST (representational state transfer – «передача состояния представления»), обеспечивающей независимость серверной части от клиентского приложения. Фреймворк работает с HTML, включающим дополнительные пользовательские атрибуты, которые описываются директивами, и связывает ввод-вывод области страницы с Порядок действий для построения ансамбля, ремоделью, состоящей из объектов JavaScript. Значения этих объектов задаются вручную или извлекаются из статических или динамических  данных. На рисунке 5 представлена форма просмотра информации о проекте. В левой части формы находится фиксированная панель навигации, позволяющая просматривать список проектов и источников данных для быстрого перехода к ним. Для поиска необходимого пункта предусмотрен функционал фильтрации. В правой части окна находится область управления, включающая элементы управления открытым объектом. Разработаны следующие формы управления объектами: проект, исследование, модель, ансамбль, исходные данные, снимок данных, преобразованные данные, набор данных. Важно отметить, что благодаря использованию технологии REST клиентское приложение системы администрирования может быть разработано на любой платформе, поддерживающей взаимодействие по HTTP-протоколу. Для оценки качества каждой модели, а также ансамбля рассчитаем среднеквадратическую ошибку (MSE) на тестовой выборке. На рисунке 7 видим, что наименьшее значение ошибки у ансамбля (на графике – out). Так как в качестве метамодели использовалась линейная регрессия (Linear Regression), вес каждой модели можно оценить в результирующем значении. Данные значения записаны в  Стоит отметить, что наборы данных различных моделей могут существенно отличаться друг от друга. Так, один набор данных может быть получен с задержкой в 4 значения, а другой – в 10. При этом объем выборок также будет отличаться. Могут использоваться и другие методы конвертации временного ряда, что делает невозможным однозначное получение результата всех моделей для одних данных. Решением данной проблемы является идентификация (id) целевых значений. Таким образом, каждый набор  в DataSet также включает и id. Благодаря этому можно получить значения всех моделей для конкретного целевого значения. В качестве исходных данных для тестирования системы использованы данные о солнечной активности за период с января 1700 года по февраль 2015 года, всего 303 значения (рис. 6). Для эксперимента построим 3 нейронные сети с задержкой в 5, 7, 13 значений. В ходе выполнения итогового скрипта осуществляются следующие действия:  подключение к хранилищу;  получение данных о наборе данных;  создание снимка данных;  преобразование данных (масштабирование, выделение тестовой выборки – 30 %);  создание проекта и исследования;  создание и инициализация модели;  обучение моделей;  создание ансамбля. Далее приведен код создания модели с применением библиотеки  Значения весов можно интерпретировать следующим образом: наибольшим весом обладает первая модель (keras-1), небольшую корректировку вносит третья модель (keras-3), компенсацию оказывает вторая модель (keras-2). На рисунке 8 представлены результаты прогноза, полученные с помощью ансамбля. Масштабированное значение Заключение В ходе выполнения данной работы был создан прототип системы хранения ансамблей нейросетевых моделей. Проведенный эксперимент по прогнозированию солнечной активности показал, что ошибка ансамбля нейросетевых моделей ниже ошибки каждой отдельно взятой нейросетевой модели. Несомненно, для улучшения результатов прогнозирования необходимы дополнительные эксперименты и совершенствование ПО. Разработаны следующие программные решения:  пакет для языка Python, обеспечивающий быстрое и упрощенное взаимодействие с БД, реализованный с использованием технологии ORM;  пакет преобразования временного ряда в конечные выборки, применяемые в моделях;  интерфейс пользователя в виде HTML-приложения, обеспечивающий наглядное отображение данных и удобное взаимодействие с хранилищем ансамблей нейросетевых моделей. Результаты проделанной работы показывают перспективность разработанных программных решений и обеспечивают высокую степень интеграции в расширяемые программные продукты на языке Python. Исследование выполнено при финансовой поддержке РФФИ, проект.
Целями применения существующих систем управления мастер-данными (MDM-систем) являются формирование и поддержка консистентного, функционально полного и актуального представления основных данных, отражающих все нюансы структуры и деятельности предприятия. Наличие и поддержка такого единственного содержательного представления мастер-данных является критическим фактором для достижения бизнес-целей предприятия. MDM-системы представляют собой инструментарий для поддержки процессов управления мастер-данными в масштабе предприятия на основе принятых руководящих документов, политик и стандартов . С точки зрения принадлежности программных продуктов к прикладному или платформенному ПО MDM-системы входят в состав платформы и являются неотъемлемой частью комплекса систем управления информацией в масштабах предприятия (Enterprise Information Management) . Системы управления мастер-данными об активах как подкласс систем управления мастер-данными Ведущая мировая аналитическая компания Gartner традиционно ежегодно выпускает два независимых аналитических обзора по разным классам  MDM-систем. Эта традиция была сохранена и в 2015 году . Системы управления мастер-данными об активах отдельно не рассматриваются в этих обзорах. Они относятся к широкому классу MDM-систем с данными о предметах, и считается, что и на них распространяется обзор  (в отличие от MDM-систем с данными о командах, на которые распространяется обзор ). Согласно , комплексная стратегия управления мастер-данными охватывает множество предметных областей: клиенты, продукты, материалы, услуги, активы, персонал или команды, поставщики и финансы. Мастер-данные об активах, как и мастер-данные о продуктах, становятся все более критичными в связи с ростом числа публикаций и интереса к Интернету вещей (IoT). Распространение IoT предполагает, что все больше устройств (вещей) обмениваются информацией. Многие из них находятся за пределами зоны действия используемых норм организации и управления информацией, в итоге теряется доверие к такой информации. Управление мастер-данными позволяет снять вопрос о доверии, обеспечить качество и консистентность данных об устройствах, а также данных, получаемых от этих устройств. Рассматривая проблематику систем управления мастер-данными об активах, необходимо ответить на вопрос, что понимается под активом. Ответ не всегда однозначен, и этот термин трактуется разными компаниями по-разному. Практически всегда в понятие активов включаются материальные активы (здания, сооружения, оборудование, земельные участки, строящиеся объекты и т.д.) и их составные части. Несколько реже в понятие включаются нематериальные активы – программные системы, БД, патенты, товарные знаки и т.д. Еще реже в российской практике  и более часто в западной в единое понятие активов включаются и финансовые активы – акции, облигации, займы, доли в совместных проектах, векселя и т.д. Общая численность парка активов в крупных компаниях, таких как OАО «РЖД», НК «Роснефть», может достигать нескольких десятков миллионов единиц. Основную часть такого большого парка (более 90 %), как правило, составляют материальные активы. Обобщенно рассматривая системы управления мастер-данными об активах как некоторый подкласс MDM-систем о вещах, в  также уделяется много внимания перспективам работы MDM-систем с данными более чем одной предметной области (применительно к теме данной статьи – не только с мастер-данными об активах). Наряду c доминирующими сейчас однодоменными MDM-системами, предназначенными для работы с данными одной предметной области (например, с данными о клиентах или о продуктах), пока еще мало распространены, но быстро расширяются сегменты мультидоменных и мультивекторных MDM-систем. Мультидоменные системы предназначены для работы с данными нескольких предметных областей. Несмотря на то, что пользователи хотели бы иметь возможность работать с любыми предметными областями в любом месте и любым способом, существующие мультидоменные MDM-системы ориентированы на одну якорную область данных. В данный момент, согласно , мультидоменные MDM-системы – это:  MDM-системы для работы с одной предметной областью, имеющие расширенные ссылки на другие предметные области;  MDM-системы для работы с одной предметной областью, в которые добавлено большое число атрибутов и иерархий, чтобы поддержать локальные бизнес-процессы или приложения;  несколько одновременно установленных MDM-систем для разных предметных областей, часто от разных поставщиков, объединенных внешним интерфейсом. Концепция мультивекторных MDM-систем расширяет понятие мультидоменных MDM-систем. Помимо работы с разными предметными областями, мультивекторные MDM-системы должны работать в разных отраслях, при разных сценариях использования (проектирование структур, операционные или аналитические MDM-системы), для разных организационных структур (централизованная, федеративная, локализованная) и при разных сценариях внедрения MDM-системы (регистровая, консолидационная, в режиме сосуществования и централизованная). По прогнозу Gartner, мультивекторные MDM-системы сменят мультидоменные и однодоменные не ранее, чем через пять лет. По мнению авторов, пока не будут реализованы и не докажут на реальных внедрениях свою работоспособность и эффективность однодоменные системы управления мастер-данными об активах, прогнозировать, что необходимые функциональные возможности по управлению мастер-данными об активах будут покрыты мультидоменными и мультивекторными MDM-системами, преждевременно. Только после того, как все необходимые механизмы будут отработаны на однодоменных системах, можно будет осуществить их перенос в мультидоменные системы. Нерешенные проблемы управления мастер-данными об активах В существующих корпоративных системах управления в нескольких модулях или подсистемах независимо ведутся мастер-данные о производственных фондах (активах), отражающие различные характеристики одних и тех же объектов. Даже когда разные приложения являются модулями единой ERP-системы, используется такой же подход . Примерная структура основных данных об активах в существующих системах управления крупными компаниями показана на рисунке 1 . Технически это реализуется следующим образом: в разных прикладных подсистемах для ведения мастер-данных об активах используются собственные структуры различной сложности и независимые классификаторы объектов. Мастер-данные ведутся разрозненно, хотя и перечень объектов, и значительная доля описывающих объекты характеристик (в том числе паспортных и эксплуатационных) необходимы для работы нескольких приложений. Как следствие, возникает дублирование данных и появляются противоречия между различными представлениями данных. Кроме снижения качества данных, это также увеличивает издержки по сопровождению . В действительности в корпоративных системах имеется намного больше подсистем, в которых используются мастер-данные об активах, чем показано на рисунке 1 . В их число входят данные:  об основных средствах (объектах бухгалтерского и налогового учета);  об объектах недвижимости;  об объектах технического обслуживания и ремонта оборудования;  об объектах имущества (правовой аспект);  об объектах подсистем мониторинга (пожарной безопасности, видеонаблюдения и др.);   об объектах подсистем оперативного управления режимами работы оборудования непрерывных производств, транспорта и связи, коммунального хозяйства;  об объектах различных MES и технологических систем управления производством;  об объектах геоинформационных систем (визуализируемых на картах);  об объектах генерации, передачи и потребления электроэнергии в подсистемах управления производством и потреблением электроэнергии;  об объектах добычи, транспортировки и отпуска нефти, нефтепродуктов и газа в подсистемах управления добычей, транспортировкой и сбытом углеводородов;  об обрабатывающих центрах и транспортных механизмах в системах технологической подготовки производства (CAM-системах);  о складах, распределительных центрах и торговых точках в системах управления розничными сетями. Перечислим основные причины, по которым системы управления мастер-данными об активах не нашли широкого применения в отличие от MDM-систем для других предметных областей .  1. В разных прикладных областях применяются различные принципы выделения активов. Например, в рамках бухгалтерского учета в одно основное средство могут быть объединены несколько рядом стоящих единиц оборудования, имеющих одинаковые правила начисления амортизации. При проведении ремонта и технического обслуживания выделяются объекты, имеющие разный порядок ремонта и обслуживания. В рамках имущественного учета выделяются объекты, которые независимо могут быть проданы или сданы в аренду, например одно волокно в пучке оптических волокон или оптоволоконном кабеле. 2. В разных приложениях, работающих с одним и тем же парком активов, используются различные иерархии вложенности объектов, причем различаются объекты как группирующих уровней, так и нижних уровней таких детализирующих иерархий. 3. При функционировании комплексных систем управления пользователям очень часто необходимо видеть смежные представления одного и того же актива. Например, из технологического представления необходимо видеть объекты технического обслуживания и ремонта. Из объектов технического обслуживания и ремонта необходимо видеть основные средства, на которые будут списываться затраты. Из основных средств необходимо видеть соответствующие объекты имущества и т.д. Почти всегда пользователям необходимо видеть объекты нескольких разных представлений. Отсутствие взаимно однозначного соответствия между объектами нижнего уровня различных приложений приводит к появлению связей многие ко многим между объектами разных представлений, описывающими один и тот же актив. Как уже было показано ранее, одному основному средству может соответствовать множество объектов технического учета. Но также справедливо и обратное: одному объекту технического учета, например асфальтированной дороге, может соответствовать множество основных средств, если ее отдельные участки, не являющиеся единицами технического учета, строились (выкупались) по разным ценам или в разное время. Отсутствие в настоящий момент на рынке системы управления мастер-данными об активах, способной преодолеть перечисленные сложности, подтверждается также обзорами аналитической компании Forester. В первом квартале 2016 года компания выпустила обзор , в котором выделила четыре класса MDM-систем и проанализировала продукты двенадцати компаний. Были выделены  MDM-системы, обеспечивающие интеграцию данных и решающие стандартные задачи;  мультидоменные MDM-системы и MDMсистемы, поддерживающие множество представлений мастер-данных;  контекстуальные MDM-системы, поддерживающие семантическое представление бизнес-данных;  аналитические MDM-системы, интегрированные внутрь аналитических платформ. Из четырех представленных классов MDM-систем сформулированные проблемы должны решаться в группе мультидоменных MDM-систем и MDM-систем, поддерживающих множество представлений мастер-данных. В качестве лидеров этой группы выделены SAS, SAP и IBM. У этих компаний в настоящий момент нет MDM-систем, способных обеспечить управление мастер-данными об активах в описанном выше ключе. Предлагаемая модель данных Для совмещения нескольких разных представлений одного и того же парка активов предлагается модель данных, включающая произвольное число ракурсов представления активов. Каждый ракурс описывается некоторой иерархией, примерный вид которой представлен на рисунке 2. Начиная с некоторых узлов дальнейшие связи обрываются, чтобы не увеличивать сложность рисунка. Число уровней иерархии может быть различным в разных ракурсах. Иерархическая структура ракурса отображает принадлежность (вхождение объектов в объекты более высокого уровня). Разные ракурсы полиморфны между собой, то есть отсутствует взаимно однозначное соответствие между объектами разных ракурсов. Каждый объект, представленный в иерархии, принадлежит к какому-то классу объектов. Класс определяет свойства (набор атрибутов) объекта. Классы объектов строятся на принципах полиморфизма и наследования. Они образуют иерархию классов, позволяя детально описывать тип и атрибуты (характеристики) каждого актива. Для каждого ракурса строится собственная система классификации объектов. Атрибуты объектов могут иметь широкий спектр типов: не только целое, вещественное, логический признак, текст, но и массив, множество, зависимость, график, таблица, документ и др. Для описания одного и того же актива, представленного в разных ракурсах, могут использоваться атрибуты с одинаковым именем, однако значения этих атрибутов могут не совпадать. Параллельные иерархии разных ракурсов (лес) провязаны на каждом уровне решетками связей для обеспечения переходов между разными ракурсами представления одного и того же актива. Пример такой решетки представлен на рисунке 3. Решетки необязательно являются полными (то есть необязательно в них присутствуют все связи каждого объекта с каждым). Наличие связей между разными объектами зависит от семантики: нужно устанавливать соотношения между объектами этих ракурсов на данном уровне или нет. На одном уровне иерархии любого из ракурсов располагается много объектов, каждый из которых связан со своей решеткой смежных объектов. Решетки одного уровня могут частично совмещаться друг с другом. Это возникает из-за того, что в отдельных ракурсах один объект может соответствовать нескольким объектам другого ракурса, расположенным на одном уровне иерархии (например, одно основное средство может объединять несколько единиц оборудования), и тогда решетки смежных объектов одного уровня могут частично соединяться в отдельных ракурсах. Если рассматривать 3D-модель графа мастерданных, то решетки на разных уровнях необязательно будут абсолютно параллельными. В отдельных ракурсах один объект может соответствовать нескольким объектам другого ракурса, расположенным на разных уровнях иерархии, и тогда решетки смежных уровней могут соединяться в отдельных ракурсах. В отдельных ракурсах между объектами имеются неиерархические связи, отражающие специфические отношения между объектами, свойственные этому ракурсу. Например, функциональное взаимодействие в процессе производства, логистический обмен товарами, соседнее территориальное расположение и др. Пример ракурса с такими связями между производственно-техническими комплексами представлен на рисунке 4. Показанные на рисунке 4 неиерархические связи могут иметь собственную систему классов и наборы атрибутов, описывающие каждую связь. В ряде случаев неиерархические связи между объектами одного ракурса могут быть параметрическими, то есть возникающими только тогда, когда значения одного или нескольких атрибутов объекта соответствуют определенным условиям.  Помимо классов объектов, определяющих их тип, в модели данных могут присутствовать структурные и функциональные модели отдельных типов активов. Одним из назначений таких моделей является проверка правильности вводимых мастерданных. Возможности использования структурных описаний для проверки правильности ввода данных об активах можно проиллюстрировать следующим простым примером. Структурная модель котельной включает обязательные компоненты: здание, установленный в нем котел и дымовую трубу. Если вновь введенный пользователем объект имеет тип «котельная», но не содержит всех трех обязательных компонентов, пользователю выводится сообщение об ошибке. Функциональные модели могут проверять соответствие объекта его типу, используя значения атрибутов этого объекта и атрибутов других, связанных с ним объектов. Структурные и функциональные модели, используемые для проверки полноты и правильности вводимых мастер-данных, могут быть достаточно сложными. Роль структурных и функциональных моделей в общем механизме управления мастер-данными об активах не ограничивается проверкой правильности вводимых данных. При описании технических ракурсов представления активов в ряде случаев используемая схема соединения компонентов определяет тип объекта, состоящего из этих компонентов. Точно так же тип объекта может зависеть от значений его атрибутов и применяемых функциональных моделей. Архитектура системы управления мастер-данными об активах Предлагаемая общая архитектура системы управления мастер-данными об активах представлена на рисунке 5. Ядро системы поддерживает создание/модификацию и хранение в памяти информационной модели активов.  Каждый интерфейсный модуль обеспечивает взаимодействие с конкретным приложением или функциональным модулем ERP-системы. По запросам, поступающим от внешних приложений или модулей ERP, интерфейсный модуль осуществляет выгрузку запрошенного фрагмента мастер-данных об активах, относящихся к конкретному ракурсу мастер-данных, с которыми работает запрашивающий модуль. Помимо объектов данных конкретного ракурса, могут быть выгружены также иерархия, классификатор, сетевые связи, структурные и функциональные модели, относящиеся к объектам этого ракурса. Все объекты активов могут быть выгружены вместе с имеющимися связями с объектами других ракурсов. Помимо выгрузки из системы управления мастер-данными, интерфейсные модули также могут обеспечивать загрузку обновленных или новых мастер-данных соответствующих ракурсов из приложений и модулей ERP в систему управления мастер-данными. Однако такая загрузка не обеспечивает использование загруженных данных в системе. Требуется последующее подтверждение загруженных данных от администратора мастерданных. Модуль диалогового интерфейса обеспечивает создание и корректировку мастер-данных администратором мастер-данных в режиме диалога. В том числе с его помощью подтверждаются мастер-данные, загруженные интерфейсными модулями извне, или осуществляются корректировки этих данных, вносятся дополнительные связи и только потом дается разрешение на использование этих данных в системе. Алгоритмические модули позволяют выполнить обработку всей модели мастер-данных в целом или отдельных ракурсов. С их помощью может выполняться проверка корректности имеющихся в системе мастер-данных или связей между объектами. Также с их помощью могут выполняться формирование новых ракурсов мастер-данных на основе имеющихся в системе данных других ракурсов и/или на основе внешних данных и другие операции над информационной моделью в целом. Инициирование работы алгоритмических модулей осуществляет администратор мастер-данных через модуль диалогового интерфейса. Алгоритм проверки корректности межракурсных связей в единой модели активов Для того чтобы единая модель активов была корректной, необходимо, чтобы в ней были корректными все связи. Для проверки структуры иерархий и неиерархических связей внутри ракурсов требуется использовать дополнительную семантическую информацию. В то же время проверку полноты и корректности всех решеток межракурсных связей можно выполнить, используя только тип объектов и существующую структуру связей без привлечения дополнительной информации о конкретных объектах. Для проверки полноты связей в решетке можно использовать шаблон, содержащий все необходимые связи, и каждую решетку можно сверить с этим шаблоном. Поскольку в больших компаниях неизбежно будет наблюдаться значительное разнообразие видов активов, большинство из которых не должно присутствовать во всех ракурсах, шаблон решетки не может быть единым, на практике должен использоваться некоторый набор правил, регламентирующих применение шаблонов для отдельных видов активов. Однако наличие конечного множества используемых шаблонов решеток и сверка с ними фактических решеток связей не избавит от ситуации, когда связи между разными объектами требуемых типов перепутаны между собой. Для борьбы с этим явлением авторы предлагают специальный алгоритм, рассматриваемый далее. Если предположить, что имеются всего три ракурса активов, решетка принимает вид, представленный на рисунке 6. Проверка корректности всех связей решетки в этом случае осуществляется с помощью простого обхода: из каждого объекта необходимо пройти по кольцу связей в одном направлении, а потом в обратном. Если для всех объектов при проходе в обоих направлениях мы возвращаемся к первоначальному объекту, все связи в решетке корректны. Обобщим этот подход для сложных многоракурсных структур описания активов. Для обхода всех объектов в иерархии каждого ракурса используем стек, показанный на рисунке 7.  В процессе проверки необходимо обойти все иерархические ракурсы, представленные в структуре активов, и запустить алгоритм проверки связей для каждого объекта. Для обхода всех колец одной решетки, начинающихся с одного объекта, используем еще один стек, представленный на рисунке 8. Пройдя по каждому из колец, необходимо вернуться к первоначальному объекту. Поскольку осуществляется перебор всех связей каждого объекта, автоматически будет реализован просмотр в прямом и обратном направлениях. Дополнительно программировать проход в обратном направлении не требуется. Чтобы не проверять многократно одни и те же варианты колец связей, начиная с разных объектов, на время выполнения алгоритма вводится разметка связей. Если при обходе колец проходим по какойто связи и для следующего перехода видим только одну связь, у которой нет отметки, или конец обхода кольца, то ставим на пройденной связи отметку. При дальнейшей проверке проходить по этой связи не нужно. В результате после обхода всех колец, начиная с объектов одного ракурса, если объекты другого ракурса не имеют дополнительных связей, которые еще не проверялись, обход этих объектов не потребует какого-либо обхода колец. Обобщенная блок-схема описанного алгоритма представлена на рисунке 9. Использование полученных результатов при разработке прототипа системы управления мастер-данными об активах Исходя из особенностей разработанной модели данных и алгоритма проверки корректности межракурсных связей, при разработке прототипа системы управления мастер-данными об активах сформулированы следующие основные требования к используемому инструментарию. 1. Инструментарий должен обладать возможностями работы с графовой БД . Это обеспечит существенные преимущества при работе с графами за счет увеличения скорости разработки и сокращения объема кода при обходе цепочек связей по сравнению с традиционным SQL, а также будет требоваться меньше ресурсов для работы прототипа. 2. Инструментарий должен обладать возможностями графовых энджинов , выполняющих сложные алгоритмы над большим графом в целом, даже если его фрагменты хранятся на разных серверах кластера. Этим требованиям соответствуют SAP HANA Graph , работающий на кластере серверов под управлением Microsoft Windows Server. Для использования Microsoft Graph Engine требуется специально конфигурировать кластер под решаемую задачу, в то время как SAP HANA Graph может работать на любой SAP HANA, установленной в режиме OnPremise. Поэтому для использования выбран SAP HANA Graph. Перед выполняемой в настоящее время разработкой прототипа системы управления мастер-данными об активах ставятся цели обеспечить поддержку разработанной модели данных, реализовать механизм проверки полноты межракурсных связей и разработанный алгоритм проверки корректности межракурсных связей.
Комплексная модель зрелости CMMI® (Capability Maturity Model Integration) – это широко известный подход к совершенствованию технологических процессов разработки и сопровождения программных продуктов и систем, разработанный в SEI . Специализированная модель CMMI-DEV (CMMI® for Development) используется как руководство по улучшению качества процессов организаций-разработчиков ПО и рекомендуется в том числе для самооценки организации. Актуальной версией CMMI-DEV является версия 1.3, появившаяся в ноябре 2010 года . Несмотря на то, что новые версии руководства не выходили почти шесть лет, интерес к нему со стороны разработчиков ПО и руководителей предприятий не уменьшается. Продолжает продвигать эту модель и компания «Kondakov Consulting»  – первая в России организация, сертифицированная для проведения оценивания организаций согласно модели CMMI®. Фундаментальным структурным элементом CMMI® является процессная область. Под нею понимается группа взаимосвязанных практик, совместное выполнение которых позволяет организации достичь набора целей, признанных важными для улучшений в этой области. Под процессами в модели CMMI® понимаются работы, которые рассматриваются как выполнение практик, при этом под практикой понимается некоторая деятельность, способствующая достижению связанной с ней цели. Цели разделяются на общие (generic goal – GG) и специфические (specific goal – SG). Соответственно практики, связанные с общей целью, также называются общими (generic practice – GP), а практики, связанные со специфической целью, – специфическими (specific practice – SP). GG относятся ко всем процессным областям, а SG всегда сформулированы для конкретной процессной области. Для каждой специфической практики в модели определяются типичные рабочие продукты, представляющие собой образцы результатов ее выполнения. В CMMI-DEV определены 22 процессные области . В модели CMMI® вводится понятие уровня зрелости производственных процессов организации, достижение которого оценивается через достижение соответствующего уровня возможностей во всех процессных областях, приписанных к данному уровню зрелости. Достигнутый уровень возможностей процессной области показывает, насколько хорошо организация осуществляет работы, относящиеся к данной процессной области. Достижение каждого уровня возможностей определяется реализацией соответствующих целей и практик. Требования к проведению оценивания в рамках модели CMMI® сформулированы в документе ARC (Appraisal Requirements for CMMI®) . Согласно ему, любой метод оценки качества процесса основывается на анализе проверенных экспертами свидетельств о реализации связанных с процессной областью общих и специфических практик – так называемых объективных свидетельств. Авторы данной статьи считают весьма полезным внедрение модели CMMI® или используемых в ней методик для оценки и самооценки зрелости процессов в отечественных компаниях, занимающихся разработкой ПО (как частной, так и государственной форм собственности). Однако следует отметить, что, поскольку ARC не содержит описания конкретных способов оценивания объективных свидетельств и качества реализации практик, практическое применение данной методологии упирается в неопределенность того, какие методы и алгоритмы следует применять для получения оценок. В статье  модель CMMI® была рассмотрена более детально, в ней также рассматривалась возможность использования методов нечеткой логики для вывода уровней выполнения практик на основе анализа имеющихся объективных свидетельств. В данной статье рассматривается методика оценки уровня выполнения практик, основанная на байесовском подходе. Формула Байеса  используется для получения вероятностных оценок истинности гипотез о том, что степень выполнения практик соответствует каждому из уровней, определенных в CMMI®. Предлагаемая методика базируется на подходе к оцениванию качества управленческих решений в железнодорожной отрасли, предложенном в  одним из авторов данной статьи был предложен простой способ определения условных вероятностей, используемых в формуле Байеса, как частот попадания экспертных отметок по интегрированной группе показателей качества в пересекающиеся интервалы 10-балльной метрической шкалы, соответствующие уровням ранжирования качества решения. Применение формулы Байеса упрощает задачу оценивания по сравнению с методами нечеткой логики: уменьшается доля самовольности ЛПР, неизбежной при определении таких параметров нечеткого логического вывода, как виды и формы функций принадлежности, способ реализации нечетких логических операций и т.д. . Подобное упрощение целесообразно, так как высокая точность при оценивании выполнения практик в любом случае невозможна да и не требуется в силу неточности исходных данных и экспертного способа получения оценок. Здесь уместно вспомнить мнение выдающегося математика, академика АН СССР Н.Н. Моисеева, который в контексте обсуждения экспертиз и неформальных процедур в  утверждал, что иногда для нужд практики достаточно использовать весьма грубые оценки. Кроме того, байесовский подход позволяет сгладить разногласия, неизбежно возникающие между экспертами (даже при условии наличия у каждого из них достаточного уровня профессиональной компетентности, исключающего сильные разногласия в оценивании), и освободить лицо, принимающее решение, от необходимости рассчитывать согласованность оценок группы экспертов . Дополнительным доводом в пользу применения байесовского подхода в новом контексте является то, что он давно и успешно используется при принятии решений в условиях неопределенности: при решении задач организационного управления, в том числе задач управления рисками , при оценивании качества продукции на основании случайного выборочного контроля. Байесовское оценивание уровней выполнения практик процессной области Рассмотрим предлагаемый способ применения байесовского подхода к оцениванию уровней выполнения практик процессных областей CMMI. В модели CMMI степень выполнения каждой практики может достигать одного из пяти уровней, упорядоченных по возрастанию качества реализации. Возможные уровни выполнения практики представлены в таблице 1. Обозначим через Hi гипотезу (hypothesis) вида «Выбранная практика достигает уровня реализации i», где порядковый номер уровня из таблицы 1. Далее предположим, что в распоряжении лица, принимающего решение, имеется n объективных свидетельств (evidence) за или против каждой из гипотез Hi. В качестве свидетельств могут использоваться документы, представляющие результат реализации практики либо являющиеся следствием ее выполнения, а также устные или письменные заявления, подтверждающие осуществление (или невыполнение) практики, предоставляемые ее исполнителями. Факт наличия каждого из свидетельств обозначим через . Отметим, что в отличие от гипотез свидетельства  никак не упорядочены по качеству (значимости) и пронумерованы в произвольном порядке. 1. Перед началом оценивания лицо, принимающее решение, формирует априорное распределение вероятностей  на множестве гипотез. Каждая вероятность  рассматривается как степень уверенности этого лица в справедливости i-й гипотезы об уровне выполнения практики до начала оценивания, то есть до получения каких-либо свидетельств за или против гипотезы. Так как априорная информация об уровне выполнения практики может быть полностью неопределенной, вероятности  могут иметь значение. При наличии достаточного обоснования допускается использование и неравномерного распределения априорных вероятностей на множестве гипотез. Например, крайние гипотезы и представляются менее вероятными, чем все остальные, поэтому их априорные вероятности могут иметь более низкие значения. Кроме того, в качестве априорных вероятностей  могут использоваться апостериорные байесовские вероятности, полученные на предыдущей итерации оценивания. 2. Условная вероятность понимается как вероятность истинности свидетельства  в предположении, что истинна гипотеза, и показывает, насколько данные, полученные из свидетельства, соответствуют i-й гипотезе об уровне выполнения практики. Значение этой условной вероятности получается путем агрегации полученных балльных экспертных оценок имеющегося свидетельства. Назначенные экспертами баллы показывают, насколько, по их мнению, каждая из гипотез подтверждается полученным свидетельством, и отражают степени предпочтения экспертами, производящими оценивание, той или иной гипотезы о достижении определенного уровня реализации рассматриваемой практики. 3. Условная вероятность понимается как степень уверенности лица, производящего оценивание, в справедливости i-й гипотезы об уровне выполнения практики после получения всех свидетельств. В соответствии с теоремой Байеса и при условии независимости всех свидетельств она вычисляется как апостериорная байесовская вероятность  4. Полученное по формуле  апостериорное распределение вероятностей на множестве гипотез является итоговой  оценкой уровня выполнения практики и показывает, насколько правдоподобными по завершении процедуры оценивания стали гипотезы о том, что степень выполнения рассматриваемой практики достигла каждого из уровней. Рассмотрим простой способ получения и агрегации экспертных оценок для определения условных вероятностей путем обработки объективных свидетельств. В каждом конкретном случае набор объективных свидетельств определяется как целями оцениваемой организации и типом разрабатываемых продуктов, так и принятым в организации способом фиксации требований к разработке. При необходимости каждое объективное свидетельство может быть оценено в ходе нескольких экспертиз с использованием различных экспертов или экспертных групп. Чем больше используется объективных свидетельств и проводится экспертиз (при условии адекватной профессиональной компетентности проводящих их экспертов), тем точнее будет полученная общая оценка уровня выполнения соответствующих практик. Для фиксации результатов экспертизы объективных свидетельств уместно использовать контрольные списки (checklist), широко применяемые при оценивании качества процессов или продукции . Экспертные оценки соответствия свидетельств  гипотезам о степени выполнения некоторой практики CMMI  формируются по результатам обработки контрольных списков следующим образом. 1. Результаты k-й экспертизы контрольного списка по свидетельству  суммируются, а итоговое значение k переводится в 10-балльную шкалу для обеспечения однородности экспертных оценок. 2. Для каждого свидетельства  подсчитываются относительные частоты попадания всех итоговых значений k в частично пересекающиеся интервалы, определенные на 10-балльной шкале и соответствующие пяти гипотезам из таблицы 1, например. Пересечение интервалов введено намеренно с целью моделирования неопределенности, возникающей при экспертном оценивании, в частности, в связи с использованием свидетельств разного уровня значимости (качества). Более того, для различных свидетельств степень пересечения интервалов может варьироваться в зависимости от уровня их значимости. 3. Полученные относительные частоты и принимаются за оценки условных вероятностей соответствия свидетельств  гипотезам об уровне выполнения практики. Они, разумеется, являются очень грубым приближением к условным 3. Полученные относительные частоты и принимаются за оценки условных вероятностей соответствия свидетельств  гипотезам об уровне выполнения практики. Они, разумеется, являются очень грубым приближением к условным вероятностям, но, как упоминалось выше, в случае оперирования весьма неопределенными исходными данными большая точность и не требуется. Оценивание практик процессной области «Разработка требований» Назначение процессной области «Разработка требований» (Requirement Development, RD) – выявление, анализ и фиксация требований заказчика, а также технических требований и ко всему продукту, и к его компонентам. Требования касаются как в целом функциональности, безопасности, надежности, модифицируемости и масштабируемости продукта, его интегрируемости с внешними приложениями, так и конкретных принимаемых архитектурных решений и определяют действия всех участников проекта по его разработке. В процессной области имеются следующие специфические цели SG и связанные с ними практики SP. 1. SG1 – Develop Customer Requirements. Сбор и перевод в требования заказчика пожеланий всех заинтересованных лиц, их ожиданий, ограничений и представлений об интерфейсах разрабатываемого продукта.  SP 1.1 – Elicit Needs. Выявление пожеланий заинтересованных лиц, их ожиданий, ограничений и представлений об интерфейсах разрабатываемого продукта на всех фазах жизненного цикла.  SP 1.2 – Transform Stakeholders Needs into Customer Requirements. Преобразование пожеланий заинтересованных лиц, их ожиданий, ограничений и представлений об интерфейсах разрабатываемого продукта в перечень требований заказчика с приоритетами. Результатами выполнения практик цели SG1 могут являться  перечень требований заказчика с приоритетами;  порядок проведения верификации;  порядок проведения валидации и т.д. 2. SG2 – Develop Product Requirements. Разработка технических требований к продукту и его компонентам путем совершенствования и уточнения требований заказчика.  SP 2.1 – Establish Product and Product Component Requirements. Установление и сохранение технических требований к продукту и его компонентам на основе требований заказчика.  SP 2.2 – Allocate Product Component Requirements. Распределение требований по компонентам продукта.  SP 2.3 – Identify Interface Requirements. Выявление интерфейсных требований (то есть требований к способам информационного обмена между программными функциями, объектами и другими элементами). Результатами выполнения практик цели SG2 могут являться  общие требования к продукту;  требования к компонентам продукта, в том числе таблицы распределения требований по компонентам;  требования к архитектуре, в том числе к связям между компонентами;  требования к интерфейсам между элементами компонентов;  проектные ограничения, в том числе внешние, и т.д. 3. SG3 – Analyze and Validate Requirements. Анализ и валидация требований.  SP 3.1 – Establish Operational Concepts and Scenarios. Установление общей концепции процесса разработки и набора реализующих ее сценариев.  SP 3.2 – Establish of Definition of Required Functionality and Quality Attributes. Определение требуемой функциональности и критериев качества.  SP 3.3 – Analyze Requirements. Анализ требований с точки зрения выявления их необходимости и достаточности.  SP 3.4 – Analyze Requirements to Achieved Balance. Анализ требований с точки зрения поиска компромисса между пожеланиями заинтересованных лиц и выявленными ограничениями.  SP 3.5 – Validate Requirements. Анализ и проверка требований для гарантии того, что разрабатываемый продукт будет функционировать корректно в среде конечного пользователя. Результатами выполнения практик цели SG3 могут являться  общая концепция процесса разработки;  концепции процессов разработки компонентов, установки продукта, его сопровождения и поддержки;  сценарии, реализующие общую концепцию процесса;  требования к функциональности продукта;  сформулированные критерии качества и технической эффективности;  варианты использования продукта;  диаграммы активности для вариантов использования;  функциональная архитектура (выявленные методы и их взаимодействие);  результаты объектно-ориентированного анализа функциональной архитектуры;  отчет о недостатках системы требований и рекомендации по их устранению;  оценка рисков, связанных с требованиями;  новые дополнительные требования и ограничения. Нетрудно заметить, что структура и содержание процессной области RD на практике в достаточной степени отражается в документации, сопровождающей разработку ПО, в том числе и в отечественной практике. В частности, многие позиции отражаются в техническом задании на разработку автоматизированной системы (ТЗ АС), соответствующем требованиям ГОСТ 34.602-89. Таким образом, экспертная оценка наполнения соответствующих пунктов ТЗ АС может служить объективным свидетельством выполнения специфической практики SP 2.2, например:  OE1 – п. 4.1.1.1 «Перечень подсистем, их назначение и основные характеристики, требования к числу уровней иерархии и степени централизации системы»;  OE2 – п. 4.1.8 «Требования к эксплуатации, техническому обслуживанию, ремонту и хранению компонентов системы»;  OE3 – п. 4.2.1 «Перечень функций, задач или их комплексов, подлежащих автоматизации, для каждой подсистемы». В качестве других объективных свидетельств, подтверждающих выполнение данной практики, можно использовать, например, результаты экспертного оценивания протокола первого совещания с заказчиком (OE4), протокола повторного совещания с заказчиком (OE5), а также иных зафиксированных в модели CMMI возможных результатов выполнения практик. Отметим, что номера, присвоенные объективным свидетельствам, никак не характеризуют их важность или приоритет с точки зрения лица, принимающего решение, а служат лишь для их идентификации. Пример байесовского оценивания практики SP 2.2 процессной области RD Для вычисления байесовской оценки степени выполнения практики SP 2.2 используем объективные свидетельства OE1–OE4, предложенные выше. Рассмотрим процесс формирования оценки уровня выполнения практики. Сначала проводятся экспертизы объективных свидетельств по контрольным спискам, составленным в соответствии с целями предприятия и типом разрабатываемого продукта. Например, контрольный список для оценки раздела ТЗ «Перечень функций, задач или их комплексов, подлежащих автоматизации, для каждой подсистемы» (как свидетельства ОЕ3) может соответствовать приведенному в таблице 2.  В предлагаемом варианте контрольного списка вопросы 2–5 повторяются блоками по N вопросов, где N – число подсистем, составляющих разрабатываемую АС: m  1, N . Обработка результатов экспертиз (то есть заполненных контрольных списков) производится в соответствии с алгоритмом вычисления частотных оценок условных вероятностей , рассмотренным выше. Приведем пример расчета условных вероятностей  и итоговых апостериорных байесовских оценок  уровней выполнения специфической практики SP 2.2 с использованием формулы Байеса. В таблице 3 показан пример расчета условных вероятностей, на основании агрегирования преобразованных в 10-балльную шкалу оценок пяти экспертов, полученных по результатам обработки заполненных ими контрольных списков. Результаты вычисления апостериорных байесовских оценок уровней выполнения практики SP 2.2 приведены в таблице 4. Полученная в примере апостериорная вероятность гипотезы о том, что по результатам обследования практика SP 2.2 достигла третьего уровня реализации (PI, «частично выполнена»), гораздо выше, чем вероятность истинности прочих гипотез. Следующей по величине апостериорной вероятности является гипотеза о выполнении практики «в основном» (LI), вероятности же прочих гипотез либо нулевые, либо почти равны нулю. Так как предложенный метод рекомендуется в основном для самооценки предприятия, заключением по результатам данного оценивания может быть решение о том, что практику SP 2.2 можно считать реализованной. Заключение В статье предложено применение байесовского подхода к оцениванию уровня выполнения практик, определенных в модели CMMI®. Данный подход предполагает использование формулы Байеса для построения распределения апостериорных вероятностей на множестве гипотез о том, что сте пень реализации рассматриваемой практики достигла некоторого возможного уровня, исходя из полученных результатов экспертного оценивания имеющихся объективных свидетельств. Подход не накладывает никаких ограничений на количество, качество и конкретный перечень используемых свидетельств, а также на состав оценивающей экспертной группы и применим не только для специфических практик процессных областей, но и для общих практик при условии, что имеются объективные свидетельства, позволяющие произвести экспертное оценивание. Для унификации результатов экспертизы предлагается использование контрольных списков, а для упрощения агрегации полученных оценок и их пересчета в условные вероятности гипотез – перевод всех оценок в единую (например 10-балльную) шкалу. Используемые при вычислении байесовской оценки уровня выполнения практики условные вероятности рассматриваются как экспертные оценки соответствия объективных свидетельств гипотезам о достижении того или иного уровня выполнения практики. Они показывают, насколько гипотезы подтверждаются полученными свидетельствами, отражают степень предпочтения, отдаваемого экспертами той или иной гипотезе. При этом точность представления этих оценок не является существенной. Преимущества предлагаемого байесовского подхода:  упрощение процедуры оценивания по сравнению с использованием методов нечеткой логики;  вероятностный, более объективный, характер экспертных оценок уровня выполнения практик, а также естественное сглаживание разногласий, возникающих между экспертами;  возможность оценивания уровня выполнения практик по ограниченному набору имеющихся объективных свидетельств (и/или экспертной группой ограниченного состава) и получения при этом вполне состоятельных оценок. Байесовский подход находит применение в менеджменте, при решении задач управления рисками и организации выборочного контроля качества продукции. Этот подход может быть использован также для оценивания качества управленческих решений и, по мнению авторов, для выполнения процедур самообследования предприятий в соответствии с критериями, предлагаемыми в модели CMMI®. Авторы глубоко благодарны крупному специалисту в области проблем управления на железнодорожном транспорте профессору А.Е. Красковскому, поддержавшему идею применения байесовского подхода при оценивании качества управленческих решений, а также выражают искреннюю признательность председателю Совета директоров группы компаний Digital Design А.Р. Фёдорову и бывшему начальнику Департамента информатизации и корпоративных процессов управления ОАО «РЖД» А.В. Илларионову, благодаря которым несколько лет назад открыли для себя модель CMMI®.
В условиях современного информационного общества широкое развитие получили многочисленные системы обработки экономической информации учетного и планирующего характера. Кроме того, появились возможности оперативной обработки и визуального представления информации, что позволяет создавать ситуационные центры (СЦ) для поддержки принятия экспертных решений как в online-, так и в offline-режиме. Такая потребность может возникать при использовании программных продуктов в ограниченном функционале, например, при анализе инвестиционных проектов внешним пользователем, а также в условиях СЦ оперативно-экспертной поддержки принятия решений. Одной из важных задач при функционировании СЦ является автоматизированное внесение информации в программные продукты, для которых имеется XML-код . Это может быть обусловлено многими факторами, например, аппаратными требованиями персонального компьютера пользователя, высокой стоимостью полного ПО, необходимостью или возможностью использования только отдельно взятых модулей системы и т.д. Таким образом, возникает необходимость частичного доступа к возможностям некоторой автоматизированной информационной системы (АИС)  без ее непосредственной установки на персональный компьютер. В статье предложены алгоритм, а также техническая реализация процесса удаленного (online) внесения изменений пользователем через определенный требованиями ПО шаблон входной информации, который может располагаться в Excelдокументе, web-форме облачного data-центра или в другом электронном источнике. Основная идея и концепция Рассмотрим решение сформулированной задачи на примере программной системы «Карма» . Система представляет собой автоматизированный комплекс для внесения, обработки и анализа входной информации экономического содержания. Данный комплекс имеет возможности создания и корректировки математических моделей в форме многопараметрических задач линейного программирования, контроля корректности внесения информации, создания собственной конфигурации проекта (разделов, блоков переменных и т.п.), а также графический анализатор, визуализирующий многопараметрические зависимости и Парето-множества. Это позволяет использовать данную систему специалисту-математику, экономисту-аналитику и бизнесмену . Однако для пользователя эта система представляется в виде XML-файла, непосредственное изменение которого ему недоступно, и не позволяет автоматизированно вносить массивы входной информации для использования функциональных возможностей системы. Рассмотрим следующий вариант получения входной информации от пользователя в указанный пакет. Информация об экономических характеристиках некоторого инвестиционного проекта заносится в файл строго фиксированного формата (Word, Excel или др.) и передается в пакет «Карма» путем ее преобразования через разработанный авторами специальный программный модуль – Instrument for Data Acquisition (IDA-модуль), работа которого будет описана далее. На рисунке представлена схема преобразования информации проектов социально-экономических систем от стадии ее внесения в исходный XML-файл до выдачи результатов расчетов. Рассмотрим подробно каждый из этапов.  Для работы с IDA пользователю необходимо внести в соответствующие поля стандартной формы IDA входную информацию о характеристиках инвестиционного, производственного или финансового проекта.  IDA-модуль автоматически вносит отредактированные значения в XML-файл, осуществляя подстановку введенных параметров в соответствующую группу показателей, необходимых для работы системы. При этом последовательность внесения информации через IDA соответствует последовательности ее обработки в XML-файле. То есть по завершении процесса внесения данных в IDA вся информация попадает в облачное хранилище, откуда с помощью административной панели IDA вносится в необходимый XML-файл для дальнейших расчетов оператором.  Результаты многопараметрического анализа проекта предоставляются заказчику в виде отчета. Техническая реализация В основе загрузки параметров для расчета моделей в  лежит хранение параметров в виде XML-файлов. Конфигурационный файл содержит экономические параметры, разделенные на соответствующие группы по принадлежности (например, «Налоги», «ФОТ и другие затраты», «ИП внешние характеристики» и т.д.). Он защищен от редактирования с помощью внутренних инструментов. При занесении информации следует учитывать повышенный риск возникновения ошибок некорректного ввода значений в XML-файл. В процессе изучения конфигурационного файла первоначально необходимо обратить внимание на его структуру, включающую элементы разметки (markup), содержимое файла, а также XML-тэги, предназначенные для определения элементов документа, их атрибутов и других конструкций языка. XML-файл имеет древовидную структуру. В документе всегда есть корневой элемент. У элемента дерева всегда существуют потомки и предки, кроме корневого элемента, у которого предков нет, и тупиковых элементов (листьев дерева), у которых нет потомков . В процессе изучения вопроса были сформулированы следующие требования к формированию XML-файла:  каждый открывающий XML-тэг определяет некоторую область данных в документе и должен иметь своего закрывающего «напарника»;  в XML-файле учитывается регистр символов;  все значения атрибутов, используемых в определении тэгов, должны быть заключены в кавычки;  вложенность тэгов в XML-файле строго контролируется, поэтому необходимо следить за очередностью открывающих и закрывающих тэгов;  вся информация, располагающаяся между начальным и конечными тэгами, рассматривается в XML-файле как данные, и поэтому учитываются все символы форматирования (пробелы, переводы строк, табуляции не игнорируются, как в HTML). После соблюдения этих правил документ принято считать формально правильным и все анализаторы, предназначенные для разбора XML-документов, смогут работать корректно . Однако очень важно отметить, что, помимо проверки на формальное соответствие грамматике языка, в XML-файле могут присутствовать средства контроля над его содержанием, за соблюдением правил, определяющих необходимые соотношения между элементами и формирующих структуру XML-файла. Чтобы обеспечить проверку корректности XML-файла, необходимо использовать анализаторы, производящие подобную проверку. На сегодняшний день практикуются два способа контроля соответствия XML-файла вышеописанным правилам: DTD-определения (DocumentTypeDefinition) и схемы данных (SemanticSchema) . Представим фрагмент конфигурационного файла информационной системы «Карма» после завершения вышеописанных действий По завершении анализа конфигурационного файла необходимо выбрать подходящую программную реализацию для внесения экономических параметров в конфигурационный файл без взаимодействия с «Кармой». Наиболее подходящим программным методом решения данной проблемы явился язык запросов к элементам XML-файла – XPath (XML PathLanguage). Он был разработан для организации доступа к частям документа формата XML в файлах трансформации XSLT (eXtensibleStylesheetLanguageTransformations) и является стандартом консорциума W3C . На каждом шаге отбираются элементы дерева XML-файла, соответствующие последовательности обращения к параметрам модели «Карма». В результате формируется множество элементов дерева, отвечающих структуре входной информации , которые могут использоваться для проведения расчетов в нем. Представим фрагмент программного кода «обращение к элементу дерева в XML-файле» Заключение В результате проведенной работы удалось реализовать программный модуль для внесения параметров в конфигурационный файл автоматизированной информационной системы финансово-аналитического содержания . В перспективе данное решение предоставляет возможность конечному пользователю взаимодействовать с системой «Карма» без непосредственной установки ее на персональном компьютере, обеспечивая при этом доступ к функциональным возможностям автоматизированной информационной системы для проведения инвестиционного, производственного и финансового анализа проектов развития социально-экономических систем. Отметим, что предложенный комплекс, состоящий из финансово-аналитической системы и пакета автоматического внесения информации в нее, ориентирован на использование в СЦ социальноэкономического анализа, является удобным инструментом оперативной поддержки экспертных решений в случае как очного, так и удаленного присутствия экспертов. Данный комплекс прошел тестирование в СЦ регионального социально-экономического развития Кемеровского филиала Российского экономического университета имени Г.В. Плеханова.
Прогресс в области микроэлектроники и информационных технологий обусловил широкое распространение обработки в реальном времени больших потоков данных. Например, многие простые операции повседневной жизни, такие как использование кредитной карты или телефона, требуют автоматизированного создания, анализа и обработки различных данных. Поскольку эти операции часто выполняются большим числом участников, необходимы распределенные и массовые потоки данных. Точно так же социальные сети содержат большое количество специфических сетевых и текстовых потоков данных. Поэтому актуальна проблема создания моделей и алгоритмов, позволяющих эффективно обрабатывать большие потоки данных, особенно в условиях ограниченных временных и других ресурсов. Для обеспечения информационной и общественной безопасности важное значение имеет анализ в телекоммуникационных сетях контента, содержащего противоправную информацию (в том числе данных, связанных с терроризмом, наркоторговлей, сетевым экстремизмом, подготовкой протестных движений или массовых беспорядков). Целями данного обзора являются сравнение современных методов решения задачи классификации текстов, обнаружение тенденций развития данного направления, а также выбор наилучших алгоритмов для применения в исследовательских и коммерческих задачах. Методы классификации текстов лежат на стыке двух областей – информационного поиска и машинного обучения. Их сходство состоит в способах  представления самих документов и способах оценки качества алгоритмов. На сегодняшний день разработано большое количество методов и их различных вариаций для классификации текстов. Каждая группа методов имеет свои преимущества и недостатки, области применения, особенности и ограничения. Особый интерес представляет случай, когда данные поступают в виде потока, например в телекоммуникационных сетях. Определенные трудности возникают из-за того, что обучение модели всегда основывается на совокупности свойств набора документов. Эти совокупные свойства могут изменяться с течением времени, и при построении потокового классификатора необходимо учитывать возможные изменения исходного распределения данных . Желательно, чтобы выбранный метод мог поддерживать инкрементное обучение, то есть чтобы классификатор обучался на каждом отдельно взятом образце в режиме реального времени. При инкрементном обучении обучающие примеры поступают последовательно в процессе работы алгоритма, так что классификатор должен постоянно корректировать результаты обучения и дообучаться. При неинкрементном обучении вся обучающая выборка предоставляется сразу полностью. Ясно, что в случае инкрементного обучения поведение классификатора в процессе работы меняется, что уменьшает его предсказуемость и может осложнить настройку системы. В то же время инкрементное обучение делает систему гораздо более гибкой, адаптируемой к изменяющимся условиям. Особенности процесса классификации в потоке связаны еще с тем, что не всегда удается контролировать скорость поступления данных. Некоторые классы документов могут встречаться в потоке только время от времени. Обнаружить этот редкий класс бывает непросто, и классификация текстов в таких случаях становится чрезвычайно сложной задачей. Сравнение методов построения классификаторов является довольно сложной задачей по причине того, что разные входные данные могут приводить к различным результатам. Поэтому необходимо осуществить их программную реализацию и вычисление эффективности на одинаковых наборах документов для обучения и тестирования. Формальная постановка задачи классификации текстов Следует отличать классификацию от кластеризации. При классификации документов категории определены заранее, при кластеризации они не заданы и даже информация об их количестве может отсутствовать. Формально постановку задачи классификации можно записать следующим образом. Имеются множество документов  и множество возможных категорий (классов). Неизвестная целевая функция задается формулой.    Требуется построить классификатор, максимально близкий к. В такой постановке задачи следует отметить, что о категориях и документах нет никакой дополнительной информации, кроме той, которую можно извлечь из самого документа. Если классификатор выдает точный ответ  то классификация называется точной. Если классификатор определяет степень подобия (Categorization Status Value) документа,  то классификация называется пороговой. В общем случае процесс обучения с учителем (обучение по прецедентам, supervised learning) заключается в следующем. Системе предъявляется набор примеров, связанных с какой-либо заранее неизвестной закономерностью. Этот набор иногда называют обучающей выборкой. Ее используют для обучения классификатора и определения значения его параметров, при которых классификатор выдает лучший результат. Далее в системе вырабатываются решающие правила, с помощью которых происходит разделение множества примеров на заданные классы. Качество разделения проверяется тестовой выборкой примеров T. При этом необходимо, чтобы выполнялись условия  Для множества примеров известны значения целевой функции. Если в задаче каждому документу может соответствовать только одна категория, то имеет место однозначная классификация, а если произвольное количество категорий, то многозначная классификация. Частным случаем однозначной классификации является бинарная классификация, когда коллекцию документов нужно разбить на две непересекающиеся категории. Например, задача определения тональности высказываний (положительная или отрицательная окраска) или задача обнаружения спама (является сообщение спамом или нет) решается при помощи бинарного классификатора. Решение задачи классификации состоит из четырех последовательных этапов:  предобработка и индексация документов;  уменьшение размерности пространства признаков;  построение и обучение классификатора с помощью методов машинного обучения;  оценка качества классификации. При выборе конкретного алгоритма классификации следует учитывать особенности каждого из них. По-прежнему остается нерешенным вопрос определения набора классифицирующих признаков, их количества и способов вычисления весов. В алгоритмах глубокого обучения точность классификации сильно зависит от наличия обучающей выборки подходящего размера. Подготовка такой выборки – очень трудоемкий процесс. До сих пор остается также открытой проблема подбора параметров некоторых алгоритмов на этапе обучения. Далее подробно рассмотрен каждый из этапов, описаны различные алгоритмы построения классификаторов, проводимые с ними эксперименты и результаты этих экспериментов. Описание методов классификации На рисунке 1 представлена общая схема процесса классификации. Рассмотрим каждый из его этапов. Предобработка и индексация документов. Предварительная обработка текста включает в себя токенизацию, удаление функциональных слов (семантически нейтральных слов, таких как союзы, предлоги, артикли и пр.). Далее осуществляется морфологический анализ (производятся разметка по частям речи и стемматизация). Это позволяет значительно сократить размерность пространства. В результате в качестве признаков документа выступают все значимые слова, встречающиеся в документе. Индексация документов – это построение некоторой числовой модели текста, которая переводит текст в удобное для дальнейшей обработки представление. Например, модель «мешка слов» (bag-of-words) позволяет представить документ в виде многомерного вектора слов и их весов в документе . Другими словами, каждый документ – это вектор в многомерном пространстве, координаты которого соответствуют номерам слов, а значения координат – значениям весов. Другая распространенная модель индексации – Word2vec . Она представляет каждое слово в виде вектора, который содержит информацию о контекстных (сопутствующих) словах. Еще одна модель индексации основана на учете n-грамм , то есть последовательностей из соседних символов. Очевидно, что для обучающих и тестовых документов должен применяться один и тот же метод индексации. Уменьшение размерности пространства признаков. Вычислительная сложность различных методов классификации напрямую зависит от размерности пространства признаков. Поэтому для эффективной работы классификатора часто прибегают к сокращению числа используемых признаков (терминов). За счет уменьшения размерности пространства терминов можно снизить эффект переобучения – явление, при котором классификатор ориентируется на случайные или ошибочные характеристики обучающих данных, а не на важные и значимые. Переобученный классификатор хорошо работает на тех экземплярах, на которых он обучался, и значительно хуже на тестовых данных. Чтобы избежать переобучения, количество обучающих примеров должно быть соразмерно числу используемых терминов. В некоторых случаях сокращение размерности пространства признаков в 10 раз (и даже в 100) может приводить лишь к незначительному ухудшению работы классификатора. Существуют несколько способов определения веса признаков документа. Наиболее распространенный – вычисление функции TF-IDF . Его основная идея состоит в том, чтобы больший вес  получали слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах. Вычисляется частота термина TF (term frequency) – оценка важности слова в пределах одного документа d по формуле  где  количество употреблений слова в документе; общее число слов в документе. Обратная частота документа IDF (inverse document frequency) – инверсия частоты, с которой слово встречается в документах коллекции. IDF уменьшает вес общеупотребительных слов по формуле  где  общее количество документов в коллекции; количество всех документов, в которых встречается слово. Итоговый вес термина в документе относительно всей коллекции документов вычисляется по формуле.  Следует отметить, что по формуле  оценивается значимость термина только с точки зрения частоты вхождения в документ, без учета порядка следования терминов в документе и их лексической сочетаемости. Для уменьшения размерности пространства терминов также применяют латентно-семантический анализ (LSA), использующий сингулярное разложение матриц , поточечную взаимную информацию (PMI)  (обобщение скрытой марковской модели). Встречаются исследования , в которых применяются статистические критерии и относительная энтропия для вероятностных распределений, называемая коэффициентом усиления информации, или дивергенцией Кульбака–Лейблера. Построение и обучение классификатора с помощью методов машинного обучения. Можно выделить следующие методы классификации:  вероятностные (например NB );  метрические (например KNN );  логические (например DT );  линейные (например SVM );  методы на основе искусственных нейронных сетей (например FFBP , DAN2). Далее обобщенно описываются эти методы, указываются преимущества и недостатки каждого из них. Метод Байеса (Naive Bayes, NB) относится к вероятностным методам классификации. Пусть вероятность того, что документ, представленный вектором, соответствует категории для . Задача классификатора заключается в том, чтобы подобрать такие значения , при которых значение вероятности будет максимальным:  Для вычисления значений пользуются теоремой Байеса где априорная вероятность того, что документ отнесен к категории вероятность найти документ, представленный вектором , в категории  вероятность того, что произвольно взятый документ можно представить в виде вектора признаков. По сути является отношением количества документов из обучающей выборки, отнесенных в категорию, к количеству всех документов из. не зависит от категории, а значения заданы заранее, поэтому знаменатель – это константа, не влияющая на выбор наибольшего из значений. Вычисление затруднительно из-за большого количества признаков, поэтому делают «наивное» предположение о том, что любые две координаты, рассматриваемые как случайные величины, статистически не зависят друг от друга. Тогда можно воспользоваться формулой. Далее все вероятности подсчитываются по методу максимального правдоподобия. Преимущества метода:  высокая скорость работы;  поддержка инкрементного обучения;  относительно простая программная реализация алгоритма;  легкая интерпретируемость результатов работы алгоритма. Недостатки метода: относительно низкое качество классификации и неспособность учитывать зависимость результата классификации от сочетания признаков. Метод k ближайших соседей (k Nearest Neighbors, KNN) относится к метрическим методам классификации. Чтобы найти категорию, соответствующую документу, классификатор сравнивает со всеми документами из обучающей выборки, то есть для каждого вычисляется расстояние. Далее из обучающей выборки выбираются k документов, ближайших к. Согласно методу ближайших соседей, документ считается принадлежащим тому классу, который является наиболее распространенным среди соседей данного документа, то есть для каждого класса вычисляется функция ранжирования ближайшие документов; известные величины, уже расклассифицированные по категориям документы обучающей выборки. Преимущества метода:  возможность обновления обучающей выборки без переобучения классификатора;  устойчивость алгоритма к аномальным выбросам в исходных данных;  относительно простая программная реализация алгоритма;  легкая интерпретируемость результатов работы алгоритма;  хорошее обучение в случае с линейно неразделимыми выборками. Недостатки метода:  репрезентативность набора данных, используемого для алгоритма;  высокая зависимость результатов классификации от выбранной метрики;  большая длительность работы из-за необходимости полного перебора обучающей выборки;  невозможность решения задач большой размерности по количеству классов и документов. Метод деревьев решений (Decision Trees, DT) относится к логическим методам классификации. Деревом решений называют ациклический граф, по которому производится классификация объектов (в нашем случае текстовых документов), описанных набором признаков. Каждый узел дерева содержит условие ветвления по одному из признаков. У каждого узла столько ветвлений, сколько значений имеет выбранный признак. В процессе классификации осуществляются последовательные переходы от одного узла к другому в соответствии со значениями признаков объекта. Классификация считается завершенной, когда достигнут один из листьев (конечных узлов) дерева. Значение этого листа определит класс, которому принадлежит рассматриваемый объект. На практике обычно используют бинарные деревья решений, в которых принятие решения перехода по ребрам осуществляется простой проверкой наличия признака в документе. Если значение признака меньше определенного значения, выбирается одна ветвь, если больше или равно, другая. В отличие от остальных подходов, представленных ранее, подход, использующий деревья решений, относится к символьным (то есть нечисловым) алгоритмам. Алгоритм построения бинарного дерева решений состоит из следующих шагов. Создается первый узел дерева, в который входят все документы, представленные всеми имеющимися признаками. Размер вектора признаков для каждого документа равен, так как. Для текущего узла дерева выбираются наиболее подходящий признак и его наилучшее пограничное значение. На основе пограничного значения выбранного признака производится разделение обучающей выборки на две части. Далее выбранный признак не включается в описание фрагментов в этих частях, то есть фрагменты в частях представляются вектором с размерностью. Образовавшиеся подмножества обрабатываются аналогично до тех пор, пока в каждом из них не останутся документы только одного класса или признаки для различения документов. Когда говорят о выборе наиболее подходящего признака, как правило, подразумевают частотный признак, то есть любой признак текста, допускающий возможность нахождения частоты его появления в тексте. Лучшим для разделения является признак, дающий максимальную на данном шаге информацию о категориях. Таким признаком для текста может являться, например, ключевое слово. С этой точки зрения любой частотный признак можно считать переменной. Тогда выбор между двумя наиболее подходящими признаками сводится к оценке степени связанности двух переменных. Поэтому для выбора подходящего признака на практике применяют различные критерии проверки гипотез, то есть критерии количественной оценки степени связанности двух переменных, поставленных во взаимное соответствие, где 0 соответствует полной независимости переменных, а 1 – их максимальной зависимости. Для исследования связи между двумя переменными удобно использовать представление совместного распределения этих переменных в виде таблицы сопряженности (факторной таблицы, или матрицы частот появления признаков). Она является наиболее универсальным средством изучения статистических связей, так как в ней могут быть представлены переменные с любым уровнем измерения. Таблицы сопряженности часто используются для проверки гипотезы о наличии связи между двумя признаками при помощи различных статистических критериев: критерия Фишера (точного теста Фишера), критерия согласия Пирсона (критерия хи-квадрат), критерия Крамера, критерия Стьюдента (t-критерия Стьюдента) и пр.  Преимущества метода:  относительно простая программная реализация алгоритма;  легкая интерпретируемость результатов работы алгоритма. Недостатки метода: неустойчивость алгоритма по отношению к выбросам в исходных данных и большой объем данных для получения точных результатов. Метод опорных векторов (Support Vector Machine, SVM) является линейным методом классификации. В настоящее время этот метод считается одним из лучших. Рассмотрим множество документов, которые необходимо расклассифицировать. Сопоставим ему множество точек в пространстве размерности. Выборку точек называют линейно разделимой, если принадлежащие разным классам точки можно разделить с помощью гиперплоскости (в двухмерном случае гиперплоскостью является прямая линия). Очевидный способ решения задачи в таком случае – провести прямую так, чтобы по одну сторону от нее лежали все точки одного класса, а по другую – все точки другого класса. Тогда для классификации неизвестных точек достаточно будет посмотреть, с какой стороны прямой они окажутся. В общем случае можно провести бесконечное множество гиперплоскостей (прямых), удовлетворяющих нашему условию. Ясно, что лучше всего выбрать прямую, максимально удаленную от имеющихся точек. В методе опорных векторов расстоянием между прямой и множеством точек считается расстояние между прямой и ближайшей к ней точкой из множества. Именно такое расстояние и максимизируется в данном методе. Гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей, называется разделяющей (на рисунке 2 обозначена буквой). Ближайшие к параллельным гиперплоскостям точки называются опорными векторами (рис. 2), через них проходят пунктирные линии. Другими словами, алгоритм работает в предположении, что, чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора, так как максимизация зазора между классами способствует более уверенной классификации. На практике структура данных зачастую бывает неизвестна и очень редко удается построить разделяющую гиперплоскость, а значит, невозможно гарантировать линейную разделимость выборки. Могут существовать такие документы, которые алгоритм отнесет к одному классу, а в действительности они должны относиться к противоположному. Такие данные называются выбросами, они создают погрешность метода, поэтому было бы лучше их игнорировать. В этом заключается суть проблемы линейной неразделимости.  Выборку называют линейно неразделимой, если точки, принадлежащие разным классам, нельзя разделить с помощью гиперплоскости. Когда такой разделяющей гиперплоскости не существует, необходимо перейти от исходного пространства признаков документов к новому, в котором обучающая выборка окажется линейно разделимой. Для этого каждое скалярное произведение необходимо заменить на некоторую функцию, отвечающую определенным требованиям. Например, можно назначать некий штраф за каждый неверно расклассифицированный документ. Эту функцию называют ядром. Замена скалярного произведения функцией-ядром позволяет перейти к другому пространству признаков, где данные уже будут разделимы. В случае линейной неразделимости проблема поиска оптимальной разделяющей гиперплоскости сводится к задаче, эквивалентной поиску седловой точки функции Лагранжа с условиями дополняющей нежесткости. Полученная система уравнений решается методами квадратичного программирования. Это уже чисто вычислительная задача. Этот вариант алгоритма называют алгоритмом с мягким зазором (soft-margin SVM), тогда как в линейно разделимом случае говорят о жестком зазоре (hard-margin SVM). Преимущества метода:  один из наиболее качественных методов;  возможность работы с небольшим набором данных для обучения;  сводимость к задаче выпуклой оптимизации, имеющей единственное решение. Недостатки метода: сложная интерпретируемость параметров алгоритма и неустойчивость по отношению к выбросам в исходных данных. Логистическая регрессия (logit model, logistic regression) является линейным методом классификации. Этот метод используется для предсказания вероятности возникновения некоторого события по значениям множества признаков. Для этого вводятся так называемая зависимая переменная y, которая может принимать лишь одно из двух значений – как правило, это числа 0 (событие не произошло) и 1 (событие произошло), и множество независимых переменных (также называемых признаками, предикторами или регрессорами) – вещественных, на основе значений которых требуется вычислить вероятность принятия того или иного значения зависимой переменной. В случае классификации документов роль зависимой переменной выполняет категория, а роль независимых переменных – набор документов. Для улучшения обобщающей способности алгоритма, то есть для уменьшения эффекта переобучения, на практике часто рассматривается логистическая регрессия с регуляризацией. Регуляризация заключается в том, что вектор параметров рассматривается как случайный вектор с некоторой заданной априорной плотностью распределения. Для обучения модели вместо метода наибольшего правдоподобия при этом используется метод максимизации апостериорной оценки, то есть должны быть найдены параметры, максимизирующие величину.  Мультиномиальная логистическая регрессия – это общий случай модели логистической регрессии, в которой зависимая переменная имеет более двух категорий. В модели мультиномиальной логистической регрессии для каждой категории зависимой переменной строится уравнение бинарной логистической регрессии. При этом одна из категорий зависимой переменной становится опорной, а все другие категории сравниваются с ней. Уравнение мультиномиальной логистической регрессии прогнозирует вероятность принадлежности к каждой категории зависимой переменной по значениям независимых переменных. Вообще говоря, логистическую регрессию можно представить в виде однослойной нейронной сети с сигмоидальной функцией активации, веса которой – коэффициенты логистической регрессии, а вес поляризации – константа регрессионного уравнения.  Преимущества метода:  является одним из наиболее качественных;  поддерживает инкрементное обучение;  имеет относительно простую программную реализацию алгоритма. Недостатки метода: сложная интерпретируемость параметров алгоритма и неустойчивость по отношению к выбросам в исходных данных.  Методы на основе искусственных нейронных сетей. Существует большое количество разновидностей нейронных сетей, основные из них – сети прямого распространения, рекуррентные сети, радиально-базисные функции и самоорганизующиеся карты. Настройка весов может быть фиксированной или динамической. В классических нейронных сетях прямого распространения (Feed Forward Back Propagation, FFBP) присутствуют входной слой, выходной слой и промежуточные слои: сигнал идет последовательно от входного слоя нейронов по промежуточным слоям к выходному. Примером такой структуры является многослойный перцептрон. Для классификации документа при помощи нейронной сети прямого распространения веса признаков документа подаются на соответствующие входы сети. Активация распространяется по сети; значения, получившиеся на выходах, и есть результат классификации. Стандартный метод обучения такой сети – метод обратного распространения ошибки. Суть его в следующем: если на одном из выходов для одного из обучающих документов получен неправильный ответ, то ошибка распространяется обратно по сети и веса ребер меняются так, чтобы уменьшить ошибку. Количество промежуточных слоев нейронной сети может быть не задано заранее, такую архитектуру называют динамической. В этом случае слои последовательно динамически генерируются до тех пор, пока не будет достигнут нужный уровень точности. Обобщенная схема DAN2 приведена на рисунке 3, взятом из статьи . Каждый элемент Fk представляет собой функцию, которая содержит текущий элемент накопленных знаний (Current Accumulated Knowledge Element), полученный на предыдущем шаге обучения сети. C обозначают константы. Вершины Gk и Hk представляют собой текущие остаточные нелинейные компоненты процесса по передаточной функции взвешенной и нормализованной суммы входных переменных (Current Residual Nonlinear Element). Сверточная нейронная сеть – однонаправленная многослойная сеть с применением операции свертки, при которой каждый фрагмент входных данных умножается на матрицу (ядро) свертки поэлементно, а результат суммируется и записывается в аналогичную позицию выходных данных. Обобщенная схема CNN приведена на рисунке 4, взятом из статьи . Рекуррентная нейронная сеть получается из многослойного перцептрона введением обратных связей. Одна из широко распространенных разновидностей рекуррентных нейронных сетей – сеть Элмана – изображена на рисунке 5 . В ней обратные связи идут не от выхода сети, а от выходов внутренних нейронов. Это позволяет учесть предысторию наблюдаемых процессов и накопить информацию для выработки правильной стратегии обучения. Главной особенностью рекуррентных нейронных сетей является запоминание последовательностей. Скрытый слой в период времени вычисляется путем преобразования текущего входного слоя и предыдущего скрытого слоя. Далее из скрытого слоя результат поступает на выходной слой. Преимущества метода:  имеет очень высокое качество алгоритма при удачном подборе параметров;  является универсальным аппроксиматором непрерывных функций;  поддерживает инкрементное обучение. Недостатки метода:  вероятность возможной расходимости или медленной сходимости, поскольку для настройки сети используются градиентные методы;  необходимость очень большого объема данных для обучения, чтобы достичь высокой точности;  низкая скорость обучения;  сложная интерпретируемость параметров алгоритма. Оценка качества классификации Для обучения и оценки качества классификации, как уже отмечалось ранее, требуются обучающая и тестовая выборки. Прежде всего нужно выбрать обучающую и тестовую выборки, далее по обучающей выборке найти оптимальные признаки, а потом проверять качество на тестовой. Если сначала найти оптимальные признаки по всей выборке, а потом оценивать качество алгоритма, то отобранные признаки уже оптимизируют качество, поэтому оценка будет слишком оптимистичной. Чтобы оценка качества классификатора была объективной, необходимо правильно выбрать соотношение объемов этих выборок. Если взять очень маленькую обучающую выборку, оценка качества будет слишком пессимистичной. Если тестовая выборка будет маленькая, оценка окажется неточной. Как правило, обучающую и тестовую выборки берут исходя из соотношения 70/30. Однако есть более объективный способ оценки качества классификатора – кросс-валидация. Суть ее состоит в следующем: все множество разбивается на частей, каждая из них по очереди выступает как тестовая. Здесь важно сделать оптимальный выбор. Обычно предпочитают брать или. Главный недостаток такого способа оценки – большие трудозатраты. Основным критерием при оценке качества классификации является комбинация точности и полноты. Точность (precision) классификации в пределах класса – это доля найденных классификатором документов, действительно принадлежащих данному классу, относительно всех документов, которые система отнесла к этому классу. Полнота (recall) классификации – это доля найденных классификатором документов, действительно принадлежащих классу, относительно всех документов этого класса в тестовой выборке. Оценка качества работы классификатора производится на тестовой выборке. Вместе с тем работу системы оценивает эксперт (см. табл. 1). В таблице приняты следующие условные обозначения: TP – истинно положительное решение; TN – истинно отрицательное решение; FP – ложно положительное решение; FN – ложно отрицательное решение. Согласно определению, точность вычисляется следующим образом.  Полнота вычисляется по формуле.  F-мера – характеристика качества работы алгоритма, которая объединяет в себе информацию о точности и полноте.  При большее значение имеет точность. При точность и полнота равноправны, тогда. При большее значение имеет полнота. Часто можно встретить другую формулу для вычисления точности (accuracy). Эту величину иногда называют правильностью или аккуратностью метода В некоторых случаях удобнее от долей перейти к процентам, умножив полученную величину на 100. Иногда для сравнения алгоритмов классификации используют специфические характеристики, такие как точка безубыточности, или сбалансированная точность. Точка безубыточности (break even point, BEP) – величина, заимствованная из экономики, отражающая объем производства и реализации продукции, при котором расходы будут компенсированы доходами, а при производстве и реализации каждой последующей единицы продукции предприятие начинает получать прибыль. В контексте рассматриваемой задачи точка безубыточности используется как мера качества классификации. Точка безубыточности наравне с F-мерой является сбалансированной характеристикой точности и полноты. Более подробное пояснение можно найти в . Под быстродействием классификатора понимается время, затрачиваемое на отнесение документа к одному из классов. Применительно к задачам классификации текстов быстродействие измеряется как процессорное время (в секундах) или как количество вычислительных операций, необходимое для классификации. Измерение производят на обучающей выборке для оценки скорости процесса обучения и отдельно на тестовой выборке. Следует отметить, что высокие затраты при обучении в дальнейшем оправдываются за счет многократного использования настроенного классификатора. Ясно, что увеличение точности классификации обычно приводит к снижению быстродействия изза усложнения решающего правила, используемого в алгоритме классификации, а увеличение быстродействия сопровождается понижением точности из-за упрощения работы классификатора. Эксперименты по сравнению методов В работе  предложен алгоритм классификации на основе нейронных сетей с динамической архитектурой DAN2. Этот вид сетей был выбран на основании экспериментального сравнения DAN2 с обычными нейронными сетями прямого распространения (FFBP) и рекуррентными нейронными сетями (RNN). Для сравнения качества классификации в  были рассмотрены DAN2, KNN и SVM.  Классификация проводилась на широко известной коллекции данных Reuters-21578, собранной и размеченной в 2004 году Д. Льюисом. Коллекция содержит 21 578 документов из ленты новостей Reuters. Обучающая выборка состоит из 9 603 документов. Тестовая выборка включает в себя 3 299 документов. В эксперименте разбиение производилось на десять наиболее часто встречающихся категорий, связанных с экономикой (нефть, пшеница, кукуруза, торговля, деньги и пр.). Авторы пришли к выводу, что оптимальное количество признаков для каждого из методов – около 2 200. Сравнение качества алгоритмов осуществлялось при помощи точности, полноты, точки безубыточности и F-меры.  Время, потраченное на обучение классификатора с использованием DAN2, варьируется в зависимости от категории, время работы уже обученного классификатора на тестовой выборке для выбранных десяти категорий составляет. Для экспериментов использовался многоядерный сервер со следующей конфигурацией. Операционная система SuSE Linux Enterprise Server (SLES, 11) 64-bit. Еще пробовали VMWare Server, OpenMPI. На основе полученных экспериментальных данных можно прийти к выводу, что DAN2 опережает KNN для всех десяти категорий и опережает SVM для девяти из десяти категорий. Вместе с тем следует отметить, что применение нейронных сетей сильно замедляет работу классификатора на этапе обучения. В работе  утверждается, что формально SVM и нейронная сеть прямого распространения (FFNN) имеют похожую структуру, так как выходная функция может быть представлена в виде линейной комбинации простых функций, то есть. В таком случае количество скрытых нейронов является долей числа опорных векторов (табл. 2). Более подробное описание используемой в таблице общепринятой терминологии можно найти, например, в . Кроме того, SVM используется в задаче выпуклой оптимизации, которая всегда позволяет найти глобальный минимум и единственное решение, в то время как FFNN тренируется при помощи метода градиентного спуска, который не всегда сходится к оптимальному (глобальному) решению. В статье  предложены техники для минимизации случая локальной сходимости, а также показано, что масштабированный метод сопряженных градиентов не сходится реже, чем традиционный метод сопряженных градиентов или метод обратного распространения с использованием градиентного спуска. В подтверждение своих наблюдений авторы приводят описание эксперимента по разделению отзывов о фильмах, книгах, GPS и фотоаппаратах на положительные и отрицательные с использованием методов SVM, NB и ANN. В методе SVM в качестве обычного нелинейного ядра была взята радиальная базисная функция. В методе с искусственными нейронными сетями было отдано предпочтение прямоточной нейронной сети (однонаправленной сети с одним скрытым слоем). Для обучения нейронной сети использовался алгоритм обратного распространения ошибки (Backpropagation). Чтобы ускорить процесс обучения и сократить риск переобучения, применялась технология «ранней остановки». Результаты классификации сравнивались на сбалансированных и несбалансированных данных. Данные для категории «фильмы» были взяты из популярной, часто цитируемой базы Movie Review Data , для остальных категорий авторы собирали коллекции самостоятельно с сайта Amazon  по 2 000 отзывов для каждого класса. Характеристиками для сравнения являлись точность (precision), полнота (recall), аккуратность (accuracy) и время в секундах. Сравнение осуществлялось с помощью 10-проходной кросс-валидации. Количество признаков в экспериментах варьировалось от 50 до 5 000. В среднем наилучшие результаты были получены при 500–1 000. Было замечено, что на 5 000 терминов для ANN время на обучение значительно увеличивается, а время работы не меняется; для SVM, наоборот, время на обучение не меняется, но слишком большое количество признаков сильно сказывается на длительности работы. На сбалансированных данных было проведено 28 тестов для четырех категорий. ANN показал лучший результат, чем SVM, в 13 тестах (t-тест с); SVM превзошел ANN только в 2, хотя в целом разница в результатах не превысила 3 %. Худшие результаты получены для класса «книги». Точность 0,88 % при 3 000 терминов для SVM; 0,86 % при 3 000 и 4 000 терминов для SVM. Полнота 0,8 при 1 000 терминов для ANN; 0,88 при 3 000 для SVM. Лучшая аккуратность (accuracy) 81,8 % при 1 000 терминов достигается для ANN. Для остальных трех категорий результаты лучше. Для класса GPS лучшая точность 0,96 и лучшая полнота 0,99 достигаются методом NB, лучшая аккуратность (accuracy) 87,3 % – методом ANN. Для класса «фильмы» лучшая точность 0,95 достигается NB, на втором месте 0,87 – ANN, лучшая полнота 0,98 – NB, на втором месте 0,87 – ANN, лучшая аккуратность (accuracy) 86,5 % получена методом ANN. Для класса «фотоаппараты» лучшая точность 0,94 получена методом NB, лучшая полнота 0,96 – NB, лучшая аккуратность (accuracy) 90,3 % – ANN. Время работы зависит от количества векторов для SVM и количества слоев для ANN. Наравне с ними рассматривался Байес. Он, бесспорно, быстрее всех при любом количестве терминов для любого класса и в некоторых случаях, как ни странно, показывал лучшую точность. Время на обучение для класса «книги»: SVM – 0,22–1,5 с, ANN – 3,7–69,4 с в зависимости от количества признаков (50–5 000). Время на обучение для класса GPS: SVM – 0,2–1,3 с, ANN – 3,1–75,4 с. Время на обучение для класса «фильмы»: SVM – 0,27–5,6 с, ANN – 2,3–65,5 с. Время на обучение для класса «фотоаппараты»: SVM – 0,2–1,1 с, ANN – 4,5–77,2 с. К сожалению, в статье отсутствуют данные об аппаратном обеспечении, на котором проводилось исследование. В работе  рассматриваются методы обнаружения ложных высказываний в текстах, когда люди намеренно говорят неправду, пытаясь обмануть. Авторы исследовали высказывания людей, вовлеченных в преступления на военных базах. Подозреваемые и свидетели описывали события своими словами. Сотрудники правоохранительных органов находили в архивных данных либо подтверждения, либо опровержения этим высказываниям. Таким образом оценивали истинность высказываний либо их ложность. Проанализировано 371 сообщение из специальных архивов о различных видах преступлений: дорожные нарушения, магазинные кражи, нападения и поджоги. Большой проблемой было собрать такую коллекцию данных, для которой можно установить истинность/ложность высказываний. Для классификации первоначально экспертами был составлен перечень из 31 признака. Далее при помощи критерия хи-квадрат было выбрано 13 наиболее подходящих признаков: количество глаголов движения, личных местоимений, количество слов с оттенком намерения, причины, с указанием времени, лексическое разнообразие и пр. Некоторые из отобранных признаков весьма специфичны и требуют составления семантических словарей. В эксперименте проводилось сравнение многослойного перцептрона (MLP, разновидность FFNN), модификации деревьев решений (CART), логистической регрессии и ансамбля классификаторов. Для построения ансамбля классификаторов, как правило, используются два основных метода: бустинг (boosting) и бэггинг (bagging). При бустинге происходит последовательное обучение классификаторов. Например, первый классификатор обучается на всем наборе данных, второй – на выборке примеров, а третий – на наборе тех данных, в которых результаты первых двух классификаторов разошлись. Бэггинг использует параллельное обучение базовых классификаторов, то есть бэггинг является улучшающим объединением, а бустинг – улучшающим пересечением. Для проверки использовалась 10-проходная кросс-валидация. Для метода MLP достигнута точность 73,46 %, для CART – 71,60 %, для логистической регрессии – 67,28 %. В результате был сделан вывод, что наиболее высокая точность, 74,07 %, достигается на ансамбле классификаторов. Преимуществом ансамблевых классификаторов является качество их работы на неравномерно распределенных данных. Эта особенность важна при потоковой обработке данных, когда некоторые редкие классы документов, появляющиеся и исчезающие в потоке, порой непросто обнаружить. В работе  рассмотрено решение сразу двух задач: извлечение терминов и классификация для англоязычного и русскоязычного корпусов. Необходимо было определить эмоциональную окраску отзывов о ресторанах и автомобилях, то есть разделить отзывы на положительные и отрицательные, построив бинарный классификатор для каждой из категорий «рестораны» и «автомобили». Для классификации использовались рекуррентные нейронные сети, в частности, нейронные сети Элмана (простые и двунаправленные BRNN) и LSTM. Для извлечения аспектных терминов в сравнении участвовали несколько методов: два вида многослойного персептрона (MLP), логистическая регрессия и условные случайные поля (CRF). Для условных случайных полей в качестве признаков использовались основы слов и принадлежность частям речи (проводилась процедура POS-tagging). Использовались две метрики для извлечения аспектных терминов: на основе точного количества и на основе пропорционального перекрытия. Сравнение результатов классификации для английских текстов проводилось на недавно собранной коллекции SemEval-2014 ABSA Restaurants. Обучающая выборка состояла из 3 041 сообщения, тестовая – из 800. Для проверки качества методов на русских текстах была использована коллекция отзывов о ресторанах и автомобилях, собранная для проведения соревнований SentiRuEval-2015 в рамках конференции «Диалог». Для проверки полученных результатов применялась 7-проходная кросс-валидация. Лучшие результаты как при извлечении терминов, так и при классификации отзывов показал метод LSTM. При извлечении терминов F-мера для LSTM составила 79,80 %. При классификации точность – 69,70 %. Метод LSTM для русскоязычных данных для класса «рестораны» показал точность 61,1 %, F-меру – 70,2 %. Для класса «автомобили» результаты хуже: точность – 58,0 %, F-мера – 62,4 %. Проблема тематической классификации коротких текстовых сообщений (от нескольких слов до 2 предложений), например, смс-сообщений, комментариев к новостям, на форумах, в социальных сетях, рассматривается в . Основная сложность, возникающая при решении этой проблемы, – определение набора признаков, по которым предстоит классифицировать. В качестве признаков классификации принято рассматривать слова и словосочетания, буквы и буквосочетания. Один из недостатков распространенных на сегодняшний день методов – привязка к конкретному естественному языку и опора на словари. В данной публикации уделяется внимание определению универсальных методов и дифференцирующих признаков для текстов на различных естественных языках. В данной работе также описан алгоритм построения и обучения классификатора на основе метода взаимной информации (PMI). Применялась процедура POS-tagging, и для классификации были выбраны следующие признаки: N – существительные, NA – существительные и прилагательные, NAV – существительные, прилагательные, глаголы, NNP – существительные и именные группы, VVP – глаголы и глагольные группы, Stem – псевдоосновы словоупотреблений текста, полученные алгоритмами аналитического морфологического анализа (имеются в виду широко известные алгоритмы Портера, Ловинса, Пейса–Хаска и пр.). Принадлежность текста к категории определяется наличием в нем признаков, релевантных данной категории и коррелирующих с признаками рассматриваемой категории, а также отсутствием нерелевантных признаков и признаков, не коррелирующих с признаками данной категории . При таком подходе тексту можно сопоставить информационную матрицу I, элементы которой определяются как пара, где  коэффициент релевантности; коэффициент корреляции. Для коэффициентов релевантности и корреляции справедливо следующее утверждение: большие значения этих коэффициентов соответствуют признакам, наиболее точно характеризующим выбранный класс. Пороговые значения релевантности и корреляции служат параметрами, определяющими точность. В процессе классификации вычисляются коэффициенты релевантности и корреляции для каждого текста как суммы соответствующих коэффициентов для данного класса по всем вхождениям признаков. Документ считается отнесенным к тем категориям, для которых произошло превышение пороговых значений по обеим характеристикам: как по коэффициенту корреляции, так и по коэффициенту релевантности. Пороговые значения для каждой категории могут быть заданы пользователем или же рассчитаны автоматически по обучающей выборке. В работе  представлены результаты экспериментов для метода NB и метода на основе PMI. Обучение классификаторов проводилось на созданных экспертами выборках текстов с сайтов из Интернета: для русского языка объемом 57,3 Мб, башкирского – 1,87 Мб, татарского – 2,68 Мб. В качестве классов условно были выбраны «наркотики», «насилие», «национализм», «отрицание традиционных ценностей», «порнография», «терроризм», «фашизм», «экстремизм». Учет выбранных морфологических признаков оказывает различное влияние на качество классификации в зависимости от класса. Для некоторых классов (например «фашизм») могут оказывать положительное влияние существительные и именные группы, а на определение некоторых тематик (например «наркотики», «фашизм») отрицательное влияние оказывает учет глагольных групп. Лучшие значения F-меры достигаются предложенным методом (на основе PMI, в качестве признаков – псевдоосновы) для классов: «жестокость» – 0,913, «отрицание традиционных ценностей» – 0,862, «наркотики» – 0,765. В итоге можно сделать вывод, что псевдоосновы, выделенные аналитическим алгоритмом морфологического анализа, могут считаться универсальными дифференцирующими признаками при классификации коротких текстовых сообщений. В работе  предложен алгоритм классификации с применением сверточных нейронных сетей (CNN), проведено сравнение этого метода с методом на основе рекуррентных нейронных сетей (LSTM) и мультиномиальной логистической регрессией (logit model) в разных вариациях («мешок слов», TF-IDF, n-граммы, Word2vec). Тестирование сверточных нейронных сетей проводилось на данных, основанных на словах (word-based) и на символах (character-based). Особенность метода CNN заключается в необходимости использования очень больших коллекций для обучения. Большинство открытых коллекций для классификации текстов (даже на английском языке) слишком малы, поэтому для эксперимента авторы самостоятельно собрали тексты с новостных сайтов, обзоры и отзывы пользователей, данные из DBPedia, которая содержит структурированные данные из википедии. В результате были получены коллекции данных на английском и китайском языках. Объемы собранных коллекций приведены в таблице 3: обучающие выборки содержали от 120 000 до 3 600 000 текстов, тестовые – от 7 600 до 650 000 текстов. Наименьшие погрешности измерения 1,31–7,64 % были достигнуты для моделей, использующих в качестве признаков n-граммы, а также сверточные нейронные сети (погрешность измерения 4,93–40,43 %) в зависимости от коллекции. Самые плохие результаты показала модель с применением метода Word2vec. Это означает, что получивший широкое распространение метод представления слов Word2vec в виде векторов не дает преимуществ в задаче классификации текстов. Хотя авторам статьи  еще предстоит детально интерпретировать полученные результаты, а также продолжать эксперименты на коллекциях текстов для других языков, сейчас можно сделать вывод, что для задачи классификации текстов лучшим оказался символьный уровень, когда рассматриваются буквосочетания (без привязки к конкретному языку). Результаты исследования Классификация текстов является одной из основных задач компьютерной лингвистики, поскольку к ней сводится ряд других задач: определение тематической принадлежности текстов, автора текста, эмоциональной окраски высказываний и др. Формально задачу классификации текстов можно описать следующим образом. Имеется множество документов и множество возможных категорий (классов). Требуется построить классификатор, относящий выбранный документ к одной из нескольких заранее определенных категорий на основании содержания документа. Наиболее распространенный современный подход к классификации основывается на методах машинного обучения. Согласно этим методам, набор правил или критерий принятия решения текстового классификатора вычисляется автоматически на основе обучающих данных. Обучающими данными являются образцы документов из каждого класса. Решение задачи классификации состоит из четырех последовательных этапов: предобработка и индексация документов, уменьшение размерности пространства признаков, построение и обучение классификатора с помощью методов машинного обучения, оценка качества классификации. Для предварительной обработки и индексации документа (то есть при построении некоторой числовой модели текста) обычно применяется одна из трех моделей: модель «мешка слов», Word2vec и модель, основанная на учете n-грамм. Для реализации первой и второй моделей необходимы дополнительные знания о морфологической и синтаксической структуре языка. Применение символьных n-грамм позволяет не накладывать ограничения на использование конкретного языка, поэтому в ряде случаев является предпочтительным. Вычислительная сложность различных методов классификации напрямую зависит от размерности пространства признаков. За счет уменьшения размерности пространства терминов можно снизить эффект переобучения – явление, при котором классификатор ориентируется на случайные или ошибочные характеристики обучающих данных, а не на важные и значимые. Переобученный классификатор хорошо работает на тех экземплярах, на которых он обучался, и значительно хуже на тестовых данных. Чтобы избежать переобучения, количество обучающих примеров должно быть соразмерно числу используемых терминов, поэтому для эффективной работы классификатора часто прибегают к сокращению числа используемых признаков (терминов). Для уменьшения размерности пространства терминов применяют такие методы, как LSA, TF-IDF, PMI, CRF, IG. Наибольшее распространение из них получил метод TF-IDF. Выводы В статье были рассмотрены следующие наиболее распространенные методы построения и обучения классификатора: NB, KNN, SVM, DT, логистическая регрессия и алгоритмы глубокого обучения, основанные на искусственных нейронных сетях (FFBP, RNN, DAN2, CNN). Для обучения и оценки качества классификации необходимо подготовить обучающую и тестовую выборки, далее по обучающей выборке найти оптимальные признаки, а затем проверять качество на тестовой выборке. Чтобы оценка качества классификатора была объективной, требуется правильно выбрать соотношение объемов этих выборок. Как правило, обучающую и тестовую выборки берут исходя из соотношения 70/30. Более объективным способом оценки качества классификатора является кросс-валидация. Общепризнанными характеристиками качества работы классификатора являются точность, полнота и их комбинация (F-мера). На основе проведенного исследования можно сделать вывод, что наилучшее соотношение этих характеристик достигается при использовании методов SVM (точность 80–85 %, полнота 83–87 %) и CNN (точность 90–95 %, полнота 80–85 %). Помимо характеристик качества классификации, целесообразно учитывать также другие факторы: время работы алгоритма, возможность работы алгоритма в инкрементном режиме, количество предварительной информации, необходимой для классификации, независимость от языка. Скорость работы алгоритма NB одна из самых высоких, однако точность для различных экспериментов сильно варьируется (71–90 %). При потоковой обработке текстов классификация документов должна осуществляться одновременно с поступлением их из источника, поэтому предпочтение должно отдаваться инкрементным алгоритмам, таким как CNN или SVM. В настоящее время по-прежнему остается нерешенным вопрос определения набора классифицирующих признаков, их количества и способов вычисления весов. При выборе определенного метода следует помнить, что при большом количестве признаков (около 5 000) время на обучение для нейронных сетей значительно увеличивается, а время работы не меняется; для SVM, наоборот, время на обучение не меняется, но слишком большое количество признаков сильно сказывается на длительности работы. Оптимальным является 500–1 000 признаков, в некоторых случаях – до 2 200 признаков. В качестве признаков удобно рассматривать частоты символьных n-грамм, чтобы не накладывать ограничения на использование конкретного языка. В алгоритмах глубокого обучения точность классификации существенно зависит от наличия обучающей выборки подходящего размера. Подготовка такой выборки – очень трудоемкий процесс. Подбор параметров некоторых алгоритмов на этапе обучения до сих пор также остается открытой проблемой. Согласно результатам проведенного исследования, для обучения и тестирования классификатора с использованием метода SVM на русском языке нужна размеченная коллекция текстов объемом 1 000–2 000 текстов для достижения точности 80–85 %. Для обучения и тестирования классификатора с применением метода CNN необходимо собрать и подготовить коллекцию текстов на русском языке объемом около 1 000 000 документов для достижения точности 90–95 %. Следует отметить, что большинство упоминаемых в обзоре экспериментов проводилось на коллекциях англоязычных текстов. Довольно часто встречаются статьи с описанием исследований применительно к китайскому языку. Исследования по сравнению различных методов классификации для русскоязычных текстов проводятся в основном в контексте задачи сентимент-анализа, где рассматриваются два класса: положительный и отрицательный. Создание общедоступной коллекции необходимого размера позволило бы российским исследователям активнее изучать проблемы автоматической обработки текстовой информации в целом и вместе с тем разрабатывать новые инструменты для решения прикладных задач в данной области. Работа выполнена при финансовой поддержке Минобрнауки РФ (договор № 02.G25.31.0146) в рамках реализации Постановления Правительства РФ № 218.
Органы исполнительной и законодательной власти становятся все более открытыми для населения, и это приводит к росту количества заявлений, жалоб и предложений. Значительная часть этих обращений поступают в электронном виде, что обусловливает необходимость их автоматизированной обработки. По каждому обращению необходимо выполнить следующие действия:  принять обращение, определить суть проблемы и специалиста для решения данной проблемы;  отправить обращение конкретному специалисту;  принять некоторые меры по решению указанной проблемы;  написать ответ с отчетом о проделанной работе. Специфика автоматизированных систем обработки текстовых документов такого рода в нестационарности тезауруса ключевых слов, которые с выходом новых правовых документов кардинально изменяются, к тому же поступающие от населения обращения являются неструктурированными и короткими, что затрудняет статистический анализ. Анализ литературных источников  показал, что текстовые документы можно классифицировать по четырем признакам: структурированность, объем, значимость ключевых слов и синтаксическая связность  Поступающие в региональную систему обращения граждан обычно относятся к типу неструктурированных, коротких или очень коротких связных текстовых документов. Эти признаки выделены на рисунке 1. Для классификации (кластеризации) текстовых документов разработано большое количество методов и алгоритмов. Рассмотрим возможности и проблемы использования известных методов для анализа документов указанного типа. Из представленных на рисунке 2 типов методов для анализа коротких неструктурированных документов при наличии динамического изменения характеристик рубрик в наибольшей степени подходят методы на основе нейросетей, вероятностные и генетические. В то же время известные варианты данных методов имеют ограничения при автоматизированном анализе обращений граждан: нейросетевые методы достаточно сложны в обучении и связаны с построением большой трудномасштабируемой сети и сложным выбором метрики ; генетические алгоритмы трудно обучаются и тяжело перестраиваются под динамические характеристики тезауруса . В этой связи представляется целесообразным использование их модификаций. Идея разработанного метода на основе весовых коэффициентов состоит в следующем:  каждому слову, соответствующему предметной области, назначается вес;  по умолчанию вес всех слов принимается равным;  проводится обучение метода на некоторой первоначальной выборке документов, в ходе которого веса слов изменяются соответственно их значимости в контексте конкретной предметной области;  проводится корректировка весовых коэффициентов экспертом и на начальном этапе, и в процессе работы системы, так как известна правильность результатов анализа. Чтобы показать модифицированную статистическую часть метода, приведем пример стандартного вероятностного классификатора в виде формулы где максимальная сумма произведения частот употребления слов на количество их употребления в документе, которая определяет предметную область; частота употребления  слова в  предметной области;  количество употреблений  слова в  текстовом документе;  – количество слов в  документе;  – порог употребления  слова в предметной области. Математическую модель разработанного метода автоматической классификации можно представить в виде формулы где максимальная сумма произведений весовых коэффициентов на количество употреблений в документе (весовой коэффициент, в свою очередь, зависит еще от синтаксического коэффициента важности и актуальности информации);  весовой коэффициент слова для предметной области; количество употреблений  слова в документе; синтаксический коэффициент важности слова в документе (определяется синтаксической значимостью слова в предложении). В документах встречаются слова (общие), которые употребляются почти во всех предметных областях, они не несут информацию о предметной области документа. Следовательно, их веса необходимо сделать намного меньше других. Слова, встречающиеся только в одной предметной области (уникальные), являются самыми значимыми, и их веса будут значительно больше других. Еще остаются редкие слова, которые не являются ни уникальными, ни общими. Они несут некоторую информацию о предметной области, поэтому им назначаются промежуточные значения весовых коэффициентов. Алгоритм обучения проводит анализ БД ключевых слов и разбивает их на три категории: уникальные, редкие и общие. Далее эксперт выбирает нужное соотношение и значение весовых коэффициентов заданных трех типов ключевых слов. Для разграничения редких и общих слов вводится порог встречаемости: если слово встретилось в документах только одной предметной области, то это уникальные слова, если не во всех и меньше порога, то редкие, все остальные слова являются общими. В ходе экспериментов были получены оптимальные значения весовых коэффициентов и порога отбора общеупотребительных слов: вес уникальных слов = 50, редких = 10, общих = 1, а порог отбора общих слов составляет 80 %. При данных характеристиках алгоритма обучения метод показывает наилучшие результаты.  Виды классификаторов  С использованием решающих правил  Линейный  На основе функций подобия  Вероятностный  AQ-алгоритм  Нейросетевые  Метод Роше  Байесов  CN2-алгоритм  На основе метода Rocchio  На основе матрицы подобия  Золотоискатель  С4.5-алгоритм На основе метода k ближайших соседей С использованием дерева решений С использованием экспертной оценки  Explora На основе метода опорных векторов  На основе регрессии  На основе жадного алгоритма На основе функций конкурентного сходства (FRiSфункции) На основе функций нечеткого логического вывода  На основе использования нечеткой метрики  Байесово сетевое обучение На основе метода наименьших квадратов  На основе генетического алгоритма  Канонический генетический алгоритм Генетический алгоритм на основе устойчивого состояния Гибридный генетический алгоритм  TF-IDF TF-RIDF  На основе Нечеткая весовых динамическая коэффициентов классификация и нечеткого предметных вывода областей  Можно выделить следующие преимущества весового алгоритма перед частотным алгоритмом:  отсутствие порога частоты употребления слов позволяет распознавать короткие предложения, содержащие большое количество сокращений, цифр и минимум одно ключевое слово;  использование весов, а не частот употребления увеличивает возможности для обучения алгоритма;  при экспертной настройке базы знаний слов предметных областей удобнее работать с весами, чем с частотами и порогами употребления;  возможность использовать слова, не принадлежащие напрямую к предметной области, но приписанные к ней с маленькими весовыми коэффициентами.  Недостатки весового алгоритма по сравнению с частотным алгоритмом:  возрастает сложность процесса обучения;  отсутствие порога распознания предложения увеличивает вероятность распознавания документа, который относится к предметной области, неизвестной нашей системе;  появление уникального слова с большим весом в другой предметной области сильно увеличит шанс на ошибочное распознание документа. Проблема динамического тезауруса в разработанном методе решается путем анализа сдвига кластеров предметных областей и своевременного запуска переобучения и подстройки метода. Методы динамической классификации изложены в .  Документы Эксперт предметной области  Динамическая классификация рубрик  Национальный корпус русского языка  Уточнение рубрик  Поиск ключевых слов  Определение веса ключевых слов  Поиск морфологий  Определение типа ключевого слова общие редкие  Эксперт-лингвист  уникальные БД весовых коэффициентов  БД морфологий  Текстовый документ Определение структурированности  Регистрация документа  Подсчет количества знаков  Поиск ключевых слов  структурированный  длинный  уникальный  частично структурированный  короткий  редко встречающийся  неструктурированный  очень короткий  Пригодный – вернуть эксперту для анализа  Весовой классификатор  Вероятностный классификатор  общий  Анализ пригодности текстового документа  БД частотного анализа  Национальный корпус русского языка Морфологический анализ  Сегментатор  Сбор статистических данных  Создание модели русского языка Синтаксический анализатор MaltParser Проверка синтаксической связности  Уточнение предметной области  Поиск знаний ...  Определение адресата  База знаний  Определение важности документа  Определение срока хранения  Нечеткий логический вывод рассмотрен в статьях . На рисунке 3 представлена схема функционирования системы автоматической классификации текстовых документов при использовании разработанного метода, предполагающего организацию обратной связи с экспертами. Текстовый документ поступает на вход системы и проходит следующие этапы анализа: регистрация в системе, где получает уникальный номер  и озаглавливается необходимыми тэгами для дальнейшей работы; сегментация на слова, предложения, абзацы; морфологический анализ, определяющий лексические характеристики слов и морфемы; классификация документа по трем характеристикам и определение метода его анализа или обозначение документа как непригодного и возврат его экспертам для анализа (если документ короткий или очень короткий и неструктурированный, то используется разработанный метод анализа, в противном случае – вероятностный); уточнение предметной области; синтаксическая разметка документа (для разработанного метода) и проверка синтаксической связности; поиск знаний, определение важности документа и адресата, отправка документа ответственному лицу; сохранение полученных знаний в базу знаний для дальнейшего использования. База знаний реализуется в виде фреймовой реляционной БД, которая хранит извлеченные понятия и отношения между ними. На рисунке 4 представлена схема базы знаний. Предметная область, к которой принадлежит извлеченный фрейм Frame, представлена в виде сущности Class. Relation описывает отношения между фреймами, а также задает тип отношений. Value предназначен для хранения характеристик фреймов и соотносится со слотами и их типами . В качестве ПО для БД используется MS SQL Server 2008, функционала которого достаточно для реализации поставленных задач. Программирование остальных этапов анализа, за исключением MaltParser, осуществляется на Microsoft Visual C#. Для морфологического анализа предварительно составляется морфологический словарь на Национальном корпусе русского языка. Для поддержания актуальности тезауруса каждый раз происходит динамическое отслеживание рубрик и их уточнение. Основываясь на недостатках разработанного и имеющихся методов, необходимо изменить весь процесс анализа текстовых документов, как предложено на рисунке 3. Весовой метод необходимо применять только при анализе коротких текстовых документов, следовательно, добавляется этап проверки длины текстового документа, который определяет метод анализа: один из известных методов или описанный выше.
Основной подход, предлагаемый в современной литературе для детектирования объектов на изображении, заключается в использовании 2D-признаков, инвариантных к возможным искажениям изображения объекта, вызванным аффинными преобразованиями , деформациями сдвига, изменением освещения или масштаба изображения объекта  и т.д. Для извлечения признаков из изображений, например, широко используется алгоритм SIFT (Scale-invariant feature transform), впервые рассмотренный в . Для достижения качественного результата в процессе обучения классификатора при использовании признакового описания объекта путем поиска особых точек и вычисления дескрипторов их окрестностей требуется специальная подготовка обучающего набора исходных изображений и отбора уникальных признаков объекта для формирования визуального словаря, например, по методике, описанной в . После извлечения 2D-признаков из тестового изображения производится их сравнение с признаками , хранящимися в БД. Чтобы распознать некоторый известный объект, система распознавания сначала извлекает множество характерных точек из изображений объекта, сделанных с различного ракурса, и запоминает извлеченные из них признаки в проиндексированную структуру, например такую, как дерево поиска. Во время распознавания признаки извлекаются из проверяемого изображения и сравниваются с сохраненными признаками объекта. Каждый раз, когда заданное число признаков, извлеченных из тестового изображения, совпадает с признаками, описывающими детектируемый объект, система распознавания вызывает процедуру верификации – проверку на взаимное совпадение пространственного положения множества точек на тестовом изображении, из которого были извлечены признаки, с взаимным положением характерных точек, описывающих объект. При формировании признакового описания объекта по данной методике возникает сложность процедуры формирования набора признаков, по которому однозначно можно идентифицировать объект, что приводит к появлению ложных распознаваний. Появление ложных распознаваний связано с тем, что исходные изображения определенного класса объектов могут содержать множество помех и похожие признаки могут принадлежать объектам различных классов. Поэтому для повышения надежности распознавания требуется извлечь из тренировочного набора изображений как можно больше признаков и сформировать словарь визуальных слов  для формирования набора уникальных признаков, что невозможно эффективно реализовать для задачи детектирования объектов при низком разрешении изображения данных объектов. Задача поиска объектов определенного класса на фотоснимках, полученных с видеокамеры наружного наблюдения, например задача распознавания транспортных средств на этих фотоснимках, связана с детектированием объектов, как правило, имеющих низкое разрешение. Для решения данной задачи необходим иной подход к формированию признакового описания объекта. Обзор современной литературы, описывающей методику формирования признакового описания объекта, показал, что наиболее передовой подход при формировании описания объекта заключается в использовании автокодировщика , являющегося нейронной сетью с симметричной архитектурой. Автокодировщик состоит из входного и выходного слоев одинаковой размерности и скрытых слоев с меньшей размерностью, чем у входного и выходного слоев. Автокодировщик имеет скрытый слой, называемый bottleneck-слоем, у которого самая наименьшая размерность из всех скрытых слоев. Размерность слоев автокодировщика при переборе от входного слоя к bottleneck-слою последовательно уменьшается, а при переборе от bottleneck-слоя к выходному слою последовательно увеличивается. Набор слоев автокодировщика от входного до bottleneck-слоя называется кодировщиком, который осуществляет преобразование некоторого входного сигнала в сигнал bottleneck, где количество нейронов в bottleneck-слое. Набор слоев автокодировщика от bottleneck-слоя до выходного называется декодировщиком, он осуществляет преобразование выходного сигнала с кодировщика, где  количество слоев в автокодировщике. В общем случае задача обучения автокодировщика заключается в поиске такой конфигурации весов слоев автокодировщика и пороговых значений которая при подаче на его вход многомерного сигнала обеспечивала бы на выходе отклик, наиболее близкий к входному сигналу. В качестве функции активации для слоев автокодировщика используют нелинейную функцию активации, например сигмоид, что позволяет автокодировщику аппроксимировать сложные зависимости. Автокодировщик может применяться для уменьшения размерности входного сигнала. Уменьшение размерности входного сигнала при его подаче на вход автокодировщика осуществляется при считывании с bottleneck- слоя выходного сигнала, составляющие которого слабо коррелируют или не коррелируют между собой. Выходной сигнал с bottleneck-слоя автокодировщика далее подается на вход модуля мультиномиальной регрессии RVM (relevance vector machine), который вычисляет вероятность наличия изображения объекта определенного класса на извлеченном участке фотоснимка. По результатам сканирования всего фотоснимка производятся анализ наличия изображений объектов определенного класса на фотоснимке и определение областей фотоснимка, содержащих данные изображения.   Разработка архитектуры автокодировщика и способа его обучения Обозначим через входной сигнал, имеющий размерность, а через  функциональную зависимость между сигналами на входе  и выходе   слоя, где    весовые коэффициенты  слоя; пороговые значения  слоя;  функция активации, в качестве которой согласно  выбрана сигмоид-функция. Далее запишем функциональную зависимость между входом и выходом автокодировщика в виде  функциональная зависимость между сигналами на входе и выходе кодировщика; – функциональная зависимость между сигналами на входе и выходе декодировщика;  – число слоев автокодировщика. Задача настройки автокодировщика заключается в поиске такой конфигурации   ,  весов и пороговых значений  , которая минимизировала бы выбранную целевую функцию. В качестве целевой функции используется функция потерь и пороговых значений  определяющая меру несоответствия  сигнала на выходе автокодировщика от ожидаемого сигнала при подаче на его вход заданного сигнала. Для стандартного автокодировщика, предназначенного только для сжатия информации, функция потерь имеет вид а задача обучения автокодировщика  сводится к минимизации целевой функции вида  где  – размер обучающей выборки .   Для обучения автокодировщика дополнительному функционалу, а именно способности извлекать информацию о заданной категории объектов, необходимо использовать другую функцию потерь, которая учитывала бы наличие объекта, относящегося к определенной категории на изображении. Для обучения автокодировщика, обладающего описанным функционалом, имеется обучающий набор изображений , состоящий из  изображений. Каждому изображению из набора назначена метка класса. Каждое изображение из набора имеет размер, при этом , где  – размерность входа автокодировщика. Изображение из набора имеет метку 1, если содержит изображение всего или части объекта, принадлежащего к классу детектируемых объектов, в противном случае изображение имеет метку –1. В качестве функции потерь для выбранного автокодировщика предлагается модифицировать стандартную целевую функцию путем добавления в нее дополнительной функции потерь, например, как в работе , для учета принадлежности входного сигнала, подаваемого на вход автокодировщика к детектируемому классу объектов.  Обозначим через  набор состояний слоя автокодировщика при его обучении, где – состояние  скрытого -го -слоя автокодировщика при подаче на его вход изображений из обучающего набора, имеющих определенную метку из набора  номер образца изображения из поднабора изображений, имеющих определенную метку;  номер итерации процесса обучения. Тогда дополнительную функцию потерь можно записать в виде мощность подмножества образцов из  обучающего набора с заданной меткой; обозначение десятичного логарифма;  евклидово расстояние между состояниями слоя автокодировщика (при подаче на вход двух различных изображений);  – пороговое значение, задающее диапазон разброса расстояний между состояниями -слоя автокодировщика. Так как на снимках, полученных с видеокамеры наружного наблюдения, размер изображений объектов, как правило, не превышает  пикселей, для их эффективного поиска методом скользящего окна экспериментально выбран размер окна . Для обнаружения объекта изображение сканируется скользящим окном, которое последовательно перемещается по заданной направляющей сетке, обеспечивающей перекрытие между соседними сканируемыми участками изображения в. Так как окно имеет размер  пикселей, перекрытие составляет 8 пикселей. Полученные при помощи аппаратуры видеонаблюдения изображения могут содержать сильную шумовую компоненту, например из-за плохой видимости или внешнего освещения, поэтому для качественной реализации решения задачи классификации необходимо производить предварительную фильтрацию изображений. В результате поиска возможных архитектурных решений для реализации автокодировщика и методик обучения автокодировщика была выбрана концепция автокодировщика, который позволяет производить не только уменьшение размерности входных данных, но и их фильтрацию от возможных помех. За основу реализации процесса обучения автокодировщика была выбрана методика процесса обучения, описанная в статье . Для обучения автокодировщика разработано ПО, для реализации которого используется фреймворк , который был разработан в . Обучение производится в две стадии. На первой стадии автокодировщик учится восстанавливать образ, поданный на его вход, как стандартный автокодировщик, по схеме, изображенной на рисунке 1. Для обучения автокодировщика во время проведения первой фазы обучения из обучающего набора используются только изображения, относящиеся к классу детектируемых объектов (то есть имеющие метку 1). После завершения первой фазы производится обучение обученного стандартному функционалу автокодировщика дополнительному функционалу с использованием функции потерь , которая производит анализ сигнала на выходе -слоя, и функции потерь из , которая производит анализ сигнала выходного слоя автокодировщика (на рисунке 1 слой с названием). Для обучения автокодировщика цветное изображение предварительно преобразуется в черно-белое, после чего производится трансформация значений интенсивности пикселей изображения из формата  путем умножения значений на масштабирующий коэффициент  После обучения автокодировщика производится обучение классификатора , с которым автокодировщик будет работать совместно, в соответствии со схемой, изображенной на рисунке 2. После подачи на вход автокодировщика участка исходного изображения с его считывается многомерный сигнал  который является сжатым и отфильтрованным образом участка исходного изображения. Полученный сигнал подается на вход классификатора , обучение которого производится на маркированном наборе образов, считанных с -слоя обученного автокодировщика при подаче на его вход изображений из обучающего набора. Изображения, используемые для обучения автокодировщика, могут быть искусственно синтезированы из исходных изображений обучающего набора путем искусственных трансформаций и добавления шумовой составляющей.  Метод релевантных векторов для задачи распознавания   Пусть  обучающая выборка, полученная из обучающего набора изображений после их обработки автокодировщиком, где вектор признаков, извлеченный из изображения обучающего набора при помощи автокодировщика;  число изображений в обучающем наборе;    вектор,  элемент которого равен 1, а остальные элементы нулевые, что указывает на принадлежность рассматриваемого образца к -му классу объектов, при этом метки от 1 до  соответствуют различным классам детектируемых объектов (например, класс детектируемых объектов «автотранспортные средства» состоит из подклассов «легковые», «грузовые» средства и т.д.). Необходимо определить, содержит ли изображение, представленное своим вектором признаков, извлеченным при помощи автокодировщика, изображение объектов определенного класса; если да, то вывести метку класса изображенного объекта. Рассмотрим сущность метода , предназначенного для решения задачи восстановления регрессии, при этом поиск решения осуществляется в классе линейных решающих правил :  – набор базисных функций (в качестве базисной функции в данной реализации алгоритма используется гауссова радиальная базисная функция);   весовые коэффициенты, которые определяют вклад, вносимый определенной базисной функцией;  аддитивный гауссовский шум с нулевым математическим ожиданием. Для обучения рассматриваемой модели используется метод максимального правдоподобия, который позволяет производить оценку весовых коэффициентов по обучающей выборке: где  набор скалярных значений,  каждое из которых соответствует определенному  вектору   из обучающей выборки   – матрица, сформированная из значений базисных функций, вычисленных для обучающей выборки. Для адаптации  к задаче бинарной классификации в статье  рассматривается функция правдоподобия, основанная на распределении Бернулли:  где    логистическая функция; обучающая выборка;   элементы вектора   Стандартный подход к решению задачи мультиклассовой классификации, рассматриваемой в данной статье, – мультиномиальная логистическая регрессия, где примем. Тогда аналогичным выражению образом вводим следующую функцию правдоподобия где til – метка класса i-го образца. Для оценки апостериорного максимума выведенной функции правдоподобия используем технику, описанную в статье . При этом делаем допущение о распределении весовых коэффициентов , где  вектор гиперпараметров, и шума измерения  а также вводим эквивалентную объектную функцию.  Реализация процесса сканирования изображения Процесс детектирования объектов определенного класса разбит на несколько фаз. Во время первой фазы фотоснимок сканируется скользящим окном, которое перемещается по регулярной сетке, обеспечивая 40 %-ное перекрытие между соседними сканируемыми участками фотоснимка. После первой, предварительной, фазы детектирования производится отбор участков фотоснимка, для которых предсказанное RVM-значение вероятности того, что данный участок фотоснимка содержит изображение детектируемого объекта определенного класса , больше некоторого заданного порогового  значения. Для каждого отобранного участка вычисляется положение центрального пикселя, которому назначается метка класса , выявленная модулем RVM. Далее для всех пикселей, соседних с центральным пикселем, вычисляется принадлежность к классу объектов с меткой . Вычисление принадлежности рассматриваемых пикселей к данному классу  производится на основании значения вероятности, которое вычисляется путем интерполяции методом  natural neighbor . Интерполяция значений вероятности   принадлежности к данному классу объектов  производится на основе данных о значении  для центральных участков  пикселей, соседних с рассматриваемым участком.  Если значение  для пикселя, принадлежность которого к классу объектов  выясняется, больше некоторого заданного порогового  значения, данному пикселю назначается метка класса объектов . Если выясняется, что исследуемый пиксель принадлежит к классу объектов , производится выявление принадлежности или непринадлежности к классу объектов  соседних с ним пикселей, принадлежность которых еще не выявлена. В противном случае, если исследуемый пиксель не принадлежит к классу объектов , процедура выявления принадлежности соседних с ним пикселей не производится. В результате описанной процедуры формируется множество пикселей, для которых вычисляется минимальный ограничивающий прямоугольник, определяющий границы изображения детектируемого объекта. Экспериментальная часть Для оценки качества детектирования объектов, имеющих низкое разрешение, и целесообразности использования разработанного алгоритма был проведен эксперимент, в котором для обучения детектора и оценки качества его работы использовался полученный с веб-ресурса набор изображений аэрофотосъемки. Изображения из набора имеют разрешение см на пиксель. Из данного набора были сформированы два набора изображений, один из которых содержит изображения транспортных средств, имеющих размеры примерно  пикселей. Второй набор изображений, полученный из исходного набора, содержит различные изображения земной поверхности без изображений транспортных средств. Обучение детектора и оценка качества детектирования производились на персональном компьютере, обладающем следующими характеристиками: центральный процессор –   7-6700; оперативная память – 8 Гб; видеопроцессор –    980 ; операционная система –  14.04 . Для измерения качества детектирования использовались следующие показатели. Сравнение качества детектирования разработанного алгоритма производилось с  DPM (deformable part model), детектором HOG+SVM, детектором RVM ,  работающим в связке со стандартным автокодировщиком. Результаты оценки работы алгоритмов приведены в таблице.   Проведенный эксперимент показал, что RVM в связке со стандартным автокодировщиком работает хуже DPM, но лучше детектора HOG+SVM. Если вместо стандартного автокодировщика использовать дискриминативный автокодировщик, качество детектирования значительно возрастет, то есть данный алгоритм превзойдет остальные алгоритмы, включая алгоритм DPM. Таким образом, результаты исследования доказывают целесообразность использования предложенного алгоритма. Выводы В результате проведенного исследования был предложен алгоритм детектирования изображений объектов определенного класса на фотоснимках, полученных с видеокамеры при малом разрешении изображений детектируемых объектов и низком качестве фотоснимков. В процессе создания алгоритма выбрана архитектура и разработана схема обучения автокодировщика, предназначенного для формирования признакового описания объектов по их изображениям. В работе предложена оригинальная функция потерь для обучения автокодировщика дополнительному функционалу, а именно способности извлекать информацию только о заданной категории объектов. Для детектирования объектов использована схема скользящего окна, которое сканирует фотоснимок, обеспечивая перекрытие между соседними сканируемыми участками фотоснимка в 40 %. С целью повышения точности детектирования границ изображения объекта применен мультиномиальный классификатор, построенный на основе алгоритма , для вычисления вероятности нахождения изображения объекта или его частей в сканируемом участке фотоснимка. Также для повышения точности обнаружения границ изображения детектируемого объекта было предложено интерполировать значения вероятности обнаружения детектируемого объекта определенного класса для каждого пикселя изображения, анализ принадлежности которого к изображению объекта проводится. После чего на основании распределения пикселей, которые были приняты за принадлежащие к изображению объекта определенного класса, производится оценка границ изображения детектируемого объекта. На основании результатов проведенного исследования было разработано компьютерное приложение (рис. 3). Результаты данного исследования могут найти широкое применение для автоматизации различных процессов, например, для сбора и анализа информации в различных аналитических системах.